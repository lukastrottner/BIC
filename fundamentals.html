<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Fundamentals Concepts | Bayesian Inference and Computation</title>
  <meta name="description" content="This book contains the lecture notes for the module Bayesian Inference and Computation." />
  <meta name="generator" content="bookdown 0.42 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Fundamentals Concepts | Bayesian Inference and Computation" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="/uob_logo.png" />
  <meta property="og:description" content="This book contains the lecture notes for the module Bayesian Inference and Computation." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Fundamentals Concepts | Bayesian Inference and Computation" />
  
  <meta name="twitter:description" content="This book contains the lecture notes for the module Bayesian Inference and Computation." />
  <meta name="twitter:image" content="/uob_logo.png" />

<meta name="author" content="Dr Mengchu Li and Dr Lukas Trottner (based on lecture notes by Dr Rowland Seymour)" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="programming-in-r.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Inference and Computation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Practicalities</a>
<ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#module-aims"><i class="fa fa-check"></i><b>0.1</b> Module Aims</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#module-structure"><i class="fa fa-check"></i><b>0.2</b> Module Structure</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#assessment"><i class="fa fa-check"></i><b>0.3</b> Assessment</a></li>
<li class="chapter" data-level="0.4" data-path="index.html"><a href="index.html#recommended-books-and-videos"><i class="fa fa-check"></i><b>0.4</b> Recommended Books and Videos</a></li>
<li class="chapter" data-level="0.5" data-path="index.html"><a href="index.html#common-distributions"><i class="fa fa-check"></i><b>0.5</b> Common Distributions</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="fundamentals.html"><a href="fundamentals.html"><i class="fa fa-check"></i><b>1</b> Fundamentals Concepts</a>
<ul>
<li class="chapter" data-level="1.1" data-path="fundamentals.html"><a href="fundamentals.html#statistical-inference"><i class="fa fa-check"></i><b>1.1</b> Statistical Inference</a></li>
<li class="chapter" data-level="1.2" data-path="fundamentals.html"><a href="fundamentals.html#frequentist-theory"><i class="fa fa-check"></i><b>1.2</b> Frequentist Theory</a></li>
<li class="chapter" data-level="1.3" data-path="fundamentals.html"><a href="fundamentals.html#bayesian-paradigm"><i class="fa fa-check"></i><b>1.3</b> Bayesian Paradigm</a></li>
<li class="chapter" data-level="1.4" data-path="fundamentals.html"><a href="fundamentals.html#bayes-theorem"><i class="fa fa-check"></i><b>1.4</b> Bayes’ Theorem</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="programming-in-r.html"><a href="programming-in-r.html"><i class="fa fa-check"></i><b>2</b> Programming in R</a>
<ul>
<li class="chapter" data-level="2.1" data-path="programming-in-r.html"><a href="programming-in-r.html#random-numbers-for-loops-and-r"><i class="fa fa-check"></i><b>2.1</b> Random Numbers, For Loops and R</a></li>
<li class="chapter" data-level="2.2" data-path="programming-in-r.html"><a href="programming-in-r.html#functions-in-r"><i class="fa fa-check"></i><b>2.2</b> Functions in R</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="programming-in-r.html"><a href="programming-in-r.html#built-in-commands"><i class="fa fa-check"></i><b>2.2.1</b> Built in commands</a></li>
<li class="chapter" data-level="2.2.2" data-path="programming-in-r.html"><a href="programming-in-r.html#user-defined-functions"><i class="fa fa-check"></i><b>2.2.2</b> User defined functions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="programming-in-r.html"><a href="programming-in-r.html#good-coding-practices"><i class="fa fa-check"></i><b>2.3</b> Good Coding Practices</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="programming-in-r.html"><a href="programming-in-r.html#code-style"><i class="fa fa-check"></i><b>2.3.1</b> Code Style</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bayesian-inference.html"><a href="bayesian-inference.html"><i class="fa fa-check"></i><b>3</b> Bayesian Inference</a>
<ul>
<li class="chapter" data-level="3.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#simple-examples"><i class="fa fa-check"></i><b>3.1</b> Simple Examples</a></li>
<li class="chapter" data-level="3.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#reporting-conclusions-from-bayesian-inference"><i class="fa fa-check"></i><b>3.2</b> Reporting Conclusions from Bayesian Inference</a></li>
<li class="chapter" data-level="3.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#conjugate-prior-and-posterior-analysis"><i class="fa fa-check"></i><b>3.3</b> Conjugate Prior and Posterior Analysis</a></li>
<li class="chapter" data-level="3.4" data-path="bayesian-inference.html"><a href="bayesian-inference.html#prediction"><i class="fa fa-check"></i><b>3.4</b> Prediction</a></li>
<li class="chapter" data-level="3.5" data-path="bayesian-inference.html"><a href="bayesian-inference.html#non-informative-prior-distibrutions"><i class="fa fa-check"></i><b>3.5</b> Non-informative Prior Distibrutions</a></li>
<li class="chapter" data-level="3.6" data-path="bayesian-inference.html"><a href="bayesian-inference.html#frequentist-analysis-of-bayesian-methods"><i class="fa fa-check"></i><b>3.6</b> Frequentist analysis of Bayesian methods</a></li>
<li class="chapter" data-level="3.7" data-path="bayesian-inference.html"><a href="bayesian-inference.html#hierarchical-models"><i class="fa fa-check"></i><b>3.7</b> Hierarchical Models</a></li>
<li class="chapter" data-level="3.8" data-path="bayesian-inference.html"><a href="bayesian-inference.html#lab"><i class="fa fa-check"></i><b>3.8</b> Lab</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="sampling.html"><a href="sampling.html"><i class="fa fa-check"></i><b>4</b> Sampling</a>
<ul>
<li class="chapter" data-level="4.1" data-path="sampling.html"><a href="sampling.html#uniform-random-numbers"><i class="fa fa-check"></i><b>4.1</b> Uniform Random Numbers</a></li>
<li class="chapter" data-level="4.2" data-path="sampling.html"><a href="sampling.html#inverse-transform-sampling"><i class="fa fa-check"></i><b>4.2</b> Inverse Transform Sampling</a></li>
<li class="chapter" data-level="4.3" data-path="sampling.html"><a href="sampling.html#rejection-sampling"><i class="fa fa-check"></i><b>4.3</b> Rejection Sampling</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="sampling.html"><a href="sampling.html#rejection-sampling-efficiency"><i class="fa fa-check"></i><b>4.3.1</b> Rejection Sampling Efficiency</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="sampling.html"><a href="sampling.html#ziggurat-sampling"><i class="fa fa-check"></i><b>4.4</b> Ziggurat Sampling</a></li>
<li class="chapter" data-level="4.5" data-path="sampling.html"><a href="sampling.html#approximate-bayesian-computation"><i class="fa fa-check"></i><b>4.5</b> Approximate Bayesian Computation</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="sampling.html"><a href="sampling.html#abc-with-rejection"><i class="fa fa-check"></i><b>4.5.1</b> ABC with Rejection</a></li>
<li class="chapter" data-level="4.5.2" data-path="sampling.html"><a href="sampling.html#summary-abc-with-rejection"><i class="fa fa-check"></i><b>4.5.2</b> Summary ABC with Rejection</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="sampling.html"><a href="sampling.html#lab-1"><i class="fa fa-check"></i><b>4.6</b> Lab</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html"><i class="fa fa-check"></i><b>5</b> Markov Chain Monte Carlo</a>
<ul>
<li class="chapter" data-level="5.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#properties-of-markov-chains"><i class="fa fa-check"></i><b>5.1</b> Properties of Markov Chains</a></li>
<li class="chapter" data-level="5.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#metropolishastings"><i class="fa fa-check"></i><b>5.2</b> Metropolis–Hastings</a></li>
<li class="chapter" data-level="5.3" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#gibbs-sampler"><i class="fa fa-check"></i><b>5.3</b> Gibbs Sampler</a></li>
<li class="chapter" data-level="5.4" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#metropolis-within-gibbs"><i class="fa fa-check"></i><b>5.4</b> Metropolis-within-Gibbs</a></li>
<li class="chapter" data-level="5.5" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#mcmc-diagnostics"><i class="fa fa-check"></i><b>5.5</b> MCMC Diagnostics</a></li>
<li class="chapter" data-level="5.6" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#beyond-mcmc"><i class="fa fa-check"></i><b>5.6</b> Beyond MCMC</a></li>
<li class="chapter" data-level="5.7" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#lab-2"><i class="fa fa-check"></i><b>5.7</b> Lab</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="advanced-computation.html"><a href="advanced-computation.html"><i class="fa fa-check"></i><b>6</b> Advanced Computation</a>
<ul>
<li class="chapter" data-level="6.1" data-path="advanced-computation.html"><a href="advanced-computation.html#gaussian-processes"><i class="fa fa-check"></i><b>6.1</b> Gaussian Processes</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="advanced-computation.html"><a href="advanced-computation.html#covariance-functions"><i class="fa fa-check"></i><b>6.1.1</b> Covariance Functions</a></li>
<li class="chapter" data-level="6.1.2" data-path="advanced-computation.html"><a href="advanced-computation.html#gaussian-process-regression"><i class="fa fa-check"></i><b>6.1.2</b> Gaussian Process Regression</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="advanced-computation.html"><a href="advanced-computation.html#data-augmentation"><i class="fa fa-check"></i><b>6.2</b> Data Augmentation</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="advanced-computation.html"><a href="advanced-computation.html#imputing-censored-observations"><i class="fa fa-check"></i><b>6.2.1</b> Imputing censored observations</a></li>
<li class="chapter" data-level="6.2.2" data-path="advanced-computation.html"><a href="advanced-computation.html#imputing-latent-variables"><i class="fa fa-check"></i><b>6.2.2</b> Imputing Latent Variables</a></li>
<li class="chapter" data-level="6.2.3" data-path="advanced-computation.html"><a href="advanced-computation.html#grouped-data"><i class="fa fa-check"></i><b>6.2.3</b> Grouped Data</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="advanced-computation.html"><a href="advanced-computation.html#prior-ellicitation"><i class="fa fa-check"></i><b>6.3</b> Prior Ellicitation</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="advanced-computation.html"><a href="advanced-computation.html#prior-summaries"><i class="fa fa-check"></i><b>6.3.1</b> Prior Summaries</a></li>
<li class="chapter" data-level="6.3.2" data-path="advanced-computation.html"><a href="advanced-computation.html#betting-with-histograms"><i class="fa fa-check"></i><b>6.3.2</b> Betting with Histograms</a></li>
<li class="chapter" data-level="6.3.3" data-path="advanced-computation.html"><a href="advanced-computation.html#prior-intervals"><i class="fa fa-check"></i><b>6.3.3</b> Prior Intervals</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="advanced-computation.html"><a href="advanced-computation.html#lab-3"><i class="fa fa-check"></i><b>6.4</b> Lab</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="advanced-computation.html"><a href="advanced-computation.html#gaussian-processes-1"><i class="fa fa-check"></i><b>6.4.1</b> Gaussian Processes</a></li>
<li class="chapter" data-level="6.4.2" data-path="advanced-computation.html"><a href="advanced-computation.html#missing-data"><i class="fa fa-check"></i><b>6.4.2</b> Missing Data</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Inference and Computation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="fundamentals" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">Chapter 1</span> Fundamentals Concepts<a href="fundamentals.html#fundamentals" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Bayesian inference is built on a different way of thinking about parameters of probability distributions than methods you have learnt so far. In the past 30 years or so, Bayesian inference has become much more popular. This is partly due to increased computational power becoming available. In this first chapter, we are going to set out to answer:</p>
<ol style="list-style-type: decimal">
<li><p>What are the fundamental principles of Bayesian inference?</p></li>
<li><p>What makes Bayesian inference different from other methods?</p></li>
</ol>
<div id="statistical-inference" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> Statistical Inference<a href="fundamentals.html#statistical-inference" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The purpose of statistical inference is to “draw conclusions, from numerical data, about quantities that are not observed” (Bayesian Data Analysis, chapter 1). Generally speaking, there are two kinds of inference:</p>
<ol style="list-style-type: decimal">
<li>Inference for quantities that are unobserved or haven’t happened yet. Examples of this might be the size of a payout an insurance company has to make, or a patients outcome in a clinical trial had they been received a certain treatment.</li>
<li>Inference for quantities that are not possible to observe. This is usual because they are part of modelling process, like parameters in a linear model.</li>
</ol>
</div>
<div id="frequentist-theory" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Frequentist Theory<a href="fundamentals.html#frequentist-theory" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Frequentist interpretation of probability is built upon the theory on long run events. Probabilities must be interpretable as frequencies over multiple repetitions of the experiment that is being analysed, and are calculated from the sampling distributions of measured quantities.</p>
<div class="definition">
<p><span id="def:unlabeled-div-1" class="definition"><strong>Definition 1.1  </strong></span>In the frequentist world, the long run relative frequency of an event is the <strong>probability</strong> of that event.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-2" class="example"><strong>Example 1.1  </strong></span>If a frequentist wanted to assign a probability to rolling a 6 on a particular dice, then they would roll the dice a large number of times and compute the relative frequency.</p>
</div>
<p>Typical frequentist analysis starts with a statistical model <span class="math inline">\(\{P_\theta: \theta \in \Theta\}\)</span>, where <span class="math inline">\(\Theta\)</span> denotes some parameter space. E.g. <span class="math inline">\(\{N(\theta,1) : \theta \in \Theta = \mathbb{R}\}\)</span>. The data are assumed to be generated from some statistical model, and we often consider the case that they are independent and identically distributed (i.i.d), i.e.
<span class="math display">\[
Y_1,\dotsc,Y_n \overset{i.i.d}{\sim} P_{\theta}, \; \theta \in \Theta.
\]</span>
Without going much into the measure-theoretic formulation, we write
<span class="math display">\[
Y_1,\dotsc,Y_n \overset{i.i.d}{\sim} \pi(y\mid {\theta}),\; \theta \in \Theta,
\]</span>
where <span class="math inline">\(\pi(x\mid {\theta})\)</span> is the probability density or mass function depending on whether <span class="math inline">\(P_{\theta}\)</span> is continuous or discrete. We shall simply refer to it as the density function of <span class="math inline">\(P_{\theta}\)</span>. The actual observed data <span class="math inline">\(\boldsymbol{y} = (y_1,\dotsc,y_n)\)</span> are considered to be realisations of the random variable <span class="math inline">\(Y = (Y_1,\dotsc,Y_n)\)</span>.</p>
<p>The fundamental difference between frequentist and Bayesian statistical analysis is that <span class="math inline">\(\theta\)</span> is viewed a deterministic (non-random) quantity by a frequentist but a Bayesian statistician would view it as a random quantity. To estimate <span class="math inline">\(\theta\)</span> in the frequentist paradigm, one would consider some estimator <span class="math inline">\(\hat{\theta}(Y)\)</span>, and analyse its finite-sample distribution or asymptotic distribution.</p>
<p>The most common way to estimate the value of <span class="math inline">\(\theta\)</span> is using maximum likelihood estimation (MLE). Although other methods do exist (e.g. method of moments). Recall that the likelihood function <span class="math inline">\(\pi(\boldsymbol{y} \mid \theta)\)</span> is simply the joint density of <span class="math inline">\(Y_1,\dotsc,Y_n\)</span>. Under the i.i.d assumption, it is
<span class="math display">\[
\pi(\boldsymbol{y} \mid \theta) = \prod_{i=1}^n \pi(y_i\mid \theta).
\]</span></p>
<div class="definition">
<p><span id="def:unlabeled-div-3" class="definition"><strong>Definition 1.2  </strong></span>The maximum likelihood estimator of <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\hat{\theta}\)</span>, is the value such that <span class="math inline">\(\hat{\theta} = \mathop{\mathrm{arg\,max}}_{\theta} \pi(Y\mid \theta)\)</span>.</p>
</div>
<p>The key to quantify uncertainty in the frequentist paradim is using confidence intervals.</p>
<div class="definition">
<p><span id="def:unlabeled-div-4" class="definition"><strong>Definition 1.3  </strong></span>An interval <span class="math inline">\([L_n(Y), U_n(Y)] \subseteq \mathbb{R}\)</span>, is called a confidence interval (CI) at level <span class="math inline">\(1 - \alpha \in (0,1)\)</span> for <span class="math inline">\(\theta\)</span>, if
<span class="math display">\[ \pi\big(L_n(Y) \leq \theta \leq U_n(Y)\big) = 1 - \alpha.
\]</span></p>
</div>
<p>It is important to stress that the confidence interval (CI) <span class="math inline">\([L_n(Y), U_n(Y)]\)</span> is a random quantity and the parameter <span class="math inline">\(\theta\)</span> is fixed (non-random) in the frequentist paradigm! The correct interpretation of CIs is that if we construct many confidence intervals from repeated random samples, <span class="math inline">\(100(1-\alpha)\%\)</span> of these intervals would contain the true parameter <span class="math inline">\(\theta\)</span>. It does <em>not</em> mean that a particular interval contains the true value of <span class="math inline">\(\theta\)</span> with probability <span class="math inline">\(1-\alpha\)</span>. Note that the CI interpretation is based on the previous definition of the probability of an event and this is why CIs based inference are considered to be frequentist.</p>
<p>Note that, in many cases, one can only hope to obtain intervals that contains <span class="math inline">\(\theta\)</span> at level <span class="math inline">\(1-\alpha\)</span> when <span class="math inline">\(n\)</span> is sufficiently large. For example, asymptotic properties of MLE
<span class="math display">\[
\sqrt{n} (\hat{\theta} - \theta) \overset{d}{\longrightarrow} N(0, I(\theta)^{-1}),
\]</span>
allows one to use
<span class="math display">\[
\hat{\theta} \pm \Phi^{-1}{(1-\alpha / 2)} \sqrt{\frac{1}{n I(\hat{\theta})}},
\]</span>
as an approxiamte CI at level <span class="math inline">\(1-\alpha\)</span>, under mild conditions, where <span class="math inline">\(\Phi(x)\)</span> is the cumulative distribution function of <span class="math inline">\(N(0,1)\)</span> and
<span class="math display">\[
I(\theta) = \mathrm{Var} \left[ \frac{\partial}{\partial \theta} \log \pi(Y\mid \theta) \right] = -\mathbb{E} \left[ \frac{\partial^2}{\partial \theta^2} \log \pi(Y \mid \theta) \right], \qquad Y \sim \pi(y\mid\theta)
\]</span>
is the Fisher information.</p>
</div>
<div id="bayesian-paradigm" class="section level2 hasAnchor" number="1.3">
<h2><span class="header-section-number">1.3</span> Bayesian Paradigm<a href="fundamentals.html#bayesian-paradigm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Given that we want to understand the properties of <span class="math inline">\(\theta\)</span> given the data we have observed <span class="math inline">\(\boldsymbol{y}\)</span>, then you might think it makes sense to investigate the distribution <span class="math inline">\(\pi(\theta \mid \boldsymbol{y})\)</span>. This distribution says what are the likely values of <span class="math inline">\(\theta\)</span> given the information we have observed from the data <span class="math inline">\(\boldsymbol{y}\)</span>. We will talk about Bayes’ theorem in more detail later on in this chapter, but, for now, we will use it to write down this distribution
<span class="math display">\[
\pi(\theta \mid \boldsymbol{y}) = \frac{\pi(\boldsymbol{y} \mid \theta)\pi(\theta)}{\pi(\boldsymbol{y})}.
\]</span>
This is where frequentist theory cannot help us, particularly the term <span class="math inline">\(\pi(\theta)\)</span>. Randomness can only come from the data, so how can we assign a probability distribution to a constant <span class="math inline">\(\theta\)</span>? The term <span class="math inline">\(\pi(\theta)\)</span> is meaningless under this philosophy. Instead, we turn to a different philosophy where we can assign a probability distribution to <span class="math inline">\(\theta\)</span>.</p>
<p>The Bayesian paradigm is built around a different interpretation of probability. This allows us to generate probability distributions for parameters values.</p>
<div class="definition">
<p><span id="def:unlabeled-div-5" class="definition"><strong>Definition 1.4  </strong></span>In the Bayesian world, the subjective belief of an event is the <strong>probability</strong> of that event.</p>
</div>
<p>This definition means we can assign probabilities to events that frequentists do not recognise as valid.</p>
<div class="example">
<p><span id="exm:unlabeled-div-6" class="example"><strong>Example 1.2  </strong></span>Consider the following events:</p>
<ol style="list-style-type: decimal">
<li><p>Lukas’s height is above 192cm.</p></li>
<li><p>Mengchu’s weight is above 74kg.</p></li>
<li><p>Man United will lose against Fulham on 26 Jan 2025.</p></li>
<li><p>There is more than 90% chance that a particular experiment is going to fail</p></li>
</ol>
</div>
<p>Probabilities can be assigned to any events in the Bayesian paradigm, but they are necessarily subjective. The key in Bayesian inference is to understand how does subject beliefs change when some data/evidence become available. This is essentially captured in the Bayes’ theorem. Before we discuss Bayes’ theorem, we recap some basic facts in probability.</p>
<div class="definition">
<p><span id="def:unlabeled-div-7" class="definition"><strong>Definition 1.5  </strong></span>Given two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, the <strong>conditional probability</strong> that event <span class="math inline">\(A\)</span> occurs given the event <span class="math inline">\(B\)</span> has already occurred is
<span class="math display">\[
\pi(A \mid B) = \frac{\pi(A \cap B)}{\pi(B)},
\]</span>
when <span class="math inline">\(\pi(B) &gt; 0\)</span>.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-8" class="definition"><strong>Definition 1.6  </strong></span>Two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are <strong>conditionally independent</strong> given event <span class="math inline">\(C\)</span> if and only if
<span class="math display">\[ \pi(A \cap B \mid C) = \pi(A \mid C)\pi(B \mid C).\]</span></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-9" class="definition"><strong>Definition 1.7  </strong></span>For two random variables <span class="math inline">\(X,Y\)</span> that have a joint density function <span class="math inline">\(\pi(x,y)\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p>The marginal density function for <span class="math inline">\(X\)</span> is <span class="math inline">\(\pi(x) = \int \pi(x,y)dy\)</span>.</p></li>
<li><p>The conditional density function of <span class="math inline">\(Y\mid X\)</span> is <span class="math inline">\(\pi(y\mid x) = \pi(x,y)/\pi(x)\)</span>, although it is actually the density function of <span class="math inline">\(Y\mid X = x\)</span>. If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent <span class="math inline">\(\pi(y \mid x) = \pi(y)\)</span>.</p></li>
<li><p>We can factorise a joint density function in different ways since <span class="math inline">\(\pi(x,y) = \pi(x)\pi(y\mid x) = \pi(x\mid y)\pi(y)\)</span>.</p></li>
<li><p>Combining 1, 2 and 3, we have the partition theorem, also known as law of total probability <span class="math inline">\(\pi(x) = \int\pi(x\mid y)\pi(y) dy\)</span>.</p></li>
<li><p>An extension to 4 is that suppose <span class="math inline">\(Z\)</span> is another random variable, then we have
<span class="math display">\[
\pi(y \mid x) = \int \pi(y \mid x, z) \, \pi(z\mid x)dz.
\]</span>
If <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> are conditionally independent given <span class="math inline">\(Z\)</span> then <span class="math inline">\(\pi(y \mid x, z) = \pi(y \mid z)\)</span>.</p></li>
</ol>
<p>For a sequence of random variables <span class="math inline">\(X_1,\dotsc,X_n\)</span>, we can factorise the joint density function in many ways, e.g. 
<span class="math display">\[
\pi(x_1,\dotsc,x_n) = \pi(x_1\mid x_2,\dotsc,x_n)\pi(x_2,x_3\mid x_4,\dotsc,x_n)\pi(x_4,\dotsc,x_n).
\]</span>
In practice, we may wish to factorise the joint density in a way that exploits the conditionally independence structure between variables.</p>
<p>We say <span class="math inline">\(X_1,\dotsc,X_n\)</span> are conditionally independent and identically distributed given some random variable <span class="math inline">\(Y\)</span> if
<span class="math display">\[
\pi(x_1,\dotsc,x_n\mid y) = \prod_{i=1}^n \pi(x_i\mid y).
\]</span></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-10" class="definition"><strong>Definition 1.8  </strong></span>Let <span class="math inline">\(\pi(y_1, \ldots, y_N)\)</span> be the joint density of <span class="math inline">\(Y_1, \ldots, Y_N\)</span>. If <span class="math inline">\(\pi(y_1, \ldots, y_N) = \pi(y_{\sigma_1}, \ldots, y_{\sigma_N})\)</span> for any permutations <span class="math inline">\(\sigma\)</span> of <span class="math inline">\(\{1, \ldots, N\}\)</span>, then <span class="math inline">\(Y_1, \ldots, Y_N\)</span> are <strong>exchangeable</strong>.</p>
</div>
<p>Exchangeability means that the labels of the random variables don’t contain any information about the outcomes. This is an important idea in many areas of probability and statistics, and it is a weaker assumption compared to saying <span class="math inline">\(Y_1,\dotsc,Y_N\)</span> are independent.</p>
<div class="example">
<p><span id="exm:unlabeled-div-11" class="example"><strong>Example 1.3  </strong></span>If <span class="math inline">\(Y_i \sim \text{Bin}(n, p)\)</span> are independent and identically distributed for <span class="math inline">\(i = 1, 2, 3\)</span>, then <span class="math inline">\(\pi(y_1, y_2, y_3) = \pi(y_3, y_1, y_2)\)</span>. Therefore, independence implies exchangeability.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-12" class="example"><strong>Example 1.4  </strong></span>Let <span class="math inline">\((X, Y)\)</span> follow a bivariate normal distribution with mean <strong>0</strong>, variances <span class="math inline">\(\sigma_x^2 = \sigma_y^2 = 1\)</span> and a correlation parameter <span class="math inline">\(\rho \in [-1, 1]\)</span>. <span class="math inline">\((X, Y)\)</span> are exchangeable, but only independent if <span class="math inline">\(\rho = 0\)</span>.</p>
</div>
<div class="proposition">
<p><span id="prp:unlabeled-div-13" class="proposition"><strong>Proposition 1.1  </strong></span>If <span class="math inline">\(\theta \sim \pi(\theta)\)</span> and <span class="math inline">\((Y_1, \ldots, Y_N)\)</span> are conditionally iid given the random variable <span class="math inline">\(\theta\)</span>, then marginally <span class="math inline">\(Y_1, \ldots, Y_N\)</span> are exchangeable.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-14" class="proof"><em>Proof</em>. </span>Suppose <span class="math inline">\((Y_1, \ldots, Y_N)\)</span> are conditionally iid given some parameter <span class="math inline">\(\theta\)</span>. Then for any permutation <span class="math inline">\(\sigma\)</span> of <span class="math inline">\(\{1, \ldots, N\}\)</span> and observations <span class="math inline">\(\{y_1, \ldots, y_N\}\)</span>
<span class="math display">\[\begin{equation}
\begin{split}
\pi(y_1, \ldots, y_N) &amp;= \int \pi(y_1, \ldots, y_N \mid \theta) \pi(\theta)\, d\theta \qquad \\
&amp; = \int \left\{\prod_{i=1}^N\pi(y_i \mid \theta)\right\} \pi(\theta)\, d\theta \qquad \textrm{(conditionally iid)}\\
&amp; = \int \left\{\prod_{i=1}^N\pi(y_{\sigma_i} \mid \theta)\right\} \pi(\theta)\, d\theta \qquad \textrm{(product is commutative)} \\
&amp; = \pi(y_{\sigma_1}, \ldots, y_{\sigma_N}) \qquad
\end{split}
\end{equation}\]</span></p>
</div>
<p>This tells us that if we have some conditionally iid random variables and a subjective prior belief about some parameter <span class="math inline">\(\theta\)</span>, then we have exchangeability. This is nice to have, but the implication in the other direction is much more interesting and powerful.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-15" class="theorem"><strong>Theorem 1.1  (de Finetti, informal) </strong></span>If a sequence of random variables <span class="math inline">\((Y_1, \ldots, Y_N)\)</span> is exchangeable, then its joint distribution can be written as
<span class="math display">\[
\pi(y_1, \ldots, y_N) = \int \left\{\prod_{i=1}^N\pi(y_i \mid \theta)\right\} \pi(\theta)\, d\theta
\]</span>
for some parameter <span class="math inline">\(\theta\)</span>, some distribution on <span class="math inline">\(\theta\)</span>, and some sampling model <span class="math inline">\(\pi(y_i \mid \theta)\)</span>.</p>
</div>
<p>This is a kind of existence theorem for Bayesian inference. It says that if we have exchangeable random varibales, then a parameter <span class="math inline">\(\theta\)</span> must exist and a subjective probability distribution <span class="math inline">\(\pi(\theta)\)</span> must also exist. The argument against Bayesian inference is that it doesn’t guarantee a <em>good</em> subjective probability distribution <span class="math inline">\(\pi(\theta)\)</span> exists.</p>
</div>
<div id="bayes-theorem" class="section level2 hasAnchor" number="1.4">
<h2><span class="header-section-number">1.4</span> Bayes’ Theorem<a href="fundamentals.html#bayes-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Now we have an understanding of conditional probability and exchangeability, we can put these two together to understand Bayes’ Theorem. Bayes’ theorem is concerned with the distribution of the parameter <span class="math inline">\(\theta\)</span> given some observed data <span class="math inline">\(y\)</span>. It tries to answer the question: what does the data tell us about the model parameters?</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-16" class="theorem"><strong>Theorem 1.2  (Bayes) </strong></span>The conditional distribution of <span class="math inline">\(\theta\mid Y\)</span> has density
<span class="math display">\[
\pi(\theta \mid y) = \frac{\pi(y \mid \theta)\pi(\theta)}{\pi(y)}
\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-17" class="proof"><em>Proof</em>. </span><span class="math display">\[\begin{align}
\pi(\theta \mid y) &amp;= \frac{\pi(\theta, y)}{\pi(y)}\\
\implies \pi(\theta, y) &amp;= \pi(\theta \mid y)\pi(y)
\end{align}\]</span>
Analogously, using <span class="math inline">\(\pi(y \mid \theta)\)</span> we can derive
<span class="math display">\[
\pi(\theta, y) = \pi(y \mid \theta)\pi(\theta)
\]</span>
Putting these two terms equal to each other and dividing by <span class="math inline">\(\pi(y)\)</span> gives
<span class="math display">\[
\pi(\theta \mid y) = \frac{\pi(y \mid \theta)\pi(\theta)}{\pi(y)}
\]</span></p>
</div>
<p>There are four terms in Bayes’ theorem:</p>
<ol style="list-style-type: decimal">
<li>The <strong>posterior distribution</strong> <span class="math inline">\(\pi(\theta \mid y)\)</span>. This tells us our belief about the model parameter <span class="math inline">\(\theta\)</span> given the data we have observed <span class="math inline">\(y\)</span>.</li>
<li>The <strong>likelihood function</strong> <span class="math inline">\(\pi(y \mid \theta)\)</span>. The likelihood function is common to both frequentist and Bayesian methods.</li>
<li>The <strong>prior distribution</strong> <span class="math inline">\(\pi(\theta)\)</span>. This is the distribution that describes our prior beliefs about the value of <span class="math inline">\(\theta\)</span>. The form of <span class="math inline">\(\theta\)</span> should be decided <strong>before we see the data</strong>. It may be a vague distribution (e.g. <span class="math inline">\(\theta \sim N(0, 10^2)\)</span>) or a specific distribution based on prior information from experts (e.g. <span class="math inline">\(\theta \sim N(5.5, 1.3^2)\)</span>).<br />
</li>
<li>The <strong>evidence of the data</strong> <span class="math inline">\(\pi(y)\)</span>. This is sometimes called the average probability of the data or the marginal likelihood. In practice, we do not need to derive this term as it can be back computed to ensure the posterior distribution sums/integrates to one.</li>
</ol>
<p>A consequence of point four is that posterior distributions are usually derived proportionality, and (up to proportionality) Bayes’ theorem says
<span class="math display">\[
\pi(\theta \mid y) \propto \pi(y\mid\theta)\pi(\theta).
\]</span>
For simple distributions, knowing the density up to constant is sufficient to identify the distributions, e.g. </p>
<ol style="list-style-type: decimal">
<li><p>If <span class="math inline">\(\theta \in \mathbb{R}\)</span> has density <span class="math inline">\(\pi(\theta) \propto\exp(-\theta^2/2)\)</span> then <span class="math inline">\(\theta \sim N(0,1)\)</span></p></li>
<li><p>If <span class="math inline">\(\theta\in \{1,2,3\}\)</span> with density (or proabibilty mass function) <span class="math inline">\(\pi(1)\propto 2, \pi(2) \propto 4, \pi(3) \propto 7\)</span>, then since <span class="math inline">\(\pi(1)+\pi(2)+\pi(3) = 1\)</span>, it must be the case <span class="math inline">\(\pi(1) = 2/13,\pi(2) = 4/13,\pi(3) = 7/13\)</span>.</p></li>
</ol>
<p>For more complicated distributions, even if we only know them up to some constant, which may be very difficult to compute, we can still sample from such distributions using MCMC algorithms.</p>
<blockquote>
<p><strong>Some history of Thomas Bayes</strong>. Thomas Bayes was an English theologean born in 1702. His “Essay towards solving a problem in the doctrine of chances” was published posthumously. It introduces theroems on conditional probability and the idea of prior probability. He discusses an experiment where the data can be modelled using the Binomial distribution and he guesses (places a prior distribution) on the probability of success.</p>
</blockquote>
<blockquote>
<p>Richard Price sent Bayes’ work to the Royal Society two years after Bayes had died. In his commentary on Bayes’ work, he suggested that the Bayesian way of thinking proves the existance of God, stating: The purpose I mean is, to show what reason we have for believing that there are in the constitution of things fixt laws according to which things happen, and that, therefore, the frame of the world must be the effect of the wisdom and power of an intelligent cause; and thus to confirm the argument taken from final causes for the existence of the Deity.</p>
</blockquote>
<blockquote>
<p>It’s not clear how Bayesian Thomas Bayes actually was, as his work was mainly about specific forms of probability theory and not his intepretation of it. The Bayesian way of thinking was really popularised by Laplace, who wrote about deductive probability in the early 19th century.</p>
</blockquote>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="programming-in-r.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
