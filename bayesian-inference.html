<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Bayesian Inference | Bayesian Inference and Computation</title>
  <meta name="description" content="This book contains the lecture notes for the module Bayesian Inference and Computation." />
  <meta name="generator" content="bookdown 0.42 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Bayesian Inference | Bayesian Inference and Computation" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="/uob_logo.png" />
  <meta property="og:description" content="This book contains the lecture notes for the module Bayesian Inference and Computation." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Bayesian Inference | Bayesian Inference and Computation" />
  
  <meta name="twitter:description" content="This book contains the lecture notes for the module Bayesian Inference and Computation." />
  <meta name="twitter:image" content="/uob_logo.png" />

<meta name="author" content="Dr Mengchu Li and Dr Lukas Trottner (based on lecture notes by Dr Rowland Seymour)" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="programming-in-r.html"/>
<link rel="next" href="sampling.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Inference and Computation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Practicalities</a>
<ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#module-aims"><i class="fa fa-check"></i><b>0.1</b> Module Aims</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#module-structure"><i class="fa fa-check"></i><b>0.2</b> Module Structure</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#assessment"><i class="fa fa-check"></i><b>0.3</b> Assessment</a></li>
<li class="chapter" data-level="0.4" data-path="index.html"><a href="index.html#recommended-books-and-videos"><i class="fa fa-check"></i><b>0.4</b> Recommended Books and Videos</a></li>
<li class="chapter" data-level="0.5" data-path="index.html"><a href="index.html#common-distributions"><i class="fa fa-check"></i><b>0.5</b> Common Distributions</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="fundamentals.html"><a href="fundamentals.html"><i class="fa fa-check"></i><b>1</b> Fundamentals of Bayesian Inference</a>
<ul>
<li class="chapter" data-level="1.1" data-path="fundamentals.html"><a href="fundamentals.html#statistical-inference"><i class="fa fa-check"></i><b>1.1</b> Statistical Inference</a></li>
<li class="chapter" data-level="1.2" data-path="fundamentals.html"><a href="fundamentals.html#frequentist-theory"><i class="fa fa-check"></i><b>1.2</b> Frequentist Theory</a></li>
<li class="chapter" data-level="1.3" data-path="fundamentals.html"><a href="fundamentals.html#bayesian-paradigm"><i class="fa fa-check"></i><b>1.3</b> Bayesian Paradigm</a></li>
<li class="chapter" data-level="1.4" data-path="fundamentals.html"><a href="fundamentals.html#bayes-theorem"><i class="fa fa-check"></i><b>1.4</b> Bayes’ Theorem</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="programming-in-r.html"><a href="programming-in-r.html"><i class="fa fa-check"></i><b>2</b> Programming in R</a>
<ul>
<li class="chapter" data-level="2.1" data-path="programming-in-r.html"><a href="programming-in-r.html#random-numbers-for-loops-and-r"><i class="fa fa-check"></i><b>2.1</b> Random Numbers, For Loops and R</a></li>
<li class="chapter" data-level="2.2" data-path="programming-in-r.html"><a href="programming-in-r.html#functions-in-r"><i class="fa fa-check"></i><b>2.2</b> Functions in R</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="programming-in-r.html"><a href="programming-in-r.html#built-in-commands"><i class="fa fa-check"></i><b>2.2.1</b> Built in commands</a></li>
<li class="chapter" data-level="2.2.2" data-path="programming-in-r.html"><a href="programming-in-r.html#user-defined-functions"><i class="fa fa-check"></i><b>2.2.2</b> User defined functions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="programming-in-r.html"><a href="programming-in-r.html#good-coding-practices"><i class="fa fa-check"></i><b>2.3</b> Good Coding Practices</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="programming-in-r.html"><a href="programming-in-r.html#code-style"><i class="fa fa-check"></i><b>2.3.1</b> Code Style</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bayesian-inference.html"><a href="bayesian-inference.html"><i class="fa fa-check"></i><b>3</b> Bayesian Inference</a>
<ul>
<li class="chapter" data-level="3.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#reporting-conclusions-from-bayesian-inference"><i class="fa fa-check"></i><b>3.1</b> Reporting Conclusions from Bayesian Inference</a></li>
<li class="chapter" data-level="3.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#conjugate-prior-and-analysis"><i class="fa fa-check"></i><b>3.2</b> Conjugate Prior and Analysis</a></li>
<li class="chapter" data-level="3.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#hierarchical-models"><i class="fa fa-check"></i><b>3.3</b> Hierarchical Models</a></li>
<li class="chapter" data-level="3.4" data-path="bayesian-inference.html"><a href="bayesian-inference.html#prediction"><i class="fa fa-check"></i><b>3.4</b> Prediction</a></li>
<li class="chapter" data-level="3.5" data-path="bayesian-inference.html"><a href="bayesian-inference.html#non-informative-prior-distibrutions"><i class="fa fa-check"></i><b>3.5</b> Non-informative Prior Distibrutions</a></li>
<li class="chapter" data-level="3.6" data-path="bayesian-inference.html"><a href="bayesian-inference.html#bernstein-von-mises-theorem"><i class="fa fa-check"></i><b>3.6</b> Bernstein-von-Mises Theorem</a></li>
<li class="chapter" data-level="3.7" data-path="bayesian-inference.html"><a href="bayesian-inference.html#lab"><i class="fa fa-check"></i><b>3.7</b> Lab</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="sampling.html"><a href="sampling.html"><i class="fa fa-check"></i><b>4</b> Sampling</a>
<ul>
<li class="chapter" data-level="4.1" data-path="sampling.html"><a href="sampling.html#uniform-random-numbers"><i class="fa fa-check"></i><b>4.1</b> Uniform Random Numbers</a></li>
<li class="chapter" data-level="4.2" data-path="sampling.html"><a href="sampling.html#inverse-transform-sampling"><i class="fa fa-check"></i><b>4.2</b> Inverse Transform Sampling</a></li>
<li class="chapter" data-level="4.3" data-path="sampling.html"><a href="sampling.html#rejection-sampling"><i class="fa fa-check"></i><b>4.3</b> Rejection Sampling</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="sampling.html"><a href="sampling.html#rejection-sampling-efficiency"><i class="fa fa-check"></i><b>4.3.1</b> Rejection Sampling Efficiency</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="sampling.html"><a href="sampling.html#ziggurat-sampling"><i class="fa fa-check"></i><b>4.4</b> Ziggurat Sampling</a></li>
<li class="chapter" data-level="4.5" data-path="sampling.html"><a href="sampling.html#approximate-bayesian-computation"><i class="fa fa-check"></i><b>4.5</b> Approximate Bayesian Computation</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="sampling.html"><a href="sampling.html#abc-with-rejection"><i class="fa fa-check"></i><b>4.5.1</b> ABC with Rejection</a></li>
<li class="chapter" data-level="4.5.2" data-path="sampling.html"><a href="sampling.html#summary-abc-with-rejection"><i class="fa fa-check"></i><b>4.5.2</b> Summary ABC with Rejection</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="sampling.html"><a href="sampling.html#lab-1"><i class="fa fa-check"></i><b>4.6</b> Lab</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html"><i class="fa fa-check"></i><b>5</b> Markov Chain Monte Carlo</a>
<ul>
<li class="chapter" data-level="5.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#properties-of-markov-chains"><i class="fa fa-check"></i><b>5.1</b> Properties of Markov Chains</a></li>
<li class="chapter" data-level="5.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#metropolishastings"><i class="fa fa-check"></i><b>5.2</b> Metropolis–Hastings</a></li>
<li class="chapter" data-level="5.3" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#gibbs-sampler"><i class="fa fa-check"></i><b>5.3</b> Gibbs Sampler</a></li>
<li class="chapter" data-level="5.4" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#metropolis-within-gibbs"><i class="fa fa-check"></i><b>5.4</b> Metropolis-within-Gibbs</a></li>
<li class="chapter" data-level="5.5" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#mcmc-diagnostics"><i class="fa fa-check"></i><b>5.5</b> MCMC Diagnostics</a></li>
<li class="chapter" data-level="5.6" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#beyond-mcmc"><i class="fa fa-check"></i><b>5.6</b> Beyond MCMC</a></li>
<li class="chapter" data-level="5.7" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#lab-2"><i class="fa fa-check"></i><b>5.7</b> Lab</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="advanced-computation.html"><a href="advanced-computation.html"><i class="fa fa-check"></i><b>6</b> Advanced Computation</a>
<ul>
<li class="chapter" data-level="6.1" data-path="advanced-computation.html"><a href="advanced-computation.html#gaussian-processes"><i class="fa fa-check"></i><b>6.1</b> Gaussian Processes</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="advanced-computation.html"><a href="advanced-computation.html#covariance-functions"><i class="fa fa-check"></i><b>6.1.1</b> Covariance Functions</a></li>
<li class="chapter" data-level="6.1.2" data-path="advanced-computation.html"><a href="advanced-computation.html#gaussian-process-regression"><i class="fa fa-check"></i><b>6.1.2</b> Gaussian Process Regression</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="advanced-computation.html"><a href="advanced-computation.html#data-augmentation"><i class="fa fa-check"></i><b>6.2</b> Data Augmentation</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="advanced-computation.html"><a href="advanced-computation.html#imputing-censored-observations"><i class="fa fa-check"></i><b>6.2.1</b> Imputing censored observations</a></li>
<li class="chapter" data-level="6.2.2" data-path="advanced-computation.html"><a href="advanced-computation.html#imputing-latent-variables"><i class="fa fa-check"></i><b>6.2.2</b> Imputing Latent Variables</a></li>
<li class="chapter" data-level="6.2.3" data-path="advanced-computation.html"><a href="advanced-computation.html#grouped-data"><i class="fa fa-check"></i><b>6.2.3</b> Grouped Data</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="advanced-computation.html"><a href="advanced-computation.html#prior-ellicitation"><i class="fa fa-check"></i><b>6.3</b> Prior Ellicitation</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="advanced-computation.html"><a href="advanced-computation.html#prior-summaries"><i class="fa fa-check"></i><b>6.3.1</b> Prior Summaries</a></li>
<li class="chapter" data-level="6.3.2" data-path="advanced-computation.html"><a href="advanced-computation.html#betting-with-histograms"><i class="fa fa-check"></i><b>6.3.2</b> Betting with Histograms</a></li>
<li class="chapter" data-level="6.3.3" data-path="advanced-computation.html"><a href="advanced-computation.html#prior-intervals"><i class="fa fa-check"></i><b>6.3.3</b> Prior Intervals</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="advanced-computation.html"><a href="advanced-computation.html#lab-3"><i class="fa fa-check"></i><b>6.4</b> Lab</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="advanced-computation.html"><a href="advanced-computation.html#gaussian-processes-1"><i class="fa fa-check"></i><b>6.4.1</b> Gaussian Processes</a></li>
<li class="chapter" data-level="6.4.2" data-path="advanced-computation.html"><a href="advanced-computation.html#missing-data"><i class="fa fa-check"></i><b>6.4.2</b> Missing Data</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Inference and Computation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bayesian-inference" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Chapter 3</span> Bayesian Inference<a href="bayesian-inference.html#bayesian-inference" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>We start this chapter with two basic examples that only have one data point. They illustrate the point of prior distributions and motivate our discussions on conjugate priors later.</p>
<div class="example">
<p><span id="exm:unlabeled-div-1" class="example"><strong>Example 3.1  </strong></span>Suppose we have a model <span class="math inline">\(Y \mid \theta \sim N(\theta, 1)\)</span> and we want to derive the posterior distribution <span class="math inline">\(\pi(\theta\mid y)\)</span>. By Bayes’ theorem,
<span class="math display">\[
\pi(\theta \mid y) \propto \pi(y \mid \theta) \pi(\theta).
\]</span>
We know the form of <span class="math inline">\(\pi(y \mid \theta) = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(y - \theta)^2}\)</span>, but how should we describe our prior beliefs about <span class="math inline">\(\theta\)</span>? Here are three options:</p>
<ol style="list-style-type: decimal">
<li><p>We can be very vague about <span class="math inline">\(\theta\)</span> – we genuinely don’t know about its value. We assign a uniform prior distribution to <span class="math inline">\(\theta\)</span> that takes values between -1,000 and +1,000, i.e. <span class="math inline">\(\theta \sim U[-1000, 1000]\)</span>. We can write explicitly its distribution as
<span class="math display">\[
\pi(\theta) =  \begin{cases}
                 \frac{1}{2000}&amp; q \in [-1000, 1000] \\
                 0 &amp; \textrm{otherwise.}
             \end{cases}
\]</span>
Up to proportionality/constant, we have <span class="math inline">\(\pi(\theta) \propto 1\)</span> for <span class="math inline">\(\theta \in [-1000, 1000]\)</span>.</p></li>
<li><p>After thinking hard about the problem, or talking to an expert, we decide that the only thing we know about <span class="math inline">\(\theta\)</span> is that it can’t be negative. We adjust our prior distribution from 1. to be <span class="math inline">\(\theta \sim U[0, 1000]\)</span>. Up to proportionality <span class="math inline">\(\pi(\theta) \propto 1\)</span> for <span class="math inline">\(\theta \in [0, 1000]\)</span>.</p></li>
<li><p>We decide to talk to a series of experts about <span class="math inline">\(\theta\)</span> asking for their views on likely values of <span class="math inline">\(\theta\)</span>. Averaging the experts opinions gives <span class="math inline">\(\theta \sim N(3, 0.7^2)\)</span>. This is a method known as prior elicitation.</p></li>
</ol>
<p>We now go and observe some data. After a lot of time and effort, we collect one data point: <span class="math inline">\(y = 0\)</span>.</p>
<p>Now we have all the ingredients to construct the posterior distribution. We multiply the likelihood function evaluated at <span class="math inline">\(y = 0\)</span> by each of the three prior distributions. This gives us the posterior distributions. These are</p>
<ol style="list-style-type: decimal">
<li>For the first uniform prior distribution, the posterior distribution is <span class="math inline">\(\pi(\theta \mid {y}) \propto \exp\left(-\frac{1}{2}\theta^2\right)\)</span> for <span class="math inline">\(\theta \in [-1000, 1000]\)</span>.<br />
</li>
<li>For the second uniform prior distribution, the posterior distribution is <span class="math inline">\(\pi(\theta \mid {y}) \propto \exp\left(-\frac{1}{2}\theta^2\right)\)</span> for <span class="math inline">\(\theta \in [0, 1000]\)</span>.</li>
<li>For the normal prior distribution, the posterior distribution is <span class="math inline">\(\pi(\theta \mid {y}) \propto \exp\left(-\frac{1}{2}\theta^2\right)\exp\left(-\frac{1}{2}\left(\frac{\theta - 3}{0.7}\right)^2\right)\)</span>. Combining like terms, we have <span class="math inline">\(\pi(\theta \mid {y}) \propto \exp\left(-\frac{1}{2}\left(\frac{1.49\theta^2 - 6\theta}{0.7^2}\right)\right)\)</span> for <span class="math inline">\(\theta \in \mathbb{R}\)</span>. By further completing the square and comparing to the normal density, one can see the posterior distribution is actually normal with mean <span class="math inline">\(300/149 \approx 2\)</span> and variance <span class="math inline">\(49/149 \approx 0.33\)</span>.</li>
</ol>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="bayesian-inference.html#cb1-1" tabindex="-1"></a><span class="co">#The likelihood function is the normal PDF</span></span>
<span id="cb1-2"><a href="bayesian-inference.html#cb1-2" tabindex="-1"></a><span class="co">#To illustrate this, we evaluate this from [-5, 5].</span></span>
<span id="cb1-3"><a href="bayesian-inference.html#cb1-3" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="fl">0.01</span>)</span>
<span id="cb1-4"><a href="bayesian-inference.html#cb1-4" tabindex="-1"></a>likelihood <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(x, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb1-5"><a href="bayesian-inference.html#cb1-5" tabindex="-1"></a></span>
<span id="cb1-6"><a href="bayesian-inference.html#cb1-6" tabindex="-1"></a><span class="co">#The first prior distribution we try is a </span></span>
<span id="cb1-7"><a href="bayesian-inference.html#cb1-7" tabindex="-1"></a><span class="co">#uniform [-1000, 1000] distribution. This is a </span></span>
<span id="cb1-8"><a href="bayesian-inference.html#cb1-8" tabindex="-1"></a><span class="co">#vague prior distribution. </span></span>
<span id="cb1-9"><a href="bayesian-inference.html#cb1-9" tabindex="-1"></a>uniform.prior <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">1</span>, <span class="fu">length</span>(x))</span>
<span id="cb1-10"><a href="bayesian-inference.html#cb1-10" tabindex="-1"></a>posterior1 <span class="ot">&lt;-</span> likelihood<span class="sc">*</span>uniform.prior</span>
<span id="cb1-11"><a href="bayesian-inference.html#cb1-11" tabindex="-1"></a></span>
<span id="cb1-12"><a href="bayesian-inference.html#cb1-12" tabindex="-1"></a></span>
<span id="cb1-13"><a href="bayesian-inference.html#cb1-13" tabindex="-1"></a><span class="co">#The second prior distribution we try is a uniform </span></span>
<span id="cb1-14"><a href="bayesian-inference.html#cb1-14" tabindex="-1"></a><span class="co">#[0, 1000] distribution, i.e. theta is non-negative. </span></span>
<span id="cb1-15"><a href="bayesian-inference.html#cb1-15" tabindex="-1"></a>step.prior <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(x <span class="sc">&gt;=</span> <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb1-16"><a href="bayesian-inference.html#cb1-16" tabindex="-1"></a>posterior2 <span class="ot">&lt;-</span> likelihood<span class="sc">*</span>step.prior</span>
<span id="cb1-17"><a href="bayesian-inference.html#cb1-17" tabindex="-1"></a></span>
<span id="cb1-18"><a href="bayesian-inference.html#cb1-18" tabindex="-1"></a></span>
<span id="cb1-19"><a href="bayesian-inference.html#cb1-19" tabindex="-1"></a><span class="co">#The third prior distribution we try is a</span></span>
<span id="cb1-20"><a href="bayesian-inference.html#cb1-20" tabindex="-1"></a><span class="co">#specific normal prior distribution. It</span></span>
<span id="cb1-21"><a href="bayesian-inference.html#cb1-21" tabindex="-1"></a><span class="co">#has mean 3 and variance 0.7.</span></span>
<span id="cb1-22"><a href="bayesian-inference.html#cb1-22" tabindex="-1"></a>normal.prior <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(x, <span class="at">mean =</span> <span class="dv">3</span>, <span class="at">sd =</span> <span class="fl">0.7</span>)</span>
<span id="cb1-23"><a href="bayesian-inference.html#cb1-23" tabindex="-1"></a>posterior3 <span class="ot">&lt;-</span> likelihood<span class="sc">*</span>normal.prior</span>
<span id="cb1-24"><a href="bayesian-inference.html#cb1-24" tabindex="-1"></a></span>
<span id="cb1-25"><a href="bayesian-inference.html#cb1-25" tabindex="-1"></a><span class="co">#Now we plot the likelihoods, prior and posterior distributions. </span></span>
<span id="cb1-26"><a href="bayesian-inference.html#cb1-26" tabindex="-1"></a><span class="co">#Each row corresponds to a different prior distribution. Each</span></span>
<span id="cb1-27"><a href="bayesian-inference.html#cb1-27" tabindex="-1"></a><span class="co">#column corresponds to a part in Bayes&#39; theorem. </span></span>
<span id="cb1-28"><a href="bayesian-inference.html#cb1-28" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">3</span>))</span>
<span id="cb1-29"><a href="bayesian-inference.html#cb1-29" tabindex="-1"></a><span class="fu">plot</span>(x, likelihood, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="at">yaxt =</span> <span class="st">&quot;n&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Likelihood&quot;</span>)</span>
<span id="cb1-30"><a href="bayesian-inference.html#cb1-30" tabindex="-1"></a><span class="fu">plot</span>(x, uniform.prior, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, <span class="at">yaxt =</span> <span class="st">&quot;n&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Prior&quot;</span>)</span>
<span id="cb1-31"><a href="bayesian-inference.html#cb1-31" tabindex="-1"></a><span class="fu">plot</span>(x, posterior1, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, <span class="at">yaxt =</span> <span class="st">&quot;n&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Posterior&quot;</span>)</span>
<span id="cb1-32"><a href="bayesian-inference.html#cb1-32" tabindex="-1"></a><span class="fu">plot</span>(x, likelihood, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="at">yaxt =</span> <span class="st">&quot;n&quot;</span>)</span>
<span id="cb1-33"><a href="bayesian-inference.html#cb1-33" tabindex="-1"></a><span class="fu">plot</span>(x, step.prior, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, <span class="at">yaxt =</span> <span class="st">&quot;n&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb1-34"><a href="bayesian-inference.html#cb1-34" tabindex="-1"></a><span class="fu">plot</span>(x, posterior2, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, <span class="at">yaxt =</span> <span class="st">&quot;n&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb1-35"><a href="bayesian-inference.html#cb1-35" tabindex="-1"></a><span class="fu">plot</span>(x, likelihood, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="at">yaxt =</span> <span class="st">&quot;n&quot;</span>)</span>
<span id="cb1-36"><a href="bayesian-inference.html#cb1-36" tabindex="-1"></a><span class="fu">plot</span>(x, normal.prior, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, <span class="at">yaxt =</span> <span class="st">&quot;n&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb1-37"><a href="bayesian-inference.html#cb1-37" tabindex="-1"></a><span class="fu">plot</span>(x, posterior3, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, <span class="at">yaxt =</span> <span class="st">&quot;n&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/prior-posterior-grid2-1.png" width="672" /></p>
<ol style="list-style-type: decimal">
<li><p>The posterior distribution is proportional to the likelihood function. The posterior distribution closely matches frequentist inference. Both the MLE and posterior mean are 0.</p></li>
<li><p>We get a lopsided posterior distribution, that is proportional to the likelihood function for positive values of <span class="math inline">\(\theta\)</span>, but is 0 for negative values of <span class="math inline">\(\theta\)</span>.</p></li>
<li><p>We get some sort of average of the likelihood function and the prior distribution.<br />
</p></li>
</ol>
</div>
<div class="example">
<p><span id="exm:binom" class="example"><strong>Example 3.2  (Binomial likelihood) </strong></span>A social media company wants to determine how many of its users are bots. A software engineer collects a random sample of 200 accounts and finds that three are bots. Assuming that any two accounts being a bot are independent of one another, she decides to model the outcome as <span class="math inline">\(Y\mid \theta \sim \text{Bin}(200,\theta)\)</span>, and the observation is <span class="math inline">\(y = 3\)</span>.</p>
<p>By Bayes’ theorem, we have
<span class="math display">\[
\pi(\theta \mid {y}) \propto \pi({y}\mid \theta) \pi(\theta).
\]</span></p>
<p><strong>Likelihood function</strong> <span class="math inline">\(\pi({y}\mid \theta)\)</span>. The Binomial likelihood function is given by
<span class="math display">\[
\pi({y}\mid \theta) = \begin{pmatrix} 200 \\ 3 \end{pmatrix} \theta^3(1-\theta)^{197} \propto \theta^3(1-\theta)^{197}.
\]</span></p>
<p><strong>Prior distribution</strong> <span class="math inline">\(\pi(\theta)\)</span>. We now need to describe our prior beliefs about <span class="math inline">\(\theta\)</span>. We have no reason to suggest <span class="math inline">\(\theta\)</span> takes any specific value, so we use a uniform prior distribution <span class="math inline">\(\theta \sim U[0, 1]\)</span>, where <span class="math inline">\(\pi(\theta) = 1\)</span> for <span class="math inline">\(\theta \in [0, 1]\)</span>.</p>
<p><strong>Posterior distribution</strong> <span class="math inline">\(\pi(\theta \mid {y})\)</span>. We can now derive the posterior distribution up to proportionality
<span class="math display">\[
\pi(\theta \mid {y}) \propto \theta^3(1-\theta)^{197}.
\]</span>
This functional dependence on <span class="math inline">\(\theta\)</span> identifies that the posterior distribution <span class="math inline">\(\pi(\theta \mid {y})\)</span> is a Beta distribution. Recall that the density function for the Beta distribution with shape parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> is
<span class="math display">\[
\pi(x \mid \alpha, \beta) = \frac{1}{B(\alpha,\beta)}x^{\alpha - 1}(1-x)^{\beta - 1}, \qquad x\in(0,1).
\]</span>
Therefore, the posterior distribution is <span class="math inline">\(\textrm{Beta}(4, 198)\)</span>. We also note that the uniform distribution on <span class="math inline">\([0,1]\)</span>, <span class="math inline">\(U[0, 1]\)</span>, is a special case of Beta distribution with <span class="math inline">\(\alpha = 1\)</span> and <span class="math inline">\(\beta = 1\)</span>.</p>
</div>
<div id="reporting-conclusions-from-bayesian-inference" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Reporting Conclusions from Bayesian Inference<a href="bayesian-inference.html#reporting-conclusions-from-bayesian-inference" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Posterior distribution <span class="math inline">\(\pi(\theta \mid y)\)</span> summaries all the information and uncertainty regarding the parameter <span class="math inline">\(\theta\)</span>, given the data <span class="math inline">\(y\)</span>. However, to make inference or answer questions, we usually need to summaries the distribution in some way.</p>
<p>We could consider reporting point estimates of <span class="math inline">\(\theta\)</span>. To give a specific estimates of <span class="math inline">\(\theta\)</span>, we may use <strong>posterior mean</strong>, i.e. <span class="math inline">\(\mathbb{E}(\theta\mid y) = \int \theta \, \pi(\theta \mid y) d\theta\)</span>, or <strong>posterior mode</strong>, defined as
<span class="math display">\[
\hat{\theta}(y) := \mathop{\mathrm{arg\,max}}_{\theta \in \Theta} \pi(\theta\mid y) = \mathop{\mathrm{arg\,max}}_{\theta \in \Theta} \pi( y \mid \theta) \pi(\theta).
\]</span>
The posterior mode is also known as <strong>maximum a posteriori (MAP)</strong> estimate. If <span class="math inline">\(\pi(\theta) \propto 1\)</span>, then <span class="math inline">\(\hat{\theta}_{\text{MAP}}(y) = \hat{\theta}_{\text{MLE}}(y)= \mathop{\mathrm{arg\,max}}_{\theta \in \Theta} \pi( y \mid \theta).\)</span></p>
<p>In the previous example, the posterior distribution is <span class="math inline">\(\textrm{Beta}(4, 198)\)</span>. The posterior mean is <span class="math inline">\(\frac{4}{198+4} = \frac{2}{101}\)</span> and the posterior mode is <span class="math inline">\(\frac{4-1}{4+198-2} = \frac{3}{200}\)</span>.</p>
<p>In addition to point estimates, it is important to share the uncertainty about <span class="math inline">\(\theta\)</span>. In the frequentist framework, this achieved via confidence intervals. The Bayesian analogue is called credible intervals.</p>
<div class="definition">
<p><span id="def:unlabeled-div-2" class="definition"><strong>Definition 3.1  </strong></span>An interval <span class="math inline">\([l,u]\)</span> is called a <strong>credible interval</strong> at level <span class="math inline">\(1-\alpha\)</span>, <span class="math inline">\(\alpha \in (0,1)\)</span>, for a random variable <span class="math inline">\(\theta \in \mathbb{R}\)</span> if
<span class="math display">\[
\pi(l \leq \theta \leq u\mid y) =  \int_{l}^u \pi(\theta\mid y) = 1-\alpha.
\]</span>
Although this definition does not identify a unique crediable interval at level <span class="math inline">\(1-\alpha\)</span>, the most common choice is choosing <span class="math inline">\(l\)</span> and <span class="math inline">\(u\)</span> such that
<span class="math inline">\(\pi(\theta &lt; l\mid y) = \pi(\theta &gt;u \mid y) = \alpha/2\)</span>. This way of specifying creaiable intervals is known as equal-tailed intervals.</p>
<p>Note that we can interpret a credible interval at level <span class="math inline">\(1-\alpha\)</span> as there is <span class="math inline">\(100(1-\alpha)\%\)</span> probability that <span class="math inline">\(\theta\)</span> belongs that particular interval given the data. The parameter <span class="math inline">\(\theta\)</span> is random and the interval is fixed in the definition of credible intervals. This is, in some sense, more intuitive than the interpretation of CIs, which relies on repeated sampling.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-3" class="example"><strong>Example 3.3  </strong></span>The 95% credible interval for the Binomial example is given by</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="bayesian-inference.html#cb2-1" tabindex="-1"></a>cred.int<span class="fl">.95</span> <span class="ot">&lt;-</span> <span class="fu">qbeta</span>(<span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>), <span class="dv">4</span>, <span class="dv">198</span>)</span>
<span id="cb2-2"><a href="bayesian-inference.html#cb2-2" tabindex="-1"></a><span class="fu">round</span>(cred.int<span class="fl">.95</span>, <span class="dv">3</span>)</span></code></pre></div>
<pre><code>## [1] 0.005 0.043</code></pre>
<p>This says that we believe there is a 95% chance that the probability of an account being a bot lies between 0.005 and 0.043.</p>
</div>
</div>
<div id="conjugate-prior-and-analysis" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Conjugate Prior and Analysis<a href="bayesian-inference.html#conjugate-prior-and-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We have seen from the two examples discussed so far</p>
<ol style="list-style-type: decimal">
<li>Normal Prior + Normal Likelihood <span class="math inline">\(\longrightarrow\)</span> Normal Posterior</li>
<li>Uniform Prior (Beta<span class="math inline">\((1,1)\)</span>) + Binomial Likelihood <span class="math inline">\(\longrightarrow\)</span> Beta Posterior</li>
</ol>
<p>Notice that in both cases, the prior and posterior distribution belong to the same family of distributions. In this case, we say the prior is conjugate with respect to the likelihood function.</p>
<div class="definition">
<p><span id="def:unlabeled-div-4" class="definition"><strong>Definition 3.2  (Conjugate Prior) </strong></span>For a given likelihood, <span class="math inline">\(\pi(y \mid \theta)\)</span>, if the prior distribution <span class="math inline">\(\pi(\theta)\)</span> and the posterior distribution <span class="math inline">\(\pi(\theta \mid {y})\)</span> are in the same family of distributions, then <span class="math inline">\(\pi(\theta)\)</span> is a conjugate prior/ is conjugate with respect to <span class="math inline">\(\pi(y \mid \theta)\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:exponential" class="example"><strong>Example 3.4  (Exponential likelihood) </strong></span>Suppose <span class="math inline">\(Y_1,\dotsc,Y_n\mid \lambda \overset{i.i.d.}{\sim} \mathrm{Exp}(\lambda)\)</span>, and consider a Gamma prior on the parameter <span class="math inline">\(\lambda \sim \text{Gamma}(\alpha,\beta)\)</span>. Let <span class="math inline">\(Y = (Y_1,\dotsc,Y_n).\)</span> By Bayes’ Theorem, we can derive the posterior distribution of <span class="math inline">\(\theta \mid Y\)</span> as
<span class="math display">\[\begin{align*}
\pi(\lambda\mid y) \propto \pi(y\mid \lambda) \pi(\lambda) &amp;= \prod_{i=1}^n\pi(y_i\mid \lambda)\pi(\lambda) \\
&amp;= \lambda^n e^{-\lambda\sum_{i=1}^n y_i}\pi(\lambda) \\
&amp; \propto \lambda^n  e^{-\lambda\sum_{i=1}^n y_i} \lambda^{\alpha-1}e^{-\beta \lambda}\\
&amp; = \lambda^{\alpha+n-1} e^{-\lambda (\beta+\sum_{i=1}^ny_i)}.
\end{align*}\]</span></p>
<p>This means that the posterior distribution is Gamma<span class="math inline">\((\alpha+n, \beta+\sum_{i=1}^ny_i)\)</span>. Therefore, we conclude that Gamma distribution is a conjugate prior for Exponential likelihood.</p>
<p>Using basic properties of Gamma distribution, we can then obtain
<span class="math display">\[
\mathbb{E}(\lambda\mid y) = \frac{\alpha+n}{\beta+\sum_{i=1}^ny_i}.
\]</span>
Note that the mean of the prior distribution is <span class="math inline">\(\mathbb{E}(\lambda) = \alpha/\beta\)</span> and the MLE for <span class="math inline">\(\lambda\)</span> is <span class="math inline">\(\hat{\lambda}_{\text{MLE}} = \frac{n}{\sum_{i=1}^ny_i}\)</span>. We can interpret the posterior mean as a weighted average of prior mean and the MLE by noticing
<span class="math display">\[
\mathbb{E}(\lambda\mid y) = \frac{\beta}{\beta+\sum_{i=1}^ny_i} \cdot \frac{\alpha}{\beta} + \frac{\sum_{i=1}^ny_i}{\beta+\sum_{i=1}^ny_i} \cdot \frac{n}{\sum_{i=1}^ny_i}.
\]</span>
As <span class="math inline">\(n \rightarrow \infty\)</span>, the weight on MLE approaches <span class="math inline">\(1\)</span>, and we would have <span class="math inline">\(\mathbb{E}(\lambda\mid y) \approx \hat{\lambda}_{\text{MLE}}\)</span>.</p>
<p>We discuss a <strong>real data example</strong> now, which also illustrates the effects of choosing different <span class="math inline">\(\beta\)</span> in the prior distribution. An insurance company wants to estimate the average time until a claim is made on a specific policy using Bayesian inference. The data <span class="math inline">\(\boldsymbol{y} = \{14, 10,  6,  7, 13,  9, 12,  7,  9,  8\}\)</span> are collected, where each entry represents the number of months until a claimed is made.</p>
<p><strong>Likelihood function</strong> The exponential distribution is a good way of modelling lifetimes or the length of time until an event happens. Therefore, the company decides to model the observed data as realizations from <span class="math inline">\(\text{Exp}(\lambda)\)</span>, where <span class="math inline">\(\lambda\)</span> represents the number of claims per month. Assuming all the claims are independent, the likelihood function is given by
<span class="math display">\[\begin{align*}
\pi(\boldsymbol{y} \mid \lambda) &amp;= \prod_{i=1}^{10} \lambda e^{-\lambda y_i} \\
&amp; = \lambda^{10}e^{-\lambda \sum_{i=1}^{10} y_i} \\
&amp; = \lambda^{10} e^{-95\lambda}.
\end{align*}\]</span></p>
<p><strong>Prior distribution</strong> <span class="math inline">\(\pi(\lambda)\)</span>. As we are modelling a rate parameter, we know it must be positive. We decide to use an exponential prior distribution for <span class="math inline">\(\lambda\)</span>, but leave the choice of the rate parameter up to the insurance professionals at the insurance company. The prior distribution is given by <span class="math inline">\(\lambda \sim \textrm{Exp}(\beta),\)</span> which is the same as <span class="math inline">\(\lambda \sim \text{Gamma}(1, \beta)\)</span>.</p>
<p><strong>Posterior distribution</strong> Using the formula we obtained before, the posterior distribution is <span class="math inline">\(\textrm{Gamma}(11, 95 + \beta)\)</span>. The posterior mean months until a claim is <span class="math inline">\(\frac{11}{95 + \beta}\)</span>.</p>
<p>We can see the effect of the choice of rate parameter <span class="math inline">\(\beta\)</span> in this mean. Small values of <span class="math inline">\(\beta\)</span> yield vague prior distribution, since <span class="math inline">\(\text{Var}(\lambda) = 1/\beta^2\)</span>, which plays a minimal role in the posterior distribution. Large values of <span class="math inline">\(\beta\)</span> result in specific prior distributions that contribute a lot to the posterior distribution. The plots below show the prior and posterior distributions for <span class="math inline">\(\beta = 0.01\)</span>, <span class="math inline">\(\beta = 50\)</span> and <span class="math inline">\(\beta = 150\)</span>.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="bayesian-inference.html#cb4-1" tabindex="-1"></a>plot.distributions <span class="ot">&lt;-</span> <span class="cf">function</span>(gamma.prior){</span>
<span id="cb4-2"><a href="bayesian-inference.html#cb4-2" tabindex="-1"></a>  <span class="co">#evaluate at selected values of lambda</span></span>
<span id="cb4-3"><a href="bayesian-inference.html#cb4-3" tabindex="-1"></a>  lambda <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.001</span>, <span class="fl">0.3</span>, <span class="fl">0.001</span>) </span>
<span id="cb4-4"><a href="bayesian-inference.html#cb4-4" tabindex="-1"></a>  </span>
<span id="cb4-5"><a href="bayesian-inference.html#cb4-5" tabindex="-1"></a>  <span class="co">#evaluate prior density</span></span>
<span id="cb4-6"><a href="bayesian-inference.html#cb4-6" tabindex="-1"></a>  prior <span class="ot">&lt;-</span> <span class="fu">dexp</span>(lambda, <span class="at">rate =</span> gamma.prior)</span>
<span id="cb4-7"><a href="bayesian-inference.html#cb4-7" tabindex="-1"></a>  </span>
<span id="cb4-8"><a href="bayesian-inference.html#cb4-8" tabindex="-1"></a>  <span class="co">#evaluate posterior density</span></span>
<span id="cb4-9"><a href="bayesian-inference.html#cb4-9" tabindex="-1"></a>  posterior <span class="ot">&lt;-</span> <span class="fu">dgamma</span>(lambda, <span class="at">shape =</span> <span class="dv">11</span>, <span class="at">rate =</span> <span class="dv">95</span> <span class="sc">+</span> gamma.prior)</span>
<span id="cb4-10"><a href="bayesian-inference.html#cb4-10" tabindex="-1"></a>  </span>
<span id="cb4-11"><a href="bayesian-inference.html#cb4-11" tabindex="-1"></a>  </span>
<span id="cb4-12"><a href="bayesian-inference.html#cb4-12" tabindex="-1"></a>  <span class="co">#plot</span></span>
<span id="cb4-13"><a href="bayesian-inference.html#cb4-13" tabindex="-1"></a>  <span class="fu">plot</span>(lambda, posterior, <span class="at">type=</span> <span class="st">&#39;l&#39;</span>, </span>
<span id="cb4-14"><a href="bayesian-inference.html#cb4-14" tabindex="-1"></a>       <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="at">xlab =</span> <span class="fu">expression</span>(lambda), <span class="at">ylab =</span> <span class="st">&quot;density&quot;</span>)</span>
<span id="cb4-15"><a href="bayesian-inference.html#cb4-15" tabindex="-1"></a>  <span class="fu">lines</span>(lambda, prior, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb4-16"><a href="bayesian-inference.html#cb4-16" tabindex="-1"></a>  <span class="fu">legend</span>(<span class="st">&#39;topright&#39;</span>, <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Posterior&quot;</span>, <span class="st">&quot;Prior&quot;</span>),  </span>
<span id="cb4-17"><a href="bayesian-inference.html#cb4-17" tabindex="-1"></a>         <span class="at">bty =</span> <span class="st">&quot;n&quot;</span>)</span>
<span id="cb4-18"><a href="bayesian-inference.html#cb4-18" tabindex="-1"></a>}</span>
<span id="cb4-19"><a href="bayesian-inference.html#cb4-19" tabindex="-1"></a></span>
<span id="cb4-20"><a href="bayesian-inference.html#cb4-20" tabindex="-1"></a><span class="fu">plot.distributions</span>(<span class="fl">0.01</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="bayesian-inference.html#cb5-1" tabindex="-1"></a><span class="fu">plot.distributions</span>(<span class="dv">50</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-2-2.png" width="672" /></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="bayesian-inference.html#cb6-1" tabindex="-1"></a><span class="fu">plot.distributions</span>(<span class="dv">150</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-2-3.png" width="672" /></p>
<p>The insurance managers recommend that because this is a new premium, a vague prior distribution be used and <span class="math inline">\(\gamma = 0.01\)</span>. The posterior mean is <span class="math inline">\(\frac{11}{95.01} \approx 0.116\)</span> and the 95% credible interval is</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="bayesian-inference.html#cb7-1" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">qgamma</span>(<span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>), <span class="dv">11</span>, <span class="fl">95.01</span>), <span class="dv">3</span>)</span></code></pre></div>
<pre><code>## [1] 0.058 0.194</code></pre>
</div>
<div class="example">
<p><span id="exm:normal" class="example"><strong>Example 3.5  (Normal likelihood) </strong></span>Suppose <span class="math inline">\(Y_1,\dotsc,Y_N \mid \mu \overset{i.i.d}{\sim} N(\mu, \sigma^2)\)</span> and assume the value of <span class="math inline">\(\sigma &gt;0\)</span> is known. Let <span class="math inline">\(\boldsymbol{Y} = (Y_1,\dotsc,Y_n)\)</span>. We impose a Normal prior distribution on the unknown parameter <span class="math inline">\(\mu \sim N(\mu_0, \sigma_0^2)\)</span>. By Bayes’ theorem, the posterior distribution is
<span class="math display">\[
\pi(\mu \mid \boldsymbol{y}) \propto \pi(\boldsymbol{y} \mid \mu) \pi(\mu)
\]</span></p>
<p><strong>Likelihood function</strong>.
As the observations are independent, the likelihood function is given by the product of the <span class="math inline">\(N\)</span> normal density functions as follows,
<span class="math display">\[\begin{align*}
\pi(\boldsymbol{y} \mid \mu) &amp;= \prod_{i=1}^{N} \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left\{-\frac{(y_i - \mu)^2}{2\sigma^2}\right\} \\
&amp;= (2\pi\sigma^2)^{-\frac{N}{2}}\exp\left\{-\sum_{i=1}^{N}\frac{(y_i - \mu)^2}{2\sigma^2}\right\}.
\end{align*}\]</span></p>
<p><strong>Prior distribution</strong>
<span class="math display">\[
\pi(\mu) = \frac{1}{\sqrt{2\pi\sigma_0^2}}\exp\left\{-\frac{1}{2\sigma_0^2}(\mu - \mu_0)^2\right\}.
\]</span></p>
<p><strong>Posterior distribution</strong>. To derive the posterior distribution, up to proportionality, we multiply the prior distribution by the likelihood function. As the fractions out the front of both terms do not depend on <span class="math inline">\(\mu\)</span>, we can ignore these.
<span class="math display">\[\begin{align*}
\pi(\mu \mid \boldsymbol{y}) &amp;\propto\exp\left\{-\sum_{i=1}^{N}\frac{(y_i - \mu)^2}{2\sigma^2}\right\}  \exp\left\{-\frac{1}{2\sigma_0^2}(\mu - \mu_0)^2\right\} \\
&amp; = \exp\left\{-\sum_{i=1}^{N}\frac{(y_i - \mu)^2}{2\sigma^2}-\frac{1}{2\sigma_0^2}(\mu - \mu_0)^2\right\} \\
&amp; = \exp\left\{-\frac{\sum_{i=1}^{N}y_i^2}{2\sigma^2} + \frac{\mu\sum_{i=1}^{N}y_i}{\sigma^2} - \frac{N\mu^2}{2\sigma^2} - \frac{\mu^2}{2\sigma_0^2} + \frac{\mu\mu_0}{\sigma_0^2} - \frac{\mu_0^2}{2\sigma_0^2}\right\}.
\end{align*}\]</span></p>
<p>We can drop the first and last term as they do not depend on <span class="math inline">\(\mu\)</span>. With some arranging, the equation becomes
<span class="math display">\[
\pi(\mu \mid \boldsymbol{y}) \propto \exp\left\{-\mu^2\left(\frac{N}{2\sigma^2}  + \frac{1}{2\sigma_0^2}\right) + \mu\left(\frac{\sum_{i=1}^{N}y_i}{\sigma^2} + \frac{\mu_0}{\sigma_0^2} \right)  \right\}
\]</span>
Defining <span class="math inline">\(a =\left(\frac{\sum_{i=1}^{N}y_i}{\sigma^2} + \frac{\mu_0}{\sigma_0^2} \right)\)</span> and <span class="math inline">\(b^2 = \left(\frac{N}{\sigma^2}  + \frac{1}{\sigma_0^2}\right)^{-1} = \frac{\sigma^2\sigma_0^2}{N\sigma_0^2+\sigma^2}\)</span> tidies this up and gives
<span class="math display">\[
\pi(\mu \mid \boldsymbol{y}) \propto \exp\left\{-\frac{\mu^2}{2b^2} + \mu a \right\}.
\]</span>
Our last step to turning this into a distribution is completing the square. Consider the exponent term, completing the square becomes
<span class="math display">\[
-\frac{\mu^2}{2b^2} + \mu a = -\frac{1}{2b^2}\left(\mu - {a}{b^2} \right)^2 + \frac{a^2b^2}{2}.
\]</span>
Therefore, the posterior distribution, up to proportionality, is given by
<span class="math display">\[
\pi(\mu \mid \boldsymbol{y}) \propto \exp\left\{-\frac{1}{2b^2}\left(\mu - ab^2 \right)^2\right\},
\]</span>
and so the posterior distribution of <span class="math inline">\(\mu \mid \boldsymbol{Y}\)</span> is <span class="math inline">\(N(ab^2, b^2)\)</span>, where the posterior mean is
<span class="math display">\[
\mu_{\text{post}}:=ab^2 = \frac{\sigma_0^2\sum_{i=1}^N y_i+\mu_0\sigma^2}{N\sigma_0^2+\sigma^2} = \frac{N\sigma_0^2}{N\sigma_0^2 + \sigma^2} \cdot \frac{\sum_{i=1}^Ny_i}{N} + \frac{\sigma^2}{N\sigma_0^2 + \sigma^2} \cdot \mu_0
\]</span>
and the posterior variance is <span class="math inline">\(b^2\)</span>. Note that we have again that the posterior mean is a weighted average between the prior mean <span class="math inline">\(\mu_0\)</span> and the MLE for <span class="math inline">\(\mu\)</span>, <span class="math inline">\(\hat{\mu}_{\text{MLE}} = \overline{y} = \sum y_i/N\)</span>. If we have either abundant data (<span class="math inline">\(N \rightarrow \infty\)</span>) or a vague prior <span class="math inline">\(\sigma^2_0 \rightarrow \infty\)</span>, the weight on MLE increases to <span class="math inline">\(1\)</span> and we then have the posterior mean <span class="math inline">\(\mathbb{E}(\mu\mid \boldsymbol{y}) \rightarrow \sum y_i/N\)</span>.</p>
<p>To further interpret the weights, we write the posterior mean as
<span class="math display">\[
\mu_{\text{post}} = \frac{N/\sigma^2}{N/\sigma^2 + 1/\sigma_0^2} \cdot \frac{\sum_{i=1}^Ny_i}{N} + \frac{1/\sigma^2_0}{N/\sigma^2 + 1/\sigma_0^2} \cdot \mu_0
\]</span>
Note that the <strong>precision</strong> of a univarite distribution is the reciprocal of its variance. Therefore, the prior distribution has precision <span class="math inline">\(1/\sigma_0^2\)</span> and the MLE <span class="math inline">\(\sum_{i=1}^NY_i/N \sim N(\mu,\sigma^2/N)\)</span> has precision <span class="math inline">\(N/\sigma^2\)</span>. Hence, we conclude that the posterior mean is a weighted average of the prior mean and the sample mean, with weights proportional to the precisions. Furthermore, with the notion of precision, it is very easy to remember the formula for posterior variance <span class="math inline">\(b^2\)</span> since it satisfies
<span class="math display">\[
\frac{1}{b^2} = \frac{N}{\sigma^2}+\frac{1}{\sigma_0^2},
\]</span>
i.e. the posterior precision is the sum of the prior precision and MLE precision in this Normal-Normal example.</p>
<p>Finally, let’s consider constructing (equal-tailed) credible intervals at level <span class="math inline">\(1-\alpha\)</span>. We need to find <span class="math inline">\(l,u \in \mathbb{R}\)</span> such that
<span class="math display">\[
\pi(\mu &lt; l\mid \boldsymbol{y}) = \pi(\mu &gt;u\mid \boldsymbol{y}) = \alpha/2
\]</span>
so that <span class="math inline">\(\pi(\mu \in [l,u] \mid \boldsymbol{y}) = 1-\alpha\)</span>. To do so, we have
<span class="math display">\[
\pi\Big(\frac{\mu - \mu_{\text{post}}}{b} &gt; \frac{u-\mu_{\text{post}}}{b}\mid \boldsymbol{y} \Big) = \pi\Big(Z &gt; \frac{u-\mu_{\text{post}}}{b}\Big) = \alpha/2,
\]</span>
where <span class="math inline">\(Z \sim N(0, 1)\)</span>. Therefore, we can choose
<span class="math display">\[
\frac{u-\mu_{\text{post}}}{b} = \Phi^{-1}(1-\alpha/2) \implies u = \mu_{\text{post}} + b \cdot \Phi^{-1}(1-\alpha/2)
\]</span>
and similarly, choosing <span class="math inline">\(l = \mu_{\text{post}} - b \cdot \Phi^{-1}(1-\alpha/2)\)</span> guarantees that
<span class="math display">\[
\mu_{\text{post}} \pm b \cdot \Phi^{-1}(1-\alpha/2)
\]</span>
is a <span class="math inline">\(1-\alpha\)</span> level credible interval for <span class="math inline">\(\mu\)</span>. Recall that when <span class="math inline">\(N\)</span>, the sample size, is large, <span class="math inline">\(\mu_{\text{post}} \approx \hat{\mu}_{\text{MLE}}\)</span> and <span class="math inline">\(b^2 \approx \sigma^2/N\)</span>, this credible interval is approximately equal to the confidence interval obtained based on MLE, i.e. <span class="math inline">\(\hat{\mu}_{\text{MLE}}\pm \frac{\sigma}{\sqrt{n}} \cdot \Phi^{-1}(1-\alpha/2)\)</span>. This phenomenon of numerical equivalence between credible interval and confidence interval based on MLE when <span class="math inline">\(N\)</span> is large holds more generally, as we will discuss at the end of this chapter. However, if <span class="math inline">\(N\)</span> is small, or there is a very strong (specific) prior belief, there could be a significant difference between credible intervals and confidence intervals.</p>
<p>We now explore the posterior distribution using R. We simulate some data with <span class="math inline">\(N = 30\)</span>, <span class="math inline">\(\mu = 5\)</span> and <span class="math inline">\(\sigma^2 = 1\)</span>. Consider a very vague prior distribution <span class="math inline">\(\mu \sim N(0,1000^2)\)</span>.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="bayesian-inference.html#cb9-1" tabindex="-1"></a><span class="co">#data</span></span>
<span id="cb9-2"><a href="bayesian-inference.html#cb9-2" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">30</span></span>
<span id="cb9-3"><a href="bayesian-inference.html#cb9-3" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb9-4"><a href="bayesian-inference.html#cb9-4" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N, <span class="dv">5</span>, sigma)</span>
<span id="cb9-5"><a href="bayesian-inference.html#cb9-5" tabindex="-1"></a><span class="co">#MLE for mu</span></span>
<span id="cb9-6"><a href="bayesian-inference.html#cb9-6" tabindex="-1"></a><span class="fu">mean</span>(y)</span></code></pre></div>
<pre><code>## [1] 4.936467</code></pre>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="bayesian-inference.html#cb11-1" tabindex="-1"></a><span class="co">#prior</span></span>
<span id="cb11-2"><a href="bayesian-inference.html#cb11-2" tabindex="-1"></a>sigma0 <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb11-3"><a href="bayesian-inference.html#cb11-3" tabindex="-1"></a>mu0     <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb11-4"><a href="bayesian-inference.html#cb11-4" tabindex="-1"></a></span>
<span id="cb11-5"><a href="bayesian-inference.html#cb11-5" tabindex="-1"></a><span class="co">#posterior</span></span>
<span id="cb11-6"><a href="bayesian-inference.html#cb11-6" tabindex="-1"></a>sigma1.sq <span class="ot">&lt;-</span> (<span class="dv">1</span><span class="sc">/</span>(sigma0<span class="sc">^</span><span class="dv">2</span>)  <span class="sc">+</span> N<span class="sc">/</span>(sigma<span class="sc">^</span><span class="dv">2</span>))<span class="sc">^-</span><span class="dv">1</span></span>
<span id="cb11-7"><a href="bayesian-inference.html#cb11-7" tabindex="-1"></a>mu1       <span class="ot">&lt;-</span> sigma1.sq<span class="sc">*</span>(<span class="fu">sum</span>(y)<span class="sc">/</span>(sigma<span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> mu0<span class="sc">/</span>(sigma0<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb11-8"><a href="bayesian-inference.html#cb11-8" tabindex="-1"></a></span>
<span id="cb11-9"><a href="bayesian-inference.html#cb11-9" tabindex="-1"></a><span class="fu">c</span>(mu1, sigma1.sq) <span class="co">#output mean and variance</span></span></code></pre></div>
<pre><code>## [1] 4.93646686 0.03333333</code></pre>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="bayesian-inference.html#cb13-1" tabindex="-1"></a><span class="co">#Create plot</span></span>
<span id="cb13-2"><a href="bayesian-inference.html#cb13-2" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">4</span>, <span class="dv">6</span>, <span class="fl">0.01</span>)</span>
<span id="cb13-3"><a href="bayesian-inference.html#cb13-3" tabindex="-1"></a>posterior <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(mu, <span class="at">mean =</span> mu1, <span class="at">sd =</span> <span class="fu">sqrt</span>(sigma1.sq))</span>
<span id="cb13-4"><a href="bayesian-inference.html#cb13-4" tabindex="-1"></a><span class="fu">plot</span>(mu, posterior, <span class="at">type =</span><span class="st">&#39;l&#39;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>The 95% credible interval for the population’s mean reaction time is</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="bayesian-inference.html#cb14-1" tabindex="-1"></a><span class="fu">qnorm</span>(<span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>), mu1, <span class="fu">sqrt</span>(sigma1.sq))</span></code></pre></div>
<pre><code>## [1] 4.578628 5.294306</code></pre>
</div>
</div>
<div id="hierarchical-models" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Hierarchical Models<a href="bayesian-inference.html#hierarchical-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In many modelling problems, there will be multiple parameters each related to one another. These parameters may be directly related to the model, or they may be parameters we introduce through prior distributions. We can form a hierarchy of these parameters, from closest to further from the data, to construct our model.</p>
<div class="example">
<p><span id="exm:unlabeled-div-5" class="example"><strong>Example 3.6  </strong></span>Let’s consider <a href="bayesian-inference.html#exm:exponential">3.4</a> again. We have some data <span class="math inline">\(\boldsymbol{y}\)</span> that are assumed to have been generated from an Exponential distribution with rate parameter <span class="math inline">\(\lambda\)</span>. We placed an Exponential prior distribution with rate <span class="math inline">\(\gamma\)</span> on <span class="math inline">\(\lambda\)</span> and the posterior distribution was <span class="math inline">\(\lambda \mid \boldsymbol{y} \sim \textrm{Gamma}(11, 95 + \gamma)\)</span>.</p>
<p>In that example, we discussed how the choice of <span class="math inline">\(\gamma\)</span> can affect the posterior distribution and conclusions presented to the company. One option is to place a prior distribution on <span class="math inline">\(\gamma\)</span> – a hyperprior distribution. The hierachy formed is
<span class="math display">\[\begin{align*}
\boldsymbol{y} \mid \lambda &amp;\sim \hbox{Exp}(\lambda) &amp; \textrm{(likelihood)} \\
\lambda \mid \gamma &amp;\sim \hbox{Exp}(\gamma) &amp; \textrm{(prior distribution)} \\
\gamma \mid \nu &amp;\sim \hbox{Exp}(\nu) &amp; \textrm{(hyperprior distribution)}  \\
\end{align*}\]</span>.
By Bayes’ theorem, we can write the posterior distribution as
<span class="math display">\[\begin{align*}
\pi(\lambda, \gamma \mid \boldsymbol{y}) \propto \pi(\boldsymbol{y} \mid \lambda)\pi(\lambda \mid \gamma)\pi(\gamma)\\
&amp;\propto \lambda^{10}e^{-\lambda(95 + \gamma)}\nu e^{-\nu\gamma}.
\end{align*}\]</span></p>
<p>To derive the full conditional distributions, we only consider the terms that depends on the parameters we are interested in. The full conditional distribution for <span class="math inline">\(\lambda\)</span> is
<span class="math display">\[
\pi(\lambda \mid \boldsymbol{y}, \,\gamma) \propto \lambda^{10}e^{-\lambda(95 + \gamma)}.
\]</span>
This is unchanged and shows that <span class="math inline">\(\lambda \mid \boldsymbol{y}, \gamma \sim \textrm{Gamma}(11, 95 + \gamma)\)</span>. The full conditional distribution for <span class="math inline">\(\gamma\)</span> is
<span class="math display">\[
\pi(\gamma \mid \boldsymbol{y}, \,\lambda) \propto e^{-\nu\gamma}.
\]</span>
Therefore the full conditional distribution of <span class="math inline">\(\gamma\)</span> is <span class="math inline">\(\gamma \mid \boldsymbol{y}, \,\lambda \sim \hbox{Exp}(\lambda + \nu)\)</span>.
In the next chapter, we will look at how to sample from these distributions.</p>
</div>
</div>
<div id="prediction" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Prediction<a href="bayesian-inference.html#prediction" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In many cases, although we are interested in drawing inference for the model parameters, what we may also be interested in is predicting new values, whose distribution is determined by the model parameters and observed data.</p>
<div class="definition">
<p><span id="def:unlabeled-div-6" class="definition"><strong>Definition 3.3  </strong></span>Suppose we observe some data <span class="math inline">\(\boldsymbol{y}\)</span> given some model parameters <span class="math inline">\(\theta\)</span> and assign a prior distribution to <span class="math inline">\(\theta\)</span> and hence derive the posterior distribution <span class="math inline">\(\pi(\theta \mid \boldsymbol{y})\)</span>. The quantity we are interested in is some future observation <span class="math inline">\(z\)</span>, we would like to the distribution of <span class="math inline">\(z\)</span> given the observed data <span class="math inline">\(\boldsymbol{y}\)</span>, denoted by <span class="math inline">\(\pi(z \mid \boldsymbol{y})\)</span>. This distribution, known as the <strong>posterior predictive distribution</strong> of <span class="math inline">\(z\)</span> must be exhibited as a mixture distribution over the possible values of <span class="math inline">\(\theta\)</span> and is written as,
<span class="math display">\[
\pi(z \mid \boldsymbol{y}) = \int \pi(z \mid \theta) \pi(\theta \mid \boldsymbol{y})\, d\theta.
\]</span></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-7" class="example"><strong>Example 3.7  </strong></span>Students have to submit coursework for a particular statistical module. However, each semester a number of students miss the deadline and hand in their coursework late. Last year, three out of 20 students handed their coursework in late. This year, the course has thirty students in. How many students can we expect to hand in their coursework late?</p>
<p>We can model the number of students handing their coursework in late, denoted by <span class="math inline">\(Y\)</span>, using a Binomial distribution, i.e. <span class="math inline">\(Y \sim \textrm{Bin}(n, \theta)\)</span> where <span class="math inline">\(n\)</span> is the number of students and <span class="math inline">\(\theta\)</span> is the probability of any particular student handing in their coursework late. As in Example <a href="bayesian-inference.html#exm:binom">3.2</a>, we assign a uniform prior distribution to <span class="math inline">\(\theta \sim U[0, 1]\)</span>. Given the observed data, we can derive <span class="math inline">\(\theta \mid \boldsymbol{y} \sim Beta(4, 28)\)</span> (See problem sheets for derivation).</p>
<p>Now we can derive the posterior predictive distribution of <span class="math inline">\(Z\)</span>, the number of students who hand in late. We model <span class="math inline">\(Z\)</span> using a Binomial distribution, <span class="math inline">\(Z \sim \textrm{Bin}(30, \theta)\)</span>. The distribution of <span class="math inline">\(Z\)</span> given the observed data is</p>
<p><span class="math display">\[\begin{align*}
\pi(z \mid \boldsymbol{y}) &amp;= \int_0^1 \pi(z \mid \theta) \pi(\theta \mid \boldsymbol{y})\, d\theta \\
&amp; = \int_0^1 \begin{pmatrix} 30 \\ z \end{pmatrix} \theta^z (1-\theta)^{30 - z} \frac{\Gamma(32)}{\Gamma(4)\Gamma(28)}\theta^{3}(1-\theta)^{27}\, d\theta \\
&amp; = \begin{pmatrix} 30 \\ z \end{pmatrix}\frac{\Gamma(32)}{\Gamma(4)\Gamma(28)}\int_0^1 \theta^{z + 3}(1-\theta)^{57 - z}\, d\theta \\
\end{align*}\]</span>
This integral is difficult to evaluate immediately. But by multiplying (and dividing outside the integral) by a constant, we can turn it into the density function of a Beta<span class="math inline">\((5 + z, 58 - z)\)</span> random variable. This integrates to 1.</p>
<p><span class="math display">\[\begin{align*}
\pi(z \mid \boldsymbol{y})  &amp; = \begin{pmatrix} 30 \\ z \end{pmatrix}\frac{\Gamma(32)}{\Gamma(4)\Gamma(28)}\frac{\Gamma(z+4)\Gamma(58-z)}{\Gamma(62)}\int_0^1 \frac{\Gamma(62)}{\Gamma(z+4)\Gamma(58-z)}\theta^{z + 3}(1-\theta)^{57 - z}\, d\theta \\
&amp; = \begin{pmatrix} 30 \\ z \end{pmatrix}\frac{\Gamma(32)\Gamma(z+4)\Gamma(58-z)}{\Gamma(4)\Gamma(28)\Gamma(62)} \quad \textrm{for }  z \in \{0,1,...,30 \}.
\end{align*}\]</span></p>
<p>This code implements the distribution</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="bayesian-inference.html#cb16-1" tabindex="-1"></a>beta.binom.posterior.predictive.distribution <span class="ot">&lt;-</span> <span class="cf">function</span>(z){</span>
<span id="cb16-2"><a href="bayesian-inference.html#cb16-2" tabindex="-1"></a>  </span>
<span id="cb16-3"><a href="bayesian-inference.html#cb16-3" tabindex="-1"></a>  </span>
<span id="cb16-4"><a href="bayesian-inference.html#cb16-4" tabindex="-1"></a>  numerator <span class="ot">&lt;-</span> <span class="fu">gamma</span>(<span class="dv">32</span>)<span class="sc">*</span><span class="fu">gamma</span>(z <span class="sc">+</span> <span class="dv">4</span>)<span class="sc">*</span><span class="fu">gamma</span>(<span class="dv">58</span><span class="sc">-</span>z)</span>
<span id="cb16-5"><a href="bayesian-inference.html#cb16-5" tabindex="-1"></a>  denominator <span class="ot">&lt;-</span> <span class="fu">gamma</span>(<span class="dv">4</span>)<span class="sc">*</span><span class="fu">gamma</span>(<span class="dv">28</span>)<span class="sc">*</span><span class="fu">gamma</span>(<span class="dv">62</span>)</span>
<span id="cb16-6"><a href="bayesian-inference.html#cb16-6" tabindex="-1"></a>  </span>
<span id="cb16-7"><a href="bayesian-inference.html#cb16-7" tabindex="-1"></a>  output <span class="ot">&lt;-</span> <span class="fu">choose</span>(<span class="dv">30</span>, z)<span class="sc">*</span>numerator<span class="sc">/</span>denominator</span>
<span id="cb16-8"><a href="bayesian-inference.html#cb16-8" tabindex="-1"></a>  <span class="fu">return</span>(output)</span>
<span id="cb16-9"><a href="bayesian-inference.html#cb16-9" tabindex="-1"></a>  </span>
<span id="cb16-10"><a href="bayesian-inference.html#cb16-10" tabindex="-1"></a>}</span></code></pre></div>
<p>We can check that our posterior predictive distribution is a valid probability mass function by checking that the probabilities sum to one.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="bayesian-inference.html#cb17-1" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">30</span></span>
<span id="cb17-2"><a href="bayesian-inference.html#cb17-2" tabindex="-1"></a>ppd <span class="ot">&lt;-</span> <span class="fu">beta.binom.posterior.predictive.distribution</span>(z)</span>
<span id="cb17-3"><a href="bayesian-inference.html#cb17-3" tabindex="-1"></a><span class="fu">sum</span>(ppd)</span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="bayesian-inference.html#cb19-1" tabindex="-1"></a><span class="fu">plot</span>(z, ppd, <span class="at">xlab =</span> <span class="st">&quot;z&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Posterior predictive mass&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>The expected number of students who hand in late is 3.75 and there’s a 95% chance that up to 8 hand in late.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="bayesian-inference.html#cb20-1" tabindex="-1"></a>z<span class="sc">%*%</span>ppd <span class="co">#expectation</span></span></code></pre></div>
<pre><code>##      [,1]
## [1,] 3.75</code></pre>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="bayesian-inference.html#cb22-1" tabindex="-1"></a><span class="fu">cbind</span>(z, <span class="fu">cumsum</span>(ppd)) <span class="co">#CDF</span></span></code></pre></div>
<pre><code>##        z           
##  [1,]  0 0.06029453
##  [2,]  1 0.18723037
##  [3,]  2 0.35156696
##  [4,]  3 0.51889148
##  [5,]  4 0.66530044
##  [6,]  5 0.78021765
##  [7,]  6 0.86309065
##  [8,]  7 0.91880359
##  [9,]  8 0.95404202
## [10,]  9 0.97513714
## [11,] 10 0.98713498
## [12,] 11 0.99363285
## [13,] 12 0.99698773
## [14,] 13 0.99863936
## [15,] 14 0.99941423
## [16,] 15 0.99976022
## [17,] 16 0.99990696
## [18,] 17 0.99996591
## [19,] 18 0.99998826
## [20,] 19 0.99999622
## [21,] 20 0.99999887
## [22,] 21 0.99999969
## [23,] 22 0.99999992
## [24,] 23 0.99999998
## [25,] 24 1.00000000
## [26,] 25 1.00000000
## [27,] 26 1.00000000
## [28,] 27 1.00000000
## [29,] 28 1.00000000
## [30,] 29 1.00000000
## [31,] 30 1.00000000</code></pre>
</div>
</div>
<div id="non-informative-prior-distibrutions" class="section level2 hasAnchor" number="3.5">
<h2><span class="header-section-number">3.5</span> Non-informative Prior Distibrutions<a href="bayesian-inference.html#non-informative-prior-distibrutions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We have seen in a few examples how the choice of the prior distribution (and prior parameters) can impact posterior distributions and the resulting conclusions. As the choice of prior distribution is subjective, it is the main criticism of Bayesian inference. A possible way around this is to use a prior distribution that reflects a lack of information about <span class="math inline">\(\theta\)</span>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-8" class="definition"><strong>Definition 3.4  </strong></span>A <strong>non-informative prior distribution</strong> is a prior distribution that places equal weight on the every possible value of <span class="math inline">\(\theta\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-9" class="example"><strong>Example 3.8  </strong></span>In Example <a href="bayesian-inference.html#exm:binom">3.2</a>, we assigned a uniform prior distribution to the parameter <span class="math inline">\(\theta\)</span>.</p>
</div>
<p>Such a prior distribution can have interesting and perhaps unintended side effects. Suppose we do indeed have some parameter <span class="math inline">\(\theta\)</span> and we place a uniform prior distribution on <span class="math inline">\(\theta\)</span> such that <span class="math inline">\(\theta \sim U[0, 1]\)</span>. This means, for example, our prior beliefs about <span class="math inline">\(\theta\)</span> are that it is equally likely to be in <span class="math inline">\([0, 0.1]\)</span> as it is to lie in <span class="math inline">\([0.8, 0.9]\)</span> or any other interval of size 0.1. However, our prior beliefs about <span class="math inline">\(\theta^2\)</span> are not uniform. Letting <span class="math inline">\(\psi = \theta^2\)</span>, changing variables gives <span class="math inline">\(\pi(\psi) = \frac{1}{2\sqrt{\psi}}\)</span>, something that is not uniform. That raises the question, if we have little to say about <span class="math inline">\(\theta\)</span> , shouldn’t we have little to say about any reasonable transformation of <span class="math inline">\(\theta\)</span>?</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-10" class="theorem"><strong>Theorem 3.1  (Jeffrey) </strong></span>Given some observed data <span class="math inline">\(\boldsymbol{y} = \{y_1, \ldots, y_N\}\)</span>, an invariant prior distribution is
<span class="math display">\[
\pi(\theta) \propto \sqrt{I_\theta(\boldsymbol{y})},
\]</span>
where <span class="math inline">\(I_\theta(\boldsymbol{y})\)</span> is the Fisher information for <span class="math inline">\(\theta\)</span> contained in <span class="math inline">\(\boldsymbol{y}\)</span>.</p>
</div>
<p>Jeffrey argues that if there are two ways of parameterising a model, e.g. via <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\psi\)</span>, then the priors on these parameters should be equivalent. In other words, the prior distribution should be invariant under sensible (one-to-one) transformations.</p>
<div class="proof">
<p><span id="unlabeled-div-11" class="proof"><em>Proof</em>. </span>Recall that the distribution of <span class="math inline">\(\psi = h(\theta)\)</span>, for some one-to-one function <span class="math inline">\(h\)</span>, is invariant to the distribution of <span class="math inline">\(\theta\)</span> if
<span class="math display">\[
\pi(\psi) = \pi(\theta) \left|\frac{d\theta}{d\psi}\right|.
\]</span>
Transforming the Fisher information for <span class="math inline">\(\psi\)</span> shows
<span class="math display">\[\begin{align*}
I_\psi({y}) &amp;= - \mathbb{E}\left(\frac{d^2\log \pi({y} \mid \psi)}{d\psi^2}\right) \\
&amp;= -\mathbb{E}\left(\frac{d}{d\psi} \left( \frac{d \log \pi(y|\theta(\psi))}{d \theta} \frac{d\theta}{d\psi} \right) \right) \tag{chain rule}\\
&amp;= -\mathbb{E}\left(\left(\frac{d^2 \log \pi(y|\theta(\psi))}{d \theta d\psi}\right)\left( \frac{d\theta}{d\psi}\right) + \left(\frac{d \log \pi(y|\theta(\psi))}{d \theta}\right) \left( \frac{d^2\theta}{d\psi^2}\right) \right)\tag{prod. rule} \\
&amp;= -\mathbb{E}\left(\left(\frac{d^2 \log \pi(y|\theta(\psi))}{d \theta^2 }\right)\left( \frac{d\theta}{d\psi}\right)^2 + \left(\frac{d \log \pi(y|\theta(\psi))}{d \theta}\right) \left( \frac{d^2\theta}{d\psi^2}\right) \right)\tag{chain rule} \\
&amp; = -\mathbb{E}\left(\left(\frac{d^2 \log \pi({y} \mid \theta)}{d\theta^2}\left(\frac{d\theta}{d\psi}\right)^2\right)\right)  \\
&amp; = I_\theta({y})\left(\frac{d\theta}{d\psi}\right)^2 .
\end{align*}\]</span>
Thus <span class="math inline">\(\sqrt{I_\psi({y})} = \sqrt{I_\theta({y})} \left|\frac{d\theta}{d\psi}\right|\)</span> and <span class="math inline">\(\sqrt{I_\psi({y})}\)</span> and <span class="math inline">\(\sqrt{I_\theta({y})}\)</span> are invariant prior distributions.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-12" class="example"><strong>Example 3.9  </strong></span>In Example <a href="bayesian-inference.html#exm:binom">3.2</a>, we modelled the number of bot accounts on a social media website by <span class="math inline">\(Y \sim \textrm{Bin}(n, \theta)\)</span>. To construct Jeffrey’s prior distribution for <span class="math inline">\(\theta\)</span>, we must first derive the Fisher information.<br />
<span class="math display">\[\begin{align*}
&amp;\pi(y \mid \theta) = \begin{pmatrix} n \\ y \end{pmatrix} \theta^y (1-\theta)^{n-y}\\
\implies &amp;\log \pi(y \mid \theta) = \log \begin{pmatrix} n \\ y \end{pmatrix} + y \log\theta + (n-y)\log(1-\theta) \\
\implies &amp;\frac{\partial \log \pi(y \mid \theta)}{\partial \theta} = \frac{y}{\theta} - \frac{n-y}{1-\theta} \\
\implies &amp;\frac{\partial^2 \log \pi(y \mid \theta)}{\partial \theta^2} = -\frac{y}{\theta^2} + \frac{n-y}{(1-\theta)^2} \\
\implies &amp;\mathbb{E}\left(\frac{\partial \log \pi(y \mid \theta)}{\partial \theta}\right) = -\frac{\mathbb{E}(y)}{\theta^2} + \frac{n-\mathbb{E}(y)}{(1-\theta)^2}\\
\implies &amp;\mathbb{E}\left(\frac{\partial \log \pi(y \mid \theta)}{\partial \theta}\right) = -\frac{n\theta}{\theta^2} + \frac{n-n\theta}{(1-\theta)^2}\\
\implies &amp;\mathbb{E}\left(\frac{\partial \log \pi(y \mid \theta)}{\partial \theta}\right) = -\frac{n}{\theta} + \frac{n}{1-\theta}\\
\implies &amp;\mathbb{E}\left(\frac{\partial \log \pi(y \mid \theta)}{\partial \theta}\right) = -\frac{n}{\theta(1-\theta)} \\
\implies &amp;I_\theta(y) \propto \frac{1}{\theta(1-\theta)}.
\end{align*}\]</span></p>
<p>Hence Jeffrey’s prior is <span class="math inline">\(\pi(\theta) \propto \theta^{-\frac{1}{2}}(1-\theta)^{-\frac{1}{2}}\)</span>. This functional dependency on <span class="math inline">\(\theta\)</span> shows that <span class="math inline">\(\theta \sim \textrm{Beta}(\frac{1}{2}, \frac{1}{2})\)</span>.</p>
</div>
</div>
<div id="bernstein-von-mises-theorem" class="section level2 hasAnchor" number="3.6">
<h2><span class="header-section-number">3.6</span> Bernstein-von-Mises Theorem<a href="bayesian-inference.html#bernstein-von-mises-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>So far, we have considered Bayesian methods in contrast to frequentist ones. The Bernstein-von-Mises theorem is a key theorem linking the two inference methods.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-13" class="theorem"><strong>Theorem 3.2  (Bernstein-von-Mises) </strong></span>For a well-specified model <span class="math inline">\(\pi(\boldsymbol{y} \mid \theta)\)</span> with a fixed number of parameters, and for a smooth prior distribution <span class="math inline">\(\pi(\theta)\)</span> that is non-zero around the MLE <span class="math inline">\(\hat{\theta}\)</span>, then
<span class="math display">\[
\left|\left| \pi(\theta \mid \boldsymbol{y}) - N\left(\hat{\theta}, \frac{I(\hat{\theta})^{-1}}{n}\right) \right|\right|_{TV} \rightarrow 0,
\]</span>
where <span class="math inline">\(||p - q||_{TV}\)</span> is the total variation distance between distributions <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span>:
<span class="math display">\[
||p - q||_{TV} = \frac{1}{2}\int|\pi(x) - q(x)|\,dx.
\]</span></p>
</div>
<p>The Berstein-von-Mises theorem says that as the number of data points approaches infinity, the posterior distribution tends to a Normal distribution centered around the MLE and variance dependent on the Fisher information. The proof of this theorem is out of the scope of this module, but can be found in Asymptotic Statistics (2000) by A. W. van der Vaart.</p>
</div>
<div id="lab" class="section level2 hasAnchor" number="3.7">
<h2><span class="header-section-number">3.7</span> Lab<a href="bayesian-inference.html#lab" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The aim of this lab is to work with some posterior distributions in cases when the prior distribution is or is not conjugate. Recall the definition of a conjugate prior distribution:</p>
<div class="defintion">
<p>If the prior distribution <span class="math inline">\(\pi(\theta)\)</span> has the same distributional family as the posterior distribution <span class="math inline">\(\pi(\theta \mid \boldsymbol{y})\)</span>, then the prior distribution is a <strong>conjugate prior distribution</strong>.</p>
</div>
<p>Working with conjugate prior distributions often makes the analytical work much easier, as we can work with the posterior distribution. But sometimes, conjugate prior distributions may not be appropriate. This is where R can help, as we do not need a closed form to carry out computations.</p>
<div class="example">
<p><span id="exm:unlabeled-div-14" class="example"><strong>Example 3.10  </strong></span>The total number of goals scored in 50 games of a low level football league is shown below.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="bayesian-inference.html#cb24-1" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">6</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">6</span>, <span class="dv">6</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">3</span>,</span>
<span id="cb24-2"><a href="bayesian-inference.html#cb24-2" tabindex="-1"></a>       <span class="dv">6</span>, <span class="dv">4</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">7</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">2</span>, <span class="dv">6</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">7</span>, <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">6</span>, <span class="dv">8</span>, <span class="dv">8</span>, <span class="dv">4</span>, <span class="dv">5</span>,</span>
<span id="cb24-3"><a href="bayesian-inference.html#cb24-3" tabindex="-1"></a>       <span class="dv">7</span>, <span class="dv">4</span>)</span>
<span id="cb24-4"><a href="bayesian-inference.html#cb24-4" tabindex="-1"></a><span class="fu">hist</span>(y, <span class="at">main =</span> <span class="st">&quot;&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Number of goals scored&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="bayesian-inference.html#cb25-1" tabindex="-1"></a><span class="fu">mean</span>(y)</span></code></pre></div>
<pre><code>## [1] 3.92</code></pre>
<p>We can model the number of goals scored using a Poisson distribution
<span class="math display">\[
y \sim \hbox{Po}(\lambda).
\]</span>
By Bayes’ theorem, the posterior distribution is given by
<span class="math display">\[
\pi(\lambda \mid \boldsymbol{y}) \propto \pi(\boldsymbol{y} \mid \lambda)\pi(\lambda).
\]</span>
The likelihood function is given by
<span class="math display">\[\begin{align*}
\pi(\boldsymbol{y} \mid \lambda) &amp;= \prod_{i=1}^{50} \frac{e^{-\lambda}\lambda^{y_i}}{y_i!}\\
&amp;= \frac{e^{-50\lambda}\lambda^{\sum y_i}}{\prod_{i=1}^{50} y_i!}
\end{align*}\]</span></p>
<p>R has a set of inbuilt functions for working with the Poisson distribution so we can rely on those to write functions for the likelihood and loglikelihood.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="bayesian-inference.html#cb27-1" tabindex="-1"></a>lambda <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="fl">0.01</span>) <span class="co">#grid of lambda values</span></span>
<span id="cb27-2"><a href="bayesian-inference.html#cb27-2" tabindex="-1"></a>likelihood.function <span class="ot">&lt;-</span> <span class="cf">function</span>(lambda, y) <span class="fu">prod</span>(<span class="fu">dpois</span>(y, lambda)) <span class="co">#compute likelihood</span></span>
<span id="cb27-3"><a href="bayesian-inference.html#cb27-3" tabindex="-1"></a>log.likelihood.function  <span class="ot">&lt;-</span> <span class="cf">function</span>(lambda, y) <span class="fu">sum</span>(<span class="fu">dpois</span>(y, lambda, <span class="at">log =</span> <span class="cn">TRUE</span>)) <span class="co">#compute loglikelihood</span></span>
<span id="cb27-4"><a href="bayesian-inference.html#cb27-4" tabindex="-1"></a>likelihood <span class="ot">&lt;-</span> <span class="fu">sapply</span>(lambda,  likelihood.function, y) <span class="co">#evaluate at grid of points</span></span>
<span id="cb27-5"><a href="bayesian-inference.html#cb27-5" tabindex="-1"></a>log.likelihood <span class="ot">&lt;-</span> <span class="fu">sapply</span>(lambda,  log.likelihood.function, y) <span class="co">#evaluate at grid of points</span></span>
<span id="cb27-6"><a href="bayesian-inference.html#cb27-6" tabindex="-1"></a></span>
<span id="cb27-7"><a href="bayesian-inference.html#cb27-7" tabindex="-1"></a><span class="co">#Plot likelihood</span></span>
<span id="cb27-8"><a href="bayesian-inference.html#cb27-8" tabindex="-1"></a><span class="fu">plot</span>(lambda, likelihood, </span>
<span id="cb27-9"><a href="bayesian-inference.html#cb27-9" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="fu">expression</span>(lambda), <span class="at">ylab =</span> <span class="st">&quot;likelihood&quot;</span>, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="bayesian-inference.html#cb28-1" tabindex="-1"></a><span class="fu">plot</span>(lambda, log.likelihood, </span>
<span id="cb28-2"><a href="bayesian-inference.html#cb28-2" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="fu">expression</span>(lambda), <span class="at">ylab =</span> <span class="st">&quot;loglikelihood&quot;</span>, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-10-2.png" width="672" />
When coding posterior distributions, we often work on the log scale because the numbers can be smaller that R can deal with. The denominator with the factorial can get very large very quickly.</p>
<p>After speaking to football experts, we decide to place a normal prior distribution on <span class="math inline">\(\lambda\)</span> with mean 5 goals and standard deviation one goal, i.e.
<span class="math display">\[
\lambda \sim N(5, 1).
\]</span>
The prior distribution can be plotted by</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="bayesian-inference.html#cb29-1" tabindex="-1"></a>lambda   <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="fl">0.01</span>) <span class="co">#grid of lambda values</span></span>
<span id="cb29-2"><a href="bayesian-inference.html#cb29-2" tabindex="-1"></a>prior    <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(lambda, <span class="dv">5</span>, <span class="dv">1</span>)</span>
<span id="cb29-3"><a href="bayesian-inference.html#cb29-3" tabindex="-1"></a>log.prior <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(lambda, <span class="dv">5</span>, <span class="dv">1</span>, <span class="at">log =</span> <span class="cn">TRUE</span>)</span>
<span id="cb29-4"><a href="bayesian-inference.html#cb29-4" tabindex="-1"></a><span class="fu">plot</span>(lambda, prior, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, <span class="at">xlab =</span> <span class="fu">expression</span>(lambda), <span class="at">ylab =</span> <span class="st">&quot;density&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="bayesian-inference.html#cb30-1" tabindex="-1"></a><span class="fu">plot</span>(lambda, log.prior, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, </span>
<span id="cb30-2"><a href="bayesian-inference.html#cb30-2" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="fu">expression</span>(lambda), <span class="at">ylab =</span> <span class="st">&quot;log density&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-11-2.png" width="672" /></p>
<p>Writing the posterior distribution up to proportionality, we get
<span class="math display">\[
\pi(\lambda \mid \boldsymbol{y}) \propto \exp\left(-50\lambda -\frac{1}{2}(\lambda - 5)^2\right)\lambda^{\sum y_i}.
\]</span>
There is no closed form for this distribution and it is not that nice to work with. But with R, we can easily evaluate the posterior distribution at a grid of points.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="bayesian-inference.html#cb31-1" tabindex="-1"></a>posterior <span class="ot">&lt;-</span> prior<span class="sc">*</span>likelihood</span>
<span id="cb31-2"><a href="bayesian-inference.html#cb31-2" tabindex="-1"></a>integrating.factor <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span><span class="fl">0.01</span><span class="sc">*</span>(posterior[<span class="dv">1</span>] <span class="sc">+</span> posterior[<span class="dv">1001</span>] <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">sum</span>(posterior[<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1001</span>)])) <span class="co">#Using trapezium rule</span></span>
<span id="cb31-3"><a href="bayesian-inference.html#cb31-3" tabindex="-1"></a>posterior <span class="ot">&lt;-</span> posterior<span class="sc">/</span>integrating.factor <span class="co">#normalise</span></span>
<span id="cb31-4"><a href="bayesian-inference.html#cb31-4" tabindex="-1"></a><span class="fu">plot</span>(lambda, posterior, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, <span class="at">xlab =</span> <span class="fu">expression</span>(lambda), </span>
<span id="cb31-5"><a href="bayesian-inference.html#cb31-5" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;posterior density&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>We can now visually inspect the posterior distribution and see that it has a strong peak around 4. One important statistic is the <strong>maximum a posteriori estimation</strong> or MAP estimate, this is the mode of the posterior distribution and it is a similar principle to the maximum likelihood estimate.</p>
<p>We can compute this using the command</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="bayesian-inference.html#cb32-1" tabindex="-1"></a>lambda[<span class="fu">which.max</span>(posterior)]</span></code></pre></div>
<pre><code>## [1] 4</code></pre>
<p>which shows the MAP estimate is exactly 4.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-15" class="exercise"><strong>Exercise 3.1  </strong></span>Adapt the code in the Example above to use an exponential prior distribution with rate 0.1. Then derive the posterior distribution analytically and compare to the numerical version.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-16" class="exercise"><strong>Exercise 3.2  </strong></span>You are given that the data are exponentially distributed with rate <span class="math inline">\(\lambda,\)</span> i.e. <span class="math inline">\(Y_1, \ldots, Y_N \sim \hbox{Exp}(\lambda)\)</span>. Your prior belief is that <span class="math inline">\(\lambda \in (0, 1)\)</span>. Show that the posterior distribution <span class="math inline">\(\pi(\lambda \mid \boldsymbol{y})\)</span> has no closed form when the prior distribution for <span class="math inline">\(\lambda \sim \hbox{Beta}(\alpha, \beta)\)</span>.</p>
<p>The data is given by</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="bayesian-inference.html#cb34-1" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">1.101558</span>, <span class="fl">1.143953</span>, <span class="fl">1.287348</span>, <span class="fl">1.181010</span>, <span class="fl">1.139132</span>, <span class="fl">1.148631</span>, <span class="fl">1.133201</span>, <span class="fl">1.361229</span>, <span class="fl">1.332540</span>, <span class="fl">1.052501</span>)</span></code></pre></div>
</div>
<p>By writing an R function to evaluate the likelihood function, evaluate the posterior distribution for <span class="math inline">\(\lambda\)</span> over a grid of points.</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-17" class="exercise"><strong>Exercise 3.3  </strong></span>Suppose you have <span class="math inline">\(X_1, ..., X_N \sim \hbox{Bin}(100, p)\)</span>. Using <span class="math inline">\(p \sim \hbox{Beta}(\alpha, \beta)\)</span> as the prior distribution, derive the posterior distribution and the posterior mean (Wikipedia is a helpful place for properties of distributions).</p>
<ol style="list-style-type: decimal">
<li>(Large data scenario) Fix <span class="math inline">\(\alpha = 2\)</span>, <span class="math inline">\(N = 150\)</span> and <span class="math inline">\(\Sigma x_i = 2,971\)</span>. Plot the prior and posterior distributions for different values of <span class="math inline">\(\beta\)</span> on the same figure. Plot the posterior mean against <span class="math inline">\(\beta \in (0, 10)\)</span>. Plot the prior mean against the posterior mean for <span class="math inline">\(\beta \in (0, 10)\)</span>.</li>
<li>(Small data scenario) Fix <span class="math inline">\(\alpha = 2\)</span>, <span class="math inline">\(N = 10\)</span> and <span class="math inline">\(\Sigma x_i = 101\)</span> Plot the prior and posterior distributions for different values of <span class="math inline">\(\beta\)</span> on the same figure.Plot the posterior distribution for different values of <span class="math inline">\(\beta\)</span>. Plot the posterior mean against <span class="math inline">\(\beta \in (0, 10)\)</span>. Plot the prior mean against the posterior mean for <span class="math inline">\(\beta \in (0, 10)\)</span>.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-18" class="exercise"><strong>Exercise 3.4  </strong></span>Code up the posterior distribution in question 4 of problem sheet 2 (the Pareto distribution). Set <span class="math inline">\(a = 1\)</span>, <span class="math inline">\(b = 2\)</span> and let the data be</p>
<pre><code>y &lt;- c(1.019844, 1.043574, 1.360953, 1.049228, 1.491926, 1.192943, 1.323738, 1.262572, 2.034768, 1.451654)</code></pre>
<p>Find the MAP estimate for <span class="math inline">\(\beta\)</span></p>
</div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="programming-in-r.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sampling.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
