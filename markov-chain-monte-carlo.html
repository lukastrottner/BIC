<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Markov Chain Monte Carlo | Bayesian Inference and Computation</title>
  <meta name="description" content="This book contains the lecture notes for the module Bayesian Inference and Computation." />
  <meta name="generator" content="bookdown 0.42 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Markov Chain Monte Carlo | Bayesian Inference and Computation" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="/uob_logo.png" />
  <meta property="og:description" content="This book contains the lecture notes for the module Bayesian Inference and Computation." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Markov Chain Monte Carlo | Bayesian Inference and Computation" />
  
  <meta name="twitter:description" content="This book contains the lecture notes for the module Bayesian Inference and Computation." />
  <meta name="twitter:image" content="/uob_logo.png" />

<meta name="author" content="Dr Mengchu Li and Dr Lukas Trottner (based on lecture notes by Dr Rowland Seymour)" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="sampling.html"/>
<link rel="next" href="advanced-computation.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Inference and Computation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Practicalities</a>
<ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#module-aims"><i class="fa fa-check"></i><b>0.1</b> Module Aims</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#module-structure"><i class="fa fa-check"></i><b>0.2</b> Module Structure</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#assessment"><i class="fa fa-check"></i><b>0.3</b> Assessment</a></li>
<li class="chapter" data-level="0.4" data-path="index.html"><a href="index.html#recommended-books-and-videos"><i class="fa fa-check"></i><b>0.4</b> Recommended Books and Videos</a></li>
<li class="chapter" data-level="0.5" data-path="index.html"><a href="index.html#common-distributions"><i class="fa fa-check"></i><b>0.5</b> Common Distributions</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="fundamentals.html"><a href="fundamentals.html"><i class="fa fa-check"></i><b>1</b> Fundamentals Concepts</a>
<ul>
<li class="chapter" data-level="1.1" data-path="fundamentals.html"><a href="fundamentals.html#statistical-inference"><i class="fa fa-check"></i><b>1.1</b> Statistical Inference</a></li>
<li class="chapter" data-level="1.2" data-path="fundamentals.html"><a href="fundamentals.html#frequentist-theory"><i class="fa fa-check"></i><b>1.2</b> Frequentist Theory</a></li>
<li class="chapter" data-level="1.3" data-path="fundamentals.html"><a href="fundamentals.html#bayesian-paradigm"><i class="fa fa-check"></i><b>1.3</b> Bayesian Paradigm</a></li>
<li class="chapter" data-level="1.4" data-path="fundamentals.html"><a href="fundamentals.html#bayes-theorem"><i class="fa fa-check"></i><b>1.4</b> Bayes’ Theorem</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="programming-in-r.html"><a href="programming-in-r.html"><i class="fa fa-check"></i><b>2</b> Programming in R</a>
<ul>
<li class="chapter" data-level="2.1" data-path="programming-in-r.html"><a href="programming-in-r.html#random-numbers-for-loops-and-r"><i class="fa fa-check"></i><b>2.1</b> Random Numbers, For Loops and R</a></li>
<li class="chapter" data-level="2.2" data-path="programming-in-r.html"><a href="programming-in-r.html#functions-in-r"><i class="fa fa-check"></i><b>2.2</b> Functions in R</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="programming-in-r.html"><a href="programming-in-r.html#built-in-commands"><i class="fa fa-check"></i><b>2.2.1</b> Built in commands</a></li>
<li class="chapter" data-level="2.2.2" data-path="programming-in-r.html"><a href="programming-in-r.html#user-defined-functions"><i class="fa fa-check"></i><b>2.2.2</b> User defined functions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="programming-in-r.html"><a href="programming-in-r.html#good-coding-practices"><i class="fa fa-check"></i><b>2.3</b> Good Coding Practices</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="programming-in-r.html"><a href="programming-in-r.html#code-style"><i class="fa fa-check"></i><b>2.3.1</b> Code Style</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bayesian-inference.html"><a href="bayesian-inference.html"><i class="fa fa-check"></i><b>3</b> Bayesian Inference</a>
<ul>
<li class="chapter" data-level="3.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#simple-examples"><i class="fa fa-check"></i><b>3.1</b> Simple Examples</a></li>
<li class="chapter" data-level="3.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#reporting-conclusions-from-bayesian-inference"><i class="fa fa-check"></i><b>3.2</b> Reporting Conclusions from Bayesian Inference</a></li>
<li class="chapter" data-level="3.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#conjugate-prior-and-posterior-analysis"><i class="fa fa-check"></i><b>3.3</b> Conjugate Prior and Posterior Analysis</a></li>
<li class="chapter" data-level="3.4" data-path="bayesian-inference.html"><a href="bayesian-inference.html#prediction"><i class="fa fa-check"></i><b>3.4</b> Prediction</a></li>
<li class="chapter" data-level="3.5" data-path="bayesian-inference.html"><a href="bayesian-inference.html#non-informative-prior-distibrutions"><i class="fa fa-check"></i><b>3.5</b> Non-informative Prior Distibrutions</a></li>
<li class="chapter" data-level="3.6" data-path="bayesian-inference.html"><a href="bayesian-inference.html#frequentist-analysis-of-bayesian-methods"><i class="fa fa-check"></i><b>3.6</b> Frequentist analysis of Bayesian methods</a></li>
<li class="chapter" data-level="3.7" data-path="bayesian-inference.html"><a href="bayesian-inference.html#hierarchical-models"><i class="fa fa-check"></i><b>3.7</b> Hierarchical Models</a></li>
<li class="chapter" data-level="3.8" data-path="bayesian-inference.html"><a href="bayesian-inference.html#lab"><i class="fa fa-check"></i><b>3.8</b> Lab</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="sampling.html"><a href="sampling.html"><i class="fa fa-check"></i><b>4</b> Sampling</a>
<ul>
<li class="chapter" data-level="4.1" data-path="sampling.html"><a href="sampling.html#uniform-random-numbers"><i class="fa fa-check"></i><b>4.1</b> Uniform Random Numbers</a></li>
<li class="chapter" data-level="4.2" data-path="sampling.html"><a href="sampling.html#inverse-transform-sampling"><i class="fa fa-check"></i><b>4.2</b> Inverse Transform Sampling</a></li>
<li class="chapter" data-level="4.3" data-path="sampling.html"><a href="sampling.html#rejection-sampling"><i class="fa fa-check"></i><b>4.3</b> Rejection Sampling</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="sampling.html"><a href="sampling.html#rejection-sampling-efficiency"><i class="fa fa-check"></i><b>4.3.1</b> Rejection Sampling Efficiency</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="sampling.html"><a href="sampling.html#ziggurat-sampling"><i class="fa fa-check"></i><b>4.4</b> Ziggurat Sampling</a></li>
<li class="chapter" data-level="4.5" data-path="sampling.html"><a href="sampling.html#approximate-bayesian-computation"><i class="fa fa-check"></i><b>4.5</b> Approximate Bayesian Computation</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="sampling.html"><a href="sampling.html#abc-with-rejection"><i class="fa fa-check"></i><b>4.5.1</b> ABC with Rejection</a></li>
<li class="chapter" data-level="4.5.2" data-path="sampling.html"><a href="sampling.html#summary-abc-with-rejection"><i class="fa fa-check"></i><b>4.5.2</b> Summary ABC with Rejection</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="sampling.html"><a href="sampling.html#lab-1"><i class="fa fa-check"></i><b>4.6</b> Lab</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html"><i class="fa fa-check"></i><b>5</b> Markov Chain Monte Carlo</a>
<ul>
<li class="chapter" data-level="5.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#properties-of-markov-chains"><i class="fa fa-check"></i><b>5.1</b> Properties of Markov Chains</a></li>
<li class="chapter" data-level="5.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#metropolishastings"><i class="fa fa-check"></i><b>5.2</b> Metropolis–Hastings</a></li>
<li class="chapter" data-level="5.6" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#beyond-mcmc"><i class="fa fa-check"></i><b>5.6</b> Beyond MCMC</a></li>
<li class="chapter" data-level="5.7" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#lab-2"><i class="fa fa-check"></i><b>5.7</b> Lab</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="advanced-computation.html"><a href="advanced-computation.html"><i class="fa fa-check"></i><b>6</b> Advanced Computation</a>
<ul>
<li class="chapter" data-level="6.1" data-path="advanced-computation.html"><a href="advanced-computation.html#gaussian-processes"><i class="fa fa-check"></i><b>6.1</b> Gaussian Processes</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="advanced-computation.html"><a href="advanced-computation.html#covariance-functions"><i class="fa fa-check"></i><b>6.1.1</b> Covariance Functions</a></li>
<li class="chapter" data-level="6.1.2" data-path="advanced-computation.html"><a href="advanced-computation.html#gaussian-process-regression"><i class="fa fa-check"></i><b>6.1.2</b> Gaussian Process Regression</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="advanced-computation.html"><a href="advanced-computation.html#data-augmentation"><i class="fa fa-check"></i><b>6.2</b> Data Augmentation</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="advanced-computation.html"><a href="advanced-computation.html#imputing-censored-observations"><i class="fa fa-check"></i><b>6.2.1</b> Imputing censored observations</a></li>
<li class="chapter" data-level="6.2.2" data-path="advanced-computation.html"><a href="advanced-computation.html#imputing-latent-variables"><i class="fa fa-check"></i><b>6.2.2</b> Imputing Latent Variables</a></li>
<li class="chapter" data-level="6.2.3" data-path="advanced-computation.html"><a href="advanced-computation.html#grouped-data"><i class="fa fa-check"></i><b>6.2.3</b> Grouped Data</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="advanced-computation.html"><a href="advanced-computation.html#prior-ellicitation"><i class="fa fa-check"></i><b>6.3</b> Prior Ellicitation</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="advanced-computation.html"><a href="advanced-computation.html#prior-summaries"><i class="fa fa-check"></i><b>6.3.1</b> Prior Summaries</a></li>
<li class="chapter" data-level="6.3.2" data-path="advanced-computation.html"><a href="advanced-computation.html#betting-with-histograms"><i class="fa fa-check"></i><b>6.3.2</b> Betting with Histograms</a></li>
<li class="chapter" data-level="6.3.3" data-path="advanced-computation.html"><a href="advanced-computation.html#prior-intervals"><i class="fa fa-check"></i><b>6.3.3</b> Prior Intervals</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="advanced-computation.html"><a href="advanced-computation.html#lab-3"><i class="fa fa-check"></i><b>6.4</b> Lab</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="advanced-computation.html"><a href="advanced-computation.html#gaussian-processes-1"><i class="fa fa-check"></i><b>6.4.1</b> Gaussian Processes</a></li>
<li class="chapter" data-level="6.4.2" data-path="advanced-computation.html"><a href="advanced-computation.html#missing-data"><i class="fa fa-check"></i><b>6.4.2</b> Missing Data</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Inference and Computation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="markov-chain-monte-carlo" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">Chapter 5</span> Markov Chain Monte Carlo<a href="markov-chain-monte-carlo.html#markov-chain-monte-carlo" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Markov Chain Monte Carlo (MCMC) is a class of algorithms that produce samples from a probability distribution. These methods combine the idea of rejection sampling with the theory of Markov chains. Before we set out the theory of Markov chains, we’ll go through an example to show how MCMC works.</p>
<div class="example">
<p><span id="exm:King" class="example"><strong>Example 5.1  </strong></span>(<a href="https://xcelab.net/rm/statistical-rethinking/">Adapted from Statistical Rethinking 9</a>) Consider an eccentric King whose kingdom consists of a ring of 10 islands. Directly north is island one, the smallest island. Going clockwise around the archipelago, next is island two, which is twice the size of island one, then island three, which is three times as large as island one. Finally, island 10 is next to island one and ten times as large.</p>
<p>The King wanted to visit all of his islands, but spending time on each one according to its size. That is he should spend the most time on island ten and the least on island one. Being climate conscious, he also decided that flying from one side of the archipelago to the other was not allowed. Instead, he would only sail from one island to either of its neighbors. So from island one, he could reach islands two and ten.</p>
<p>He decided to travel according to these rules:</p>
<ol style="list-style-type: decimal">
<li><p>At the end of each week, he decides to stay on the same island or move to a neighboring island according to a coin toss. If it’s heads he proposes moving clockwise, and tails anti-clockwise. The island he is considering moving to is called the proposal island.</p></li>
<li><p>To decide if he is going to move to the proposal island, the King counts out a number of shells equal to the number of size of the island. So if island five is the proposal island, he counts out five shells. He then counts out a number of stones equal to the size of the current island.</p></li>
<li><p>If the number of seashells is greater than the number of stones, he moves to the proposed island. If the number of seashells is less than the number of stones, he takes a different strategy. He discards the number of stones equal to the number of seashells. So if there are six stones and five seashells, he ends up with 6-5=1 stones. He then places the stones and seashells into a bag and pulls one at random. If he picks a seashell, he moves to the proposed island, otherwise if he picks a stone, he stays put.</p></li>
</ol>
<p>This is a complex way of moving around, but it produces the required result; the time he spends on each island is proportionate to the size of the island. The code below shows an example of this over 10,000 weeks.</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="markov-chain-monte-carlo.html#cb100-1" tabindex="-1"></a>weeks <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb100-2"><a href="markov-chain-monte-carlo.html#cb100-2" tabindex="-1"></a>island <span class="ot">&lt;-</span> <span class="fu">numeric</span>(weeks)</span>
<span id="cb100-3"><a href="markov-chain-monte-carlo.html#cb100-3" tabindex="-1"></a>current <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb100-4"><a href="markov-chain-monte-carlo.html#cb100-4" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>weeks){</span>
<span id="cb100-5"><a href="markov-chain-monte-carlo.html#cb100-5" tabindex="-1"></a>  <span class="do">## record current position</span></span>
<span id="cb100-6"><a href="markov-chain-monte-carlo.html#cb100-6" tabindex="-1"></a>  island[i] <span class="ot">&lt;-</span> current</span>
<span id="cb100-7"><a href="markov-chain-monte-carlo.html#cb100-7" tabindex="-1"></a>  </span>
<span id="cb100-8"><a href="markov-chain-monte-carlo.html#cb100-8" tabindex="-1"></a>  <span class="co">#Flip a coin to move to a propose a new island</span></span>
<span id="cb100-9"><a href="markov-chain-monte-carlo.html#cb100-9" tabindex="-1"></a>  proposed <span class="ot">&lt;-</span> current <span class="sc">+</span> <span class="fu">sample</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>), <span class="at">size =</span> <span class="dv">1</span>)</span>
<span id="cb100-10"><a href="markov-chain-monte-carlo.html#cb100-10" tabindex="-1"></a>  </span>
<span id="cb100-11"><a href="markov-chain-monte-carlo.html#cb100-11" tabindex="-1"></a>  <span class="co">#Ensure he loops round the island</span></span>
<span id="cb100-12"><a href="markov-chain-monte-carlo.html#cb100-12" tabindex="-1"></a>  <span class="cf">if</span>(proposed <span class="sc">&lt;</span> <span class="dv">1</span>) </span>
<span id="cb100-13"><a href="markov-chain-monte-carlo.html#cb100-13" tabindex="-1"></a>    proposed <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb100-14"><a href="markov-chain-monte-carlo.html#cb100-14" tabindex="-1"></a>  <span class="cf">if</span>(proposed <span class="sc">&gt;</span> <span class="dv">10</span>)</span>
<span id="cb100-15"><a href="markov-chain-monte-carlo.html#cb100-15" tabindex="-1"></a>    proposed <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb100-16"><a href="markov-chain-monte-carlo.html#cb100-16" tabindex="-1"></a>  </span>
<span id="cb100-17"><a href="markov-chain-monte-carlo.html#cb100-17" tabindex="-1"></a>  <span class="co">#Decide to move</span></span>
<span id="cb100-18"><a href="markov-chain-monte-carlo.html#cb100-18" tabindex="-1"></a>  p <span class="ot">&lt;-</span> proposed<span class="sc">/</span>current</span>
<span id="cb100-19"><a href="markov-chain-monte-carlo.html#cb100-19" tabindex="-1"></a>  u <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>)</span>
<span id="cb100-20"><a href="markov-chain-monte-carlo.html#cb100-20" tabindex="-1"></a>  <span class="cf">if</span>(u <span class="sc">&lt;</span> p)</span>
<span id="cb100-21"><a href="markov-chain-monte-carlo.html#cb100-21" tabindex="-1"></a>    current <span class="ot">&lt;-</span> proposed</span>
<span id="cb100-22"><a href="markov-chain-monte-carlo.html#cb100-22" tabindex="-1"></a>}</span>
<span id="cb100-23"><a href="markov-chain-monte-carlo.html#cb100-23" tabindex="-1"></a></span>
<span id="cb100-24"><a href="markov-chain-monte-carlo.html#cb100-24" tabindex="-1"></a><span class="co">#Plot results</span></span>
<span id="cb100-25"><a href="markov-chain-monte-carlo.html#cb100-25" tabindex="-1"></a><span class="fu">plot</span>(island, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, <span class="at">xlab =</span> <span class="st">&quot;Week&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Island&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-36-1.png" width="672" /></p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="markov-chain-monte-carlo.html#cb101-1" tabindex="-1"></a><span class="fu">barplot</span>(<span class="fu">table</span>(island)<span class="sc">/</span>weeks, <span class="at">xlab =</span> <span class="st">&quot;Island&quot;</span>, </span>
<span id="cb101-2"><a href="markov-chain-monte-carlo.html#cb101-2" tabindex="-1"></a>        <span class="at">ylab =</span> <span class="st">&quot;Proportion of time&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-36-2.png" width="672" /></p>
</div>
<p>We can recognise several different statistical principles in this example. The King decides to move islands dependent on where he is currently, not based on where he has been previously (Markov property). He proposes an island to move to and accepts or rejects this decision based on some distribution (rejection principle). We are now going to describe some of the properties of Markov chains, including the Markov property.</p>
<div id="properties-of-markov-chains" class="section level2 hasAnchor" number="5.1">
<h2><span class="header-section-number">5.1</span> Properties of Markov Chains<a href="markov-chain-monte-carlo.html#properties-of-markov-chains" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="definition">
<p><span id="def:Markov" class="definition"><strong>Definition 5.1  </strong></span>A sequence of random variables <span class="math inline">\(Y = (Y_n)_{n \in \mathbb{N}_0}\)</span> is a <strong><em>Markov chain</em></strong> if <span class="math inline">\(\mathbb{P}(Y_{n+1} \in \cdot \mid Y_{n}, \ldots, Y_0) = \mathbb{P}(Y_{n+1} \in \cdot \mid Y_{n})\)</span> a.s. That is, the distribution of the next state <span class="math inline">\(Y_{n+1}\)</span> conditional on its entire past only depends on the current state <span class="math inline">\(Y_n\)</span>.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-80" class="definition"><strong>Definition 5.2  </strong></span>A Markov chain is called <strong><em>homogeneous</em></strong> if for any <span class="math inline">\(n,m \in \mathbb{N}_0\)</span> it holds that <span class="math inline">\(\mathbb{P}(Y_{n+ m} \in \cdot \mid Y_n) = \mathbb{P}(Y_{m} \in \cdot \mid Y_0)\)</span>. That is, transition probabilities only depend on the time differences.</p>
</div>
<p>From here on, we will only deal with homogeneous Markov chains and will therefore synonymously use the terms <em>Markov chain</em> and <em>homogeneous Markov chain</em>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-81" class="definition"><strong>Definition 5.3  </strong></span>If for some finite set <span class="math inline">\(S = \{i_1,\ldots,i_N\}\)</span> we have <span class="math inline">\(\mathbb{P}(Y_n \in S) = 1\)</span> for all <span class="math inline">\(n \in \mathbb{N}_0\)</span>, we call <span class="math inline">\(Y\)</span> a <strong><em>finite Markov chain</em></strong> and <span class="math inline">\(S\)</span> its <strong><em>state space</em></strong>.</p>
</div>
<p>By relabelling, we may always assume wlog that <span class="math inline">\(S = \{1,\ldots,N\}\)</span> for a finite Markov chain with <span class="math inline">\(N\)</span> states.</p>
<div class="definition">
<p><span id="def:unlabeled-div-82" class="definition"><strong>Definition 5.4  </strong></span>For a finite Markov chain the probability of transitioning from state <span class="math inline">\(i\)</span> to state <span class="math inline">\(j\)</span> in a Markov chain is given by <span class="math inline">\(p_{ij}\)</span>. The <strong><em>transition matrix</em></strong> for a Markov chain with <span class="math inline">\(N\)</span> states is the <span class="math inline">\(N \times N\)</span> matrix <span class="math inline">\(P = (p_{ij})_{i,j=1,\ldots,N}\)</span>, where the <span class="math inline">\(\{i, j\}^{th}\)</span> entry <span class="math inline">\(p_{ij}\)</span> is the probability of moving from state <span class="math inline">\(i\)</span> to state <span class="math inline">\(j\)</span>, i.e., <span class="math inline">\(\mathbb{P}(Y_{n+1} = j \mid Y_n = i) = \mathbb{P}(Y_{1} = j \mid Y_0 = i).\)</span></p>
</div>
<p>These properties make Markov chains nice to work with. Two other important definitions are:</p>
<div class="definition">
<p><span id="def:unlabeled-div-83" class="definition"><strong>Definition 5.5  </strong></span>For a finite Markov chain, the <strong><em>period</em></strong> of a state <span class="math inline">\(i\)</span> is given by <span class="math inline">\(d_i = \textrm{gcd}\{n &gt; 0; P^n(i,i) &gt; 0 \}\)</span>. A state is <strong><em>aperiodic</em></strong> if <span class="math inline">\(d_i = 1\)</span>. An <strong><em>aperiodic chain</em></strong> is a chain where all states are aperiodic.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-84" class="definition"><strong>Definition 5.6  </strong></span>A finite Markov chain is <strong><em>irreducible</em></strong> if for any <span class="math inline">\(i,j \in S\)</span> there exists an <span class="math inline">\(n \in \mathbb{N}_0\)</span> such that <span class="math inline">\(\mathbb{P} (Y_n = j \mid Y_0 = i) &gt; 0\)</span>. In other words, it is possible to move from any state to any other state in a finite number of steps.</p>
</div>
<p>We can use these definitions to start working with distributions. Suppose, the state we start at is drawn from some distribution <span class="math inline">\(Y_0 \sim q\)</span>. Then the distributions of the second state <span class="math inline">\(Y_1\)</span> depends on the distribution of <span class="math inline">\(Y_0\)</span> and the transition probabilities
<span class="math display">\[
\mathbb{P}(Y_1 = j) = \sum_{i=1}^N q_ip_{ij}.
\]</span>
If we denote the distribution of <span class="math inline">\(Y_1 \sim {q}^{(1)}\)</span>, then we can write it in terms of the transition matrix <span class="math inline">\({q}^{(1)} = {q}P\)</span>. Now suppose we would like to determine the distribution of <span class="math inline">\(Y_2 \sim {q}^{(2)}\)</span>; thanks to the Markov property, this is the distribution for <span class="math inline">\(Y_1\)</span> multiplied by the transition matrix from the right, so <span class="math inline">\(Y_2 \sim qP^2\)</span>. Inductively, <span class="math inline">\(Y_k \sim qP^{k}\)</span>. To use Markov chains to sample from distributions, we need to identify the eigenvalues of the transition matrix.</p>
<div class="definition">
<p><span id="def:unlabeled-div-85" class="definition"><strong>Definition 5.7  </strong></span>Suppose that there exists a probability vector <span class="math inline">\(\pi\)</span> (that is, <span class="math inline">\(\pi_i \geq 0\)</span> and <span class="math inline">\(\sum_{i=1}^n \pi(i) = 1\)</span>) that is a left-eigenvector with eigenvalue <span class="math inline">\(1\)</span> for the transition matrix <span class="math inline">\(P\)</span>, i.e.,
<span class="math display">\[
\pi P = \pi.
\]</span>
Then <span class="math inline">\(\pi\)</span> is called the <strong><em>stationary distribution</em></strong> of the Markov chain corresponding to <span class="math inline">\(P\)</span>.</p>
</div>
<p>The terminology arises from the fact that if the Markov chain is started in <span class="math inline">\(\pi\)</span>, the marginal distributions <span class="math inline">\(\mathbb{P}_{Y_n}\)</span> remain unchanged (they are <strong><em>invariant</em></strong> under time translation), since then
<span class="math display">\[q^{(n)} = \pi P^n = (\pi P)P^{n-1} = \pi P^{n-1}  = \cdots = \pi.\]</span>
A basic fact is the following.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-86" class="theorem"><strong>Theorem 5.1  </strong></span>If a finite Markov chain is irreducible, a unique stationary distribution exists.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-87" class="definition"><strong>Definition 5.8  </strong></span>A finite Markov chain is called ergodic if for all <span class="math inline">\(i,j \in S\)</span> it holds that
<span class="math display">\[\lim_{n \to \infty} \mathbb{P}(Y_n = i \mid Y_0 = j) = \pi_i.\]</span>
Equivalently, for any initial distribution <span class="math inline">\(q\)</span>,
<span class="math display">\[\lim_{n \to \infty} qP^n = \pi.\]</span></p>
</div>
<p>Irreducibility is not sufficient to guarantee ergodicity of a Markov chain: take a two state Markov chain with transition matrix
<span class="math display">\[P = \begin{pmatrix} 0 &amp; 1\\ 1 &amp; 0 \end{pmatrix},\]</span>
that is the chain deterministically moves from state <span class="math inline">\(1\)</span> to <span class="math inline">\(2\)</span> and vice versa. Then <span class="math inline">\(Y\)</span> is clearly irreducible and <span class="math inline">\(\pi = (1/2,1/2)\)</span> is its unique stationary distribution since<br />
<span class="math display">\[(1/2, 1/2) =  \begin{pmatrix} 0 &amp; 1\\ 1 &amp; 0 \end{pmatrix} = (1/2,1/2).\]</span>
However, if started in <span class="math inline">\(1\)</span>, the chain does not converge, since then <span class="math inline">\(\mathbb{P}(Y_{2n} = 1) = 1 \neq 1/2 = \pi(1).\)</span> The problem is the lack of aperiodicity, since by its very nature, <span class="math inline">\(d_1 = d_2 = 2 &gt; 1\)</span>. In fact, aperiodicity is the missing piece to guarantee convergence to the stationary distribution, irrespectively of the initialisation:</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-88" class="theorem"><strong>Theorem 5.2  </strong></span>A finite Markov chain is ergodic if, and only if, it is irreducible and aperiodic.</p>
</div>
<p>This important concept underpins MCMC methods. It says that no matter where we start our ergodic chain, we’ll eventually end up sampling states according to the target distribution <span class="math inline">\(\pi\)</span>. It make take a long time to reach the stationary distribution, but it will eventually get there.</p>
<p>In order to check whether our finite Markov chain will converge to a stationary distribution, we therefore need to check:</p>
<ol style="list-style-type: decimal">
<li><p>the Markov chain is irreducible, and</p></li>
<li><p>the Markov chain is aperiodic.</p></li>
</ol>
<p>When we want to sample from a prescribed distribution <span class="math inline">\(\pi\)</span> we additionally need to check the following condition for a finite Markov chain satisfying the above:</p>
<ol start="3" style="list-style-type: decimal">
<li><span class="math inline">\(\pi\)</span> is stationary for the Markov chain.</li>
</ol>
<div class="example">
<p><span id="exm:unlabeled-div-89" class="example"><strong>Example 5.2  </strong></span>In Example <a href="markov-chain-monte-carlo.html#exm:King">5.1</a>, the King wanted to visit the islands according to how large they are. We can think of the islands as the states and the stationary distribution as <span class="math inline">\(p(Y = i) \propto i\)</span>. The eccentric method the King used allowed him to construct a transition matrix for an aperiodic Markov chain. He also never visited islands regularly using this method.</p>
</div>
<p>When designing a Markov chain, it is usually straightforward to design one that meets conditions one and two. Constructing a chain with prescribed stationary distribution <span class="math inline">\(\pi\)</span> is more difficult, but a detailed balance criterion can help with this.</p>
<div class="definition">
<p><span id="def:unlabeled-div-90" class="definition"><strong>Definition 5.9  </strong></span>The Markov chain with transition matrix <span class="math inline">\(P\)</span> satisfies the <strong>detailed balance</strong> equation with respect to the distribution <span class="math inline">\(\pi\)</span> if
<span class="math display">\[
\pi_i p_{ij} = \pi_j p_{ji}.
\]</span></p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-91" class="theorem"><strong>Theorem 5.3  (Detailed Balance) </strong></span>Let <span class="math inline">\(P\)</span> be a transition matrix that satisfies detailed balance with respect to the distribution <span class="math inline">\(\pi\)</span>. Then <span class="math inline">\(\pi P = \pi\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-92" class="proof"><em>Proof</em>. </span>The <span class="math inline">\(j^{th}\)</span> entry of <span class="math inline">\(\pi P\)</span> is
<span class="math display">\[\begin{align*}
\sum_{i} \pi_i p_{ij} &amp; = \sum_{i} \pi_j p_{ji} \quad \textrm{(detailed balance)} \\
&amp; = \pi_j \sum_{i} p_{ji} \\
&amp; = \pi_j.\qquad \textrm{(probaility sums to 1)}
\end{align*}\]</span>
Hence <span class="math inline">\(\pi P = \pi\)</span>.</p>
</div>
<p>So far, we have shown that we can use <em>finite</em> state Markov chain theory to simulate from a <em>discrete</em> probability distribution <span class="math inline">\(\pi\)</span> by running a Markov chain with <span class="math inline">\(\pi\)</span> as a stationary distribution for a sufficently long time. However, for discrete distributions this is of course overkill, since we can easily sample from such distributions via the inverse transformation method. All of the above concepts translate however in a natural way to Markov chains with more complicated state spaces, and we will just briefly comment on this now:</p>
<p>The generalisation of the transition matrix is a <strong><em>Markov kernel</em></strong> <span class="math inline">\(P \colon S \times \mathcal{B}(S) \to [0,1]\)</span> on a general Borel state space <span class="math inline">\((S,\mathcal{B}(S))\)</span>, which satisfies the following properties:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\forall x \in S: \quad P(x, \cdot) \text{ is a probability measure}\)</span>,</li>
<li><span class="math inline">\(\forall A \in \mathcal{B}(S): \quad x \mapsto P(x, A) \text{ is measurable.}\)</span></li>
</ol>
<p>For two Markov kernels <span class="math inline">\(P,Q\)</span> we let
<span class="math display">\[Q\circ P(x,A) = \int_{S} P(x, dy ) Q(y,A), \quad (x,A) \in S \times \mathcal{B}(S),\]</span>
and for a probability measure <span class="math inline">\(q\)</span> on <span class="math inline">\(S\)</span> let <span class="math inline">\(qP\)</span> be the probability measure
<span class="math display">\[qP(A) = \int_S q(dx) P(x,A), \quad A \in \mathcal{B}(S).\]</span></p>
<p>If <span class="math inline">\(S\)</span> is nice enough, the Markov property is equivalent to demanding that for some Markov kernel <span class="math inline">\(P\)</span> we have
<span class="math display">\[\forall A \in \mathcal{B}(S): \quad \mathbb{P}(Y_{n+1} \in A \mid Y_n, \ldots, Y_0) = P(Y_n, A). \]</span>
Then, if we define iteratively the Markov kernels <span class="math inline">\(P^1 = P\)</span> and
<span class="math display">\[P^{n+1}(x,A) = P \circ P^n(x,A) = \int_{S} P^n(x, dy) P(y, A), \quad (x,A) \in S \times \mathcal{B}(S),\]</span>
we have the <em>Chapman–Kolmogorov equation</em>
<span class="math display">\[
P^{n+m} = P^n \circ P^m
\]</span>
and
<span class="math display">\[
\mathbb{P}(Y_1 \in A_1, \ldots, Y_n \in A_n \mid Y_0 = x) = \int_S P(x, dy_0)  \int_{A_1 \times \cdots \times A_n} \prod_{i=1}^n P(y_{i-1}, dy_i).
\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-93" class="example"><strong>Example 5.3  </strong></span>Let <span class="math inline">\(X= (X_n)_{n\in \mathbb{N}}\)</span> be an i.i.d. sequence of <span class="math inline">\(\mathbb{R}^d\)</span>-valued random variables with distribution <span class="math inline">\(\mathbb{P}_{X_1} = \eta\)</span> and <span class="math inline">\(X_0 \sim \mu\)</span> be independent of <span class="math inline">\((X_n)_{n\in \mathbb{N}}\)</span>. A <strong><em>random walk</em></strong> <span class="math inline">\(Y\)</span> associated to the sequence of innovations <span class="math inline">\(X\)</span> and initialistion <span class="math inline">\(X_0\)</span> is defined by <span class="math inline">\(Y_0 = X_0\)</span> and
<span class="math display">\[Y_n = X_0 + \sum_{i=1}^n X_i, \quad n \in \mathbb{N},\]</span>
which can be equivalently characterised by the recursive relation
<span class="math display">\[Y_{n+1} = Y_n + X_{n+1}, \quad n \in \mathbb{N}_0. \]</span>
This immediately yields the Markov property
<span class="math display">\[\mathbb{P}(Y_{n+1} \in A \mid Y_0,\ldots,Y_n) = \mathbb{P}(Y_{n+1} \in A \mid Y_n) \]</span>
and defining for <span class="math inline">\(A -x := \{z = y -x: y \in A\}\)</span> the Markov kernel
<span class="math display">\[P(x,A) = \mathbb{P}(X_1 \in A - x) = \eta(A -x) \]</span>
we obtain
<span class="math display">\[\mathbb{P}(Y_{n+1} \in A \mid Y_n) = \mathbb{P}(Y_n + X_{n+1} \in A \mid Y_n) = \mathbb{P}(X_{n+1} \in A - Y_n \mid Y_n) = P(Y_n, A). \]</span></p>
</div>
<div class="example">
<p><span id="exm:Langevin" class="example"><strong>Example 5.4  </strong></span>Let <span class="math inline">\((\xi_n)_{n \in \mathbb{N}} \overset{\mathrm{i.i.d}}{\sim} N(0, I)\)</span> and <span class="math inline">\(V \colon \mathbb{R}^d \to \mathbb{R}\)</span> be a continuously differentiable function. Then, for <span class="math inline">\(Y_0 \sim \mu\)</span> independent of <span class="math inline">\((\xi_n)_{n \in \mathbb{N}}\)</span> and a <em>step size</em> <span class="math inline">\(h &gt; 0\)</span> we define recursively
<span class="math display">\[\begin{align*}
Y_{n+1} &amp;= Y_n - h \nabla V(Y_n) + \sqrt{2h} \xi_{n+1}, \quad n \in \mathbb{N}_0.
\end{align*}\]</span>
This is very reminiscent of a gradient descent algorithm with an additional stochastic perturbation that is distributed according to <span class="math inline">\(N(0,2hI)\)</span> and therefore centered and rather small for small step sizes <span class="math inline">\(h\)</span>. We call this Markov chain a <em>Langevin Markov Chain with step size <span class="math inline">\(h\)</span> and potential <span class="math inline">\(V\)</span></em>. Because the innovations are i.i.d. the chain is clearly Markov and since <span class="math inline">\(Y_{n +1} \mid Y_n \sim N(Y_n - h \nabla V(Y_n), 2h I )\)</span> its Markov kernel is given by
<span class="math display">\[P(x,dy) = \phi_{x - h\nabla V(x), 2h I }(y) \, dy, \]</span>
where <span class="math inline">\(\phi_{\mu,\Sigma}\)</span> denotes the pdf of a normal random vector with mean <span class="math inline">\(\mu\)</span> and covariance matrix <span class="math inline">\(\Sigma\)</span>.</p>
</div>
<p>Analogously to the finite state space case we then call a distribution <span class="math inline">\(\pi\)</span> on <span class="math inline">\(S\)</span> a <strong><em>stationary distribution</em></strong> if
<span class="math display">\[\pi P = \pi \qquad (\iff \forall A \in \mathcal{B}(S): \int \pi(dx)\, P(x,A) = \pi(A)),\]</span>
and thus iteratively
<span class="math display">\[\mathbb{P}(Y_n \in \cdot \mid Y_0 \sim \pi) = \pi.\]</span></p>
<div class="definition">
<p><span id="def:unlabeled-div-94" class="definition"><strong>Definition 5.10  </strong></span>A Markov chain <span class="math inline">\(Y\)</span> with unique stationary distribution <span class="math inline">\(\pi\)</span> is called <strong><em>ergodic</em></strong> if
<span class="math display">\[\forall x \in S: \quad \lim_{n \to \infty} \lVert P^n(x,\cdot) - \pi \rVert_{\mathrm{TV}} = 0,\]</span>
where we recall that for two probability measures <span class="math inline">\(\mu,\nu\)</span> on <span class="math inline">\(S\)</span> their total variation distance is given by
<span class="math display">\[\lVert \mu - \nu \rVert_{\mathrm{TV}} = \sup_{A \in \mathcal{B}(S)} \lvert \mu(A) - \nu(A) \rvert.\]</span></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-95" class="example"><strong>Example 5.5  </strong></span>Let’s return to the Langevin Markov chain from above, where
<span class="math display">\[Y_{n+1} = Y_n - h\nabla V(Y_n) + \sqrt{2h} \xi_{n+1}, \quad (\xi_n)_{n\in \mathbb{N}} \overset{\mathrm{i.i.d.}}{\sim} N(0,I).  \]</span>
Given appropriate conditions on <span class="math inline">\(h\)</span> the potential <span class="math inline">\(V\)</span>, which are similar to those that guarantee convergence of a gradient descent algorithm to obtain minimisers of <span class="math inline">\(V\)</span> (say, strong convexity and Lipschitz-continuity), the Markov chain is ergodic with invariant distribution <span class="math inline">\(\pi^h\)</span>. Here, for sufficiently small step sizes <span class="math inline">\(h\)</span> we have <span class="math inline">\(\pi^h \approx \pi\)</span>, where <span class="math inline">\(\pi\)</span> has density proportional to
<span class="math display">\[\pi(y) \propto \mathrm{e}^{-V(y)}, \quad y \in \mathbb{R}^d. \]</span>
This is our first example of an MCMC algorithm for the target density <span class="math inline">\(\pi\)</span> (called the <strong><em>Langevin Monte-Carlo algorithm</em></strong>) and is especially relevant in high dimensions, where the normalising constant <span class="math inline">\(\int_{\mathbb{R}^d} \mathrm{e}^{-V(y)}\, dy\)</span> is very expensive to calculate numerically, therefore making rejection algorithms infeasible.</p>
</div>
<p>Determining whether a general Markov chain is ergodic is a much more demanding task than in the finite state space situation. For a given Markov kernel, verification is based on checking extensions of the notions of irreducibility, aperiodicity and <strong><em>recurrence</em></strong> for countable Markov chains, where the latter property encodes the idea that return times to any state <span class="math inline">\(i\)</span> should be finite almost surely. The main challenge for general state spaces is that there need not be any states <span class="math inline">\(x \in S\)</span> such that <span class="math inline">\(P(y,\{x\}) &gt; 0\)</span> for any <span class="math inline">\(y \in S\)</span> (e.g. when the transition probabilities are absolutely continuous). Instead of going into technical details, we only cite a quantitative result from <span class="citation">(<a href="#ref-hairer11">Hairer and Mattingly 2011</a>)</span> that provides <strong><em>exponentially fast</em></strong> convergence to the invariant distribution given explicit criteria on <span class="math inline">\(P\)</span>.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-96" class="theorem"><strong>Theorem 5.4  </strong></span>Suppose that <span class="math inline">\(Y\)</span> satisfies the following conditions:</p>
<ol style="list-style-type: decimal">
<li><p><strong><em>Drift condition</em></strong>: There exists a <em>penalty function</em> <span class="math inline">\(V\colon S \to [1,\infty)\)</span> and constants <span class="math inline">\(K \geq 0, \gamma \in (0,1)\)</span> such that
<span class="math display">\[\forall x \in S: \underbrace{\int_S V(y)\, P(x,dy)}_{= \mathbb{E}[V(Y_1) \mid Y_0 = x]}  \leq \gamma V(x) + K. \]</span></p></li>
<li><p><strong><em>Minorisation condition</em></strong>: There exists a constant <span class="math inline">\(\alpha \in (0,1)\)</span> and a probability measure <span class="math inline">\(\nu\)</span> such that
<span class="math display">\[\forall A \in \mathcal{B}(S): \quad \inf_{x \in C} P(x,A) \geq \alpha \nu(A)\]</span>
where <span class="math inline">\(C = \{x \in S: V(x) \leq R\}\)</span> for some <span class="math inline">\(R &gt; 2K/(1-\gamma).\)</span></p></li>
</ol>
<p>Then, <span class="math inline">\(Y\)</span> has a unique stationary distribution <span class="math inline">\(\pi\)</span> and there exist explicit constants <span class="math inline">\(D &gt; 0, \kappa \in (0,1)\)</span> such that
<span class="math display">\[\lVert P^n(x,\cdot) - \pi \rVert_{\mathrm{TV}} \leq DV(x) \kappa^n, \quad x \in S.\]</span></p>
</div>
<p>As a corollary we obtain the following result, which is mostly useful for bounded state spaces.</p>
<div class="corollary">
<p><span id="cor:doeblin" class="corollary"><strong>Corollary 5.1  </strong></span>Suppose that for some probability measure <span class="math inline">\(\nu\)</span> and <span class="math inline">\(\alpha \in (0,1)\)</span> the following <strong><em>Doeblin minorisation criterion</em></strong> holds:
<span class="math display">\[\forall x \in S, A \in \mathcal{B}(S): \quad P(x,A) \geq \alpha \nu(A). \]</span>
Then, there exist constants <span class="math inline">\(D &gt; 0,\kappa \in (0,1)\)</span> such that
<span class="math display">\[\sup_{x \in S} \lVert P^n(x,\cdot) - \pi \rVert_{\mathrm{TV}} \leq D\kappa^n.\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-97" class="proof"><em>Proof</em>. </span>Choose <span class="math inline">\(\gamma = 1/2, V \equiv 1, K = 1/2\)</span>. Then, for arbitrary <span class="math inline">\(R &gt; 2 = 2K/(1-\gamma)\)</span>, we have <span class="math inline">\(C = \{x \in S: V(x) \leq R\} = S\)</span> and hence by assumption
<span class="math display">\[\inf_{x \in C} P(x,A) = \inf_{x \in S} P(x,A)  \geq \alpha \nu.\]</span>
Moreover, for any <span class="math inline">\(x \in S\)</span> by construction.
<span class="math display">\[\mathbb{E}[V(Y_1) \mid Y_0 = x] = 1 = \gamma V(x) +K.\]</span><br />
Consequently, the drift and minorisation criteria from the previous theorem are satisfied, yielding the assertion.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-98" class="remark"><em>Remark</em>. </span>It can be shown that <span class="math inline">\(\kappa = 1 - \alpha, D=1\)</span>, see Theorem 16.0.2 in <span class="citation">(<a href="#ref-meyntweedie">Meyn and Tweedie 2009</a>)</span>.</p>
</div>
<p>Such results provide the foundation theory for MCMC and allow us to sample approximatively from a posterior distribution <span class="math inline">\(\pi\)</span> for an appropriately constructed chain. What it doesn’t tell us is how to design the Markov chain, and that is what the next sections deal with. The following extension of the detailed balance condition will be helpful for this purpose:</p>
<div class="theorem">
<p><span id="thm:reversibility" class="theorem"><strong>Theorem 5.5  (Detailed Balance) </strong></span>Let <span class="math inline">\(P\)</span> be a Markov kernel and <span class="math inline">\(\pi\)</span> be a distribution on the state space <span class="math inline">\(S\)</span>. If <span class="math inline">\(\pi\)</span> and <span class="math inline">\(P\)</span> satisfy detailed balance in the sense
<span class="math display">\[\begin{align*} &amp;\forall x,y \in S: \pi(dx)\, P(x,dy) = \pi(dy)\, P(y,dx) \\
&amp;(\text{i.e.,} \forall B \in \mathcal{B}(S) \otimes \mathcal{B}(S): \int \mathbf{1}_B(x,y)\, \pi(dx) \, P(x,dy) = \int \mathbf{1}_B(x,y)\, \pi(dy) \, P(y,dx)
\end{align*}\]</span>
then <span class="math inline">\(\pi P = \pi\)</span>, i.e., <span class="math inline">\(\pi\)</span> is stationary.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-99" class="proof"><em>Proof</em>. </span>Using detailed balance and Fubini, we find for any <span class="math inline">\(A \in \mathcal{B}(S)\)</span>,
<span class="math display">\[\begin{align*}
\pi P(A) &amp;= \int_{x \in S} \int_{y \in A} \pi(dx)\, P(x,dy) =\int_{x \in S} \int_{y \in A} \pi(dy) \, P(y,dx)\\
&amp;= \int_{y \in A}  \underbrace{\Big(\int_{x \in S} P(y,dx) \Big)}_{= \mathbb{P}(Y_1 \in S \mid Y_0 = y) = 1} \, \pi(dy) = \pi(A).
\end{align*}\]</span></p>
</div>
</div>
<div id="metropolishastings" class="section level2 hasAnchor" number="5.2">
<h2><span class="header-section-number">5.2</span> Metropolis–Hastings<a href="markov-chain-monte-carlo.html#metropolishastings" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We’re now going to look at MCMC algorithms. The first algorithm we are going to look at is the Metropolis–Hastings algorithm. This is a useful algorithm if we cannot sample directly from the posterior distribution and if the conditional distributions do not have a closed form. The Metropolis–Hastings algorithm is like the island example we saw earlier. At each iteration, we propose a new sample and then accept or reject it based on the likelihood function, the prior and how likely we are to propose this new sample given the current one.</p>
<p>Suppose we want to sample from the posterior distribution <span class="math inline">\(\pi(\theta \mid \boldsymbol{y})\)</span>. The Metropolis–Hastings works as follows:</p>
<ol style="list-style-type: decimal">
<li><p>Set the initial value <span class="math inline">\(\theta^{(0)}\)</span>.</p></li>
<li><p>Set <span class="math inline">\(i = 1\)</span></p></li>
<li><p>Propose a new value of <span class="math inline">\(\theta&#39;\)</span> from some distribution <span class="math inline">\(q(\cdot \mid \theta)\)</span> for <span class="math inline">\(\theta = \theta^{(i-1)}\)</span></p></li>
<li><p>Accept <span class="math inline">\(\theta&#39; = \theta^{(i)}\)</span> with probability
<span class="math display">\[
p_{\textrm{acc}} = p_{\textrm{acc}}(\theta,\theta^\prime) = \min\left\{\frac{\pi(\theta&#39; \mid \boldsymbol{y})}{\pi(\theta \mid \boldsymbol{y})}\frac{q(\theta \mid \theta&#39;)}{q(\theta&#39; \mid \theta)}, 1\right\},
\]</span>
otherwise set <span class="math inline">\(\theta^{(i)} = \theta\)</span> (in the acceptance probability use the convention <span class="math inline">\(x/0 = +\infty\)</span> for any <span class="math inline">\(x\)</span>)</p></li>
<li><p>Set <span class="math inline">\(i = i+1\)</span> and repeat steps 3 to 4 for <span class="math inline">\(i = 2, \ldots, n-1\)</span>.</p></li>
</ol>
<p>This yields a Markov chain with transition kernel given by
<span class="math display">\[P(\theta,d\theta^\prime) = p_{\textrm{acc}}(\theta,\theta^\prime) q(\theta^\prime \mid \theta)\, d\theta^\prime + \delta_{\theta}(d \theta^\prime) \int (1- p_{\mathrm{acc}}(\theta,z)) q(z \mid \theta) \, dz. \]</span>
Let us also note that the specific choice of a posterior as target sampling density is just for the sake of exposition in this chapter. The algorithm can be applied to <em>any</em> target distribution <span class="math inline">\(\pi\)</span>.
There are two parts to the acceptance probability in step 4. The first is the posterior ratio, similar to saying the likelihood of <span class="math inline">\(\theta&#39;\)</span> given the observed data over the likelihood of <span class="math inline">\(\theta\)</span> given the data. The second is the proposal ratio. It is similar to saying the likelihood of proposing <span class="math inline">\(\theta\)</span> given the current value <span class="math inline">\(\theta&#39;\)</span>, over the likelihood of proposing <span class="math inline">\(\theta&#39;\)</span> given the current value <span class="math inline">\(\theta\)</span>.</p>
<p>In practice, we don’t need to evaluate the full posterior distribution. Recall
<span class="math display">\[
\pi(\theta \mid \boldsymbol{y}) = \frac{\pi(\boldsymbol{y} \mid \theta) \pi(\theta)}{\pi(y)}
\]</span>
As the the denominator doesn’t depend on <span class="math inline">\(\theta\)</span>, it cancels in the ratio. The ratio becomes
<span class="math display">\[
\frac{\pi(\theta&#39; \mid \boldsymbol{y})}{\pi(\theta \mid \boldsymbol{y})} = \frac{\pi(\boldsymbol{y} \mid \theta&#39;) \pi(\theta&#39;)}{\pi(\boldsymbol{y} \mid \theta) \pi(\theta)}.
\]</span>
This is the likelihood ratio multiplied by the prior ratio.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-100" class="theorem"><strong>Theorem 5.6  </strong></span>The Markov chain generated by the Metropolis–Hastings algorithm satisfies detailed balance with respect to the posterior distribution and the posterior is stationary.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-101" class="proof"><em>Proof</em>. </span>For detailed balance we need to show that
<span class="math display">\[\pi(\theta \mid \boldsymbol{y}) P(\theta, d\theta^\prime) \, d\theta = \pi(\theta^\prime) P(\theta^\prime, d\theta) \, d\theta^\prime,  \]</span>
and stationarity will follow from that by Theorem <a href="markov-chain-monte-carlo.html#thm:reversibility">5.5</a>. The case <span class="math inline">\(\theta = \theta^\prime\)</span> is trivial, so let us consider the case <span class="math inline">\(\theta \neq \theta^\prime\)</span>. We then have
<span class="math display">\[\begin{align*}
\frac{P(\theta,d\theta^\prime)}{d\theta^\prime} &amp;= q(\theta&#39; \mid \theta)p_{acc}(\theta,\theta^\prime)\\
&amp;=  q(\theta&#39; \mid \theta)\min\left\{\frac{\pi(\theta&#39; \mid \boldsymbol{y})}{\pi(\theta \mid \boldsymbol{y})}\frac{q(\theta \mid \theta&#39;)}{q(\theta&#39; \mid \theta)}, \, 1\right\} \\
&amp; = \min\left\{\frac{\pi(\theta&#39; \mid \boldsymbol{y})}{\pi(\theta \mid \boldsymbol{y})}q(\theta \mid \theta&#39;),\, q(\theta&#39; \mid \theta)\right\},
\end{align*}\]</span>
which shows that the lhs of the desired detailed balance equation is given by
<span class="math display">\[
\pi(\theta \mid \boldsymbol{y})P(\theta, d\theta^\prime) d\theta = \min\{\pi(\theta&#39; \mid \boldsymbol{y})q(\theta \mid \theta&#39;),\, \pi(\theta \mid \boldsymbol{y})q(\theta&#39; \mid \theta)\} \, d\theta^\prime\,  d\theta.
\]</span>
By symmetric arguments, it follows that the rhs is given by
<span class="math display">\[
\pi(\theta^\prime \mid \boldsymbol{y})P(\theta^\prime, d\theta) \, d\theta^\prime = \min\{\pi(\theta \mid \boldsymbol{y})q(\theta^\prime \mid \theta),\, \pi(\theta^\prime \mid \boldsymbol{y})q(\theta \mid \theta&#39;)\} \, d\theta\, d\theta^\prime,
\]</span>
which establishes equality of the lhs and rhs, whence detailed balance for <span class="math inline">\(\theta \neq \theta^\prime\)</span>.</p>
</div>
<p>The speed of convergence of the Metropolis–Hastings algorithm and related MCMC algorithms has been a central research questions for the past 30 years or so, as this is fundamental to evaluate their sampling efficiency. Here, we focus on a simple case, where sampling works particularly well, and which sometimes can be applied to target posteriors <span class="math inline">\(\pi(\cdot \mid \boldsymbol{y}\)</span> that are uniformly bounded away from zero on its bounded support.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-102" class="theorem"><strong>Theorem 5.7  </strong></span>Suppose that <span class="math inline">\(\pi(\theta \mid \boldsymbol{y})\)</span> is a posterior with support <span class="math inline">\(S\)</span> and that there exist finite constants <span class="math inline">\(m,M &gt; 0\)</span> we have <span class="math inline">\(m \leq \inf_{\theta \in S} \pi(\theta \mid \boldsymbol{y}) \leq \sup_{\theta \in S} \pi(\theta \mid \boldsymbol{y}) \leq M\)</span>. Let also <span class="math inline">\(q\)</span> be a symmetric proposal density on <span class="math inline">\(S\)</span> (that is, <span class="math inline">\(q(\theta^\prime \mid \theta) = q(\theta \mid \theta^\prime)\)</span> for all <span class="math inline">\(\theta,\theta^\prime \in S\)</span>) such that <span class="math inline">\(q(\theta^\prime \mid \theta) \geq \rho\nu(\theta^\prime)\)</span> for all <span class="math inline">\(\theta \in S\)</span>, some <span class="math inline">\(\rho \in (0,1)\)</span> and some probability density <span class="math inline">\(\nu\)</span> on <span class="math inline">\(S\)</span>. Then, for the Metropolis–Hastings chain it holds that
<span class="math display">\[\sup_{x \in S} \lVert P^n(x,\cdot) - \pi(\cdot \mid \boldsymbol{y}) \rVert_{\mathrm{TV}} \leq \Big(1 - \frac{\rho m}{M}\Big)^n.\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-103" class="proof"><em>Proof</em>. </span>By symmetry of the proposal density and the upper and lower bounds on the posterior, it follows that<br />
<span class="math display">\[\begin{align*}
p_{\mathrm{acc}}(\theta,\theta^\prime) = \min\Big\{\frac{\pi(\theta^\prime \mid \boldsymbol{y})}{\pi(\theta \mid \boldsymbol{y})}, 1 \Big\} \geq \frac{m}{M}.
\end{align*}\]</span>
Therefore, for any <span class="math inline">\(A \in \mathcal{B}(S)\)</span> it follows that
<span class="math display">\[\begin{align*}
\forall \theta \in S: \quad P(\theta,A) &amp;= \int_A p_{\textrm{acc}}(\theta,\theta^\prime) q(\theta^\prime \mid \theta)\, d\theta^\prime + \delta_{\theta}(A) \int (1- p_{\mathrm{acc}}(\theta,z)) q(z \mid \theta) \, dz \\
&amp;\geq \int_A p_{\textrm{acc}}(\theta,\theta^\prime) q(\theta^\prime \mid \theta)\, d\theta^\prime \\
&amp;\geq \frac{m\rho}{M} \int_A \nu(\theta^\prime) \,d\theta^\prime.
\end{align*}\]</span>
Since the rhs is independent of <span class="math inline">\(\theta\)</span>, the Doeblin minorisation criterion is fulfilled with <span class="math inline">\(\alpha = \rho m/M\)</span> and Corollary <a href="markov-chain-monte-carlo.html#cor:doeblin">5.1</a> yields
<span class="math display">\[\sup_{x \in S} \lVert P^n(x,\cdot) - \pi(\cdot \mid \boldsymbol{y}) \rVert_{\mathrm{TV}} \leq (1-\alpha)^n = \Big(1 - \frac{\rho m}{M}\Big)^n.\]</span></p>
</div>
<div class="example">
<p><span id="exm:norm" class="example"><strong>Example 5.6  </strong></span>A counter monitors the time until atoms decays. It collects the data <span class="math inline">\(X_1, \ldots, X_N\)</span> and we assume <span class="math inline">\(X_i \sim \hbox{Exp}(\lambda)\)</span>. The time until these atom decay is long, certainly more than 1 second (or minute, hour etc.), so we assume <span class="math inline">\(\lambda \sim \hbox{Beta}(\alpha, \beta)\)</span>.</p>
<p>The prior distribution is given by
<span class="math display">\[
\pi(\lambda \mid \boldsymbol{x}) \propto \lambda^{N + \alpha - 1} (1 - \lambda)^{\beta - 1}\exp\left(-\lambda \sum x_i\right)
\]</span>
This doesn’t have a closed form, so we need to use a Metropolis–Hastings algorithm to generate samples from this distribution. A suitable algorithm is</p>
<ol style="list-style-type: decimal">
<li>Decide on an starting value the Markov chain and denote it by <span class="math inline">\(\lambda^{(0)}\)</span>. Set <span class="math inline">\(i = 0\)</span>.</li>
<li>Propose a new value for the parameter and denote it <span class="math inline">\(\lambda&#39;\)</span>. In this example, we are going to propose values using a random walk method where <span class="math inline">\(\lambda&#39; \sim N(\lambda^{(i)}, \sigma^2)\)</span>.</li>
<li>Accept <span class="math inline">\(\lambda&#39;\)</span> as the value <span class="math inline">\(\lambda^{(i+1)}\)</span> with probability <span class="math inline">\(p_{\textrm{acc}}\)</span>. Otherwise reject this value and set <span class="math inline">\(\lambda^{(i+1)} = \lambda^{(i)}\)</span>.<br />
</li>
<li>Set <span class="math inline">\(i = i + 1\)</span>.</li>
<li>Repeat steps 2, 3, 4, for <span class="math inline">\(i = 1, \ldots, n-1\)</span>.</li>
</ol>
<p>The value <span class="math inline">\(p_{\textrm{acc}}\)</span> is given by
<span class="math display">\[
p_{\textrm{acc}} = \min \left\{1, \, \frac{\lambda&#39;^{N + \alpha - 1} (1 - \lambda&#39;)^{\beta - 1}\exp\left(-\lambda&#39; \sum x_i\right)}{\lambda^{N + \alpha - 1} (1 - \lambda)^{\beta - 1}\exp\left(-\lambda \sum x_i\right)}\right\} \\
= \min \left\{1, \, \left(\frac{\lambda&#39;}{\lambda}\right)^{N + \alpha - 1} \left(\frac{1-\lambda&#39;}{1-\lambda}\right)^{\beta -1} \exp\left((\lambda- \lambda&#39;)\sum x\right)\right\}
\]</span>
As we our proposal distribution is symmetric, <span class="math inline">\(q(\lambda \mid \lambda&#39;) = q(\lambda&#39; \mid \lambda)\)</span> and this term cancels.</p>
<p>The code below shows this algorithm in action</p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="markov-chain-monte-carlo.html#cb102-1" tabindex="-1"></a><span class="co"># Set Up MCMC Algorithm ---------------------------------------------------</span></span>
<span id="cb102-2"><a href="markov-chain-monte-carlo.html#cb102-2" tabindex="-1"></a></span>
<span id="cb102-3"><a href="markov-chain-monte-carlo.html#cb102-3" tabindex="-1"></a>n.iter <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb102-4"><a href="markov-chain-monte-carlo.html#cb102-4" tabindex="-1"></a>lambda.store <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n.iter) <span class="co">#Store value of Markov chain at end of every iteration</span></span>
<span id="cb102-5"><a href="markov-chain-monte-carlo.html#cb102-5" tabindex="-1"></a></span>
<span id="cb102-6"><a href="markov-chain-monte-carlo.html#cb102-6" tabindex="-1"></a><span class="co">#Initialise Prior Parameters and Data</span></span>
<span id="cb102-7"><a href="markov-chain-monte-carlo.html#cb102-7" tabindex="-1"></a>lambda <span class="ot">&lt;-</span> <span class="fl">0.5</span></span>
<span id="cb102-8"><a href="markov-chain-monte-carlo.html#cb102-8" tabindex="-1"></a>sum.x <span class="ot">&lt;-</span> <span class="fl">67.6</span></span>
<span id="cb102-9"><a href="markov-chain-monte-carlo.html#cb102-9" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb102-10"><a href="markov-chain-monte-carlo.html#cb102-10" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb102-11"><a href="markov-chain-monte-carlo.html#cb102-11" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb102-12"><a href="markov-chain-monte-carlo.html#cb102-12" tabindex="-1"></a></span>
<span id="cb102-13"><a href="markov-chain-monte-carlo.html#cb102-13" tabindex="-1"></a></span>
<span id="cb102-14"><a href="markov-chain-monte-carlo.html#cb102-14" tabindex="-1"></a><span class="co"># Run MCMC Algorithm ------------------------------------------------------</span></span>
<span id="cb102-15"><a href="markov-chain-monte-carlo.html#cb102-15" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n.iter){</span>
<span id="cb102-16"><a href="markov-chain-monte-carlo.html#cb102-16" tabindex="-1"></a>  </span>
<span id="cb102-17"><a href="markov-chain-monte-carlo.html#cb102-17" tabindex="-1"></a>  <span class="co">#Propose new value of lambda</span></span>
<span id="cb102-18"><a href="markov-chain-monte-carlo.html#cb102-18" tabindex="-1"></a>  lambda.prop <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, lambda, <span class="fl">0.1</span>)</span>
<span id="cb102-19"><a href="markov-chain-monte-carlo.html#cb102-19" tabindex="-1"></a>  </span>
<span id="cb102-20"><a href="markov-chain-monte-carlo.html#cb102-20" tabindex="-1"></a>  <span class="co">#Check lambda \in [0, 1]</span></span>
<span id="cb102-21"><a href="markov-chain-monte-carlo.html#cb102-21" tabindex="-1"></a>  <span class="cf">if</span>(lambda.prop <span class="sc">&gt;</span> <span class="dv">0</span> <span class="sc">&amp;</span> lambda.prop <span class="sc">&lt;</span> <span class="dv">1</span>){</span>
<span id="cb102-22"><a href="markov-chain-monte-carlo.html#cb102-22" tabindex="-1"></a>    </span>
<span id="cb102-23"><a href="markov-chain-monte-carlo.html#cb102-23" tabindex="-1"></a>    <span class="co">#Compute p_acc</span></span>
<span id="cb102-24"><a href="markov-chain-monte-carlo.html#cb102-24" tabindex="-1"></a>    log.p.acc <span class="ot">&lt;-</span> (N <span class="sc">+</span> alpha <span class="sc">-</span> <span class="dv">1</span>)<span class="sc">*</span><span class="fu">log</span>(lambda.prop<span class="sc">/</span>lambda) <span class="sc">+</span> </span>
<span id="cb102-25"><a href="markov-chain-monte-carlo.html#cb102-25" tabindex="-1"></a>      (beta <span class="sc">-</span> <span class="dv">1</span>)<span class="sc">*</span><span class="fu">log</span>((<span class="dv">1</span><span class="sc">-</span>lambda.prop)<span class="sc">/</span>(<span class="dv">1</span><span class="sc">-</span>lambda)) <span class="sc">+</span></span>
<span id="cb102-26"><a href="markov-chain-monte-carlo.html#cb102-26" tabindex="-1"></a>      (lambda <span class="sc">-</span> lambda.prop)<span class="sc">*</span>sum.x</span>
<span id="cb102-27"><a href="markov-chain-monte-carlo.html#cb102-27" tabindex="-1"></a>    </span>
<span id="cb102-28"><a href="markov-chain-monte-carlo.html#cb102-28" tabindex="-1"></a>    <span class="co">#Accept/Reject step</span></span>
<span id="cb102-29"><a href="markov-chain-monte-carlo.html#cb102-29" tabindex="-1"></a>    <span class="cf">if</span>(<span class="fu">log</span>(<span class="fu">runif</span>(<span class="dv">1</span>)) <span class="sc">&lt;</span> log.p.acc)</span>
<span id="cb102-30"><a href="markov-chain-monte-carlo.html#cb102-30" tabindex="-1"></a>      lambda <span class="ot">&lt;-</span> lambda.prop</span>
<span id="cb102-31"><a href="markov-chain-monte-carlo.html#cb102-31" tabindex="-1"></a>    </span>
<span id="cb102-32"><a href="markov-chain-monte-carlo.html#cb102-32" tabindex="-1"></a>  }</span>
<span id="cb102-33"><a href="markov-chain-monte-carlo.html#cb102-33" tabindex="-1"></a>  </span>
<span id="cb102-34"><a href="markov-chain-monte-carlo.html#cb102-34" tabindex="-1"></a>  <span class="co">#Store current value of Markov Chain</span></span>
<span id="cb102-35"><a href="markov-chain-monte-carlo.html#cb102-35" tabindex="-1"></a>  lambda.store[i] <span class="ot">&lt;-</span> lambda</span>
<span id="cb102-36"><a href="markov-chain-monte-carlo.html#cb102-36" tabindex="-1"></a>  </span>
<span id="cb102-37"><a href="markov-chain-monte-carlo.html#cb102-37" tabindex="-1"></a>}</span>
<span id="cb102-38"><a href="markov-chain-monte-carlo.html#cb102-38" tabindex="-1"></a></span>
<span id="cb102-39"><a href="markov-chain-monte-carlo.html#cb102-39" tabindex="-1"></a><span class="co">#Plot trace plot (Markov chain values)</span></span>
<span id="cb102-40"><a href="markov-chain-monte-carlo.html#cb102-40" tabindex="-1"></a><span class="fu">plot</span>(lambda.store, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, <span class="at">xlab =</span> <span class="st">&quot;iteration&quot;</span>, <span class="at">ylab =</span> <span class="fu">expression</span>(lambda))</span>
<span id="cb102-41"><a href="markov-chain-monte-carlo.html#cb102-41" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="fl">0.3</span>, <span class="at">col =</span> <span class="dv">2</span>) <span class="co">#the value I used to simulate the data</span></span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-37-1.png" width="672" /></p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="markov-chain-monte-carlo.html#cb103-1" tabindex="-1"></a><span class="co">#Plot posterior density</span></span>
<span id="cb103-2"><a href="markov-chain-monte-carlo.html#cb103-2" tabindex="-1"></a><span class="fu">hist</span>(lambda.store, <span class="at">prob =</span> <span class="cn">TRUE</span>, <span class="at">xlab =</span> <span class="fu">expression</span>(lambda), <span class="at">main =</span> <span class="st">&quot;Posterior density&quot;</span>)</span>
<span id="cb103-3"><a href="markov-chain-monte-carlo.html#cb103-3" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span><span class="fl">0.3</span>, <span class="at">col =</span> <span class="dv">2</span>) <span class="co">#the value I used to simulate the data</span></span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-37-2.png" width="672" /></p>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb104-1"><a href="markov-chain-monte-carlo.html#cb104-1" tabindex="-1"></a><span class="fu">mean</span>(lambda.store) <span class="co">#posterior mean</span></span></code></pre></div>
<pre><code>## [1] 0.3086762</code></pre>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="markov-chain-monte-carlo.html#cb106-1" tabindex="-1"></a><span class="fu">quantile</span>(lambda.store, <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>)) <span class="co">#95% CI</span></span></code></pre></div>
<pre><code>##      2.5%     97.5% 
## 0.1933055 0.4555422</code></pre>
</div>
<div class="example">
<p><span id="exm:norm" class="example"><strong>Example 5.6  </strong></span>The time until lorry drivers react (in milliseconds) to an obstacle in the road is</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="markov-chain-monte-carlo.html#cb108-1" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.34</span>, <span class="fl">0.47</span>, <span class="fl">0.58</span>, <span class="fl">0.27</span>, <span class="fl">0.74</span>, <span class="fl">0.44</span>, <span class="fl">0.46</span>, <span class="fl">0.65</span>, <span class="fl">0.36</span>, <span class="fl">0.55</span>, <span class="fl">0.58</span>, <span class="fl">0.55</span>, </span>
<span id="cb108-2"><a href="markov-chain-monte-carlo.html#cb108-2" tabindex="-1"></a>       <span class="fl">0.53</span>, <span class="fl">0.56</span>, <span class="fl">0.54</span>, <span class="fl">0.61</span>, <span class="fl">0.43</span>, <span class="fl">0.52</span>, <span class="fl">0.45</span>, <span class="fl">0.49</span>, <span class="fl">0.32</span>, <span class="fl">0.33</span>, <span class="fl">0.47</span>, <span class="fl">0.58</span>, </span>
<span id="cb108-3"><a href="markov-chain-monte-carlo.html#cb108-3" tabindex="-1"></a>       <span class="fl">0.34</span>, <span class="fl">0.60</span>, <span class="fl">0.59</span>, <span class="fl">0.43</span>, <span class="fl">0.57</span>, <span class="fl">0.34</span>)</span>
<span id="cb108-4"><a href="markov-chain-monte-carlo.html#cb108-4" tabindex="-1"></a><span class="fu">hist</span>(y, <span class="at">main =</span> <span class="st">&quot;&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Reaction time (ms)&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-38-1.png" width="672" /></p>
<p>Assuming that for fixed <span class="math inline">\(\sigma^2 &gt; 0\)</span>, <span class="math inline">\(Y_i \mid \mu \sim N(\mu, \sigma^2)\)</span> are independent and identically distributed for <span class="math inline">\(i=1,...,n\)</span>, by Bayes’ theorem, the posterior distribution is</p>
<p><span class="math display">\[
\pi(\mu \mid \boldsymbol{y}, \sigma^2) \propto \pi(\boldsymbol{y} \mid \mu, \sigma^2) \pi(\mu).
\]</span>
One of the issues here is that we have assigned a normal prior distribution to the population mean parameter <span class="math inline">\(\mu\)</span>. The advantage previously was that we could derive a posterior distribution with closed form. The disadvantage however is that the choice of prior distribution assigns some positive probability to impossible values of <span class="math inline">\(\mu\)</span>, i.e. reaction times less than zero.</p>
<p>Now we have a tool to sample from posterior distributions that don’t have a closed form. We can instead assign an exponential prior distribution, a distribution which only has non-negative support. Letting <span class="math inline">\(\mu \sim \textrm{Exp}(10)\)</span> sets a vague prior distribution on <span class="math inline">\(\mu\)</span>. It can be shown that the posterior distribution (exercise) is therefore
<span class="math display">\[
\pi(\mu \mid \boldsymbol{y}, \sigma^2) \propto \exp\left\{-10\mu -\sum_{i=1}^{30}\frac{(y_i - \mu)^2}{\sigma^2}\right\}
\]</span></p>
<p>We can use the Metropolis–Hastings algorithm to sample from this posterior distribution. But how should we propose new value of <span class="math inline">\(\mu\)</span>? A common method is a <em>Metropolis–Hastings Random Walk</em> proposal distribution. The proposal distribution is symmetric and centered on <span class="math inline">\(\mu\)</span>. The two most common methods are <span class="math inline">\(\mu&#39; \mid \mu \sim U[\mu - \varepsilon, \mu + \varepsilon]\)</span> and <span class="math inline">\(\mu&#39; \mid \mu \sim N(\mu, \tau^2)\)</span>. We choose the uniform proposal distribution, with
<span class="math display">\[
q(\mu&#39; \mid \mu) = \frac{1}{2\varepsilon}.
\]</span></p>
<p>The acceptance probability is therefore
<span class="math display">\[
p_\textrm{acc} = \min\left\{\frac{\exp\left\{-10\mu&#39; -\sum_{i=1}^{30}\frac{(y_i - \mu&#39;)^2}{\sigma^2}\right\} }{\exp\left\{-10\mu -\sum_{i=1}^{30}\frac{(y_i - \mu)^2}{\sigma^2}\right\} }, 1\right\}
\]</span></p>
<p>We can implement a sampler in R as follows:</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="markov-chain-monte-carlo.html#cb109-1" tabindex="-1"></a><span class="co">#Set up elements for MCMC</span></span>
<span id="cb109-2"><a href="markov-chain-monte-carlo.html#cb109-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>) <span class="co">#to reproduce</span></span>
<span id="cb109-3"><a href="markov-chain-monte-carlo.html#cb109-3" tabindex="-1"></a>n.iter   <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb109-4"><a href="markov-chain-monte-carlo.html#cb109-4" tabindex="-1"></a>mu.store <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n.iter)</span>
<span id="cb109-5"><a href="markov-chain-monte-carlo.html#cb109-5" tabindex="-1"></a></span>
<span id="cb109-6"><a href="markov-chain-monte-carlo.html#cb109-6" tabindex="-1"></a><span class="co">#Initial values</span></span>
<span id="cb109-7"><a href="markov-chain-monte-carlo.html#cb109-7" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="dv">1</span> </span>
<span id="cb109-8"><a href="markov-chain-monte-carlo.html#cb109-8" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="fl">0.1</span> <span class="co">#known</span></span>
<span id="cb109-9"><a href="markov-chain-monte-carlo.html#cb109-9" tabindex="-1"></a></span>
<span id="cb109-10"><a href="markov-chain-monte-carlo.html#cb109-10" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n.iter){</span>
<span id="cb109-11"><a href="markov-chain-monte-carlo.html#cb109-11" tabindex="-1"></a>  </span>
<span id="cb109-12"><a href="markov-chain-monte-carlo.html#cb109-12" tabindex="-1"></a>  <span class="co">#Propose value for mu</span></span>
<span id="cb109-13"><a href="markov-chain-monte-carlo.html#cb109-13" tabindex="-1"></a>  mu.proposed <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>, mu <span class="sc">-</span> <span class="fl">0.01</span>, mu <span class="sc">+</span> <span class="fl">0.01</span>)</span>
<span id="cb109-14"><a href="markov-chain-monte-carlo.html#cb109-14" tabindex="-1"></a>  </span>
<span id="cb109-15"><a href="markov-chain-monte-carlo.html#cb109-15" tabindex="-1"></a>  <span class="cf">if</span>(mu.proposed <span class="sc">&gt;</span> <span class="dv">0</span>){ <span class="co">#If mu &lt; 0 we can reject straight away</span></span>
<span id="cb109-16"><a href="markov-chain-monte-carlo.html#cb109-16" tabindex="-1"></a>    </span>
<span id="cb109-17"><a href="markov-chain-monte-carlo.html#cb109-17" tabindex="-1"></a>    <span class="co">#Compute (log) acceptance probability</span></span>
<span id="cb109-18"><a href="markov-chain-monte-carlo.html#cb109-18" tabindex="-1"></a>    log.numerator   <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">10</span><span class="sc">*</span>mu.proposed <span class="sc">-</span> </span>
<span id="cb109-19"><a href="markov-chain-monte-carlo.html#cb109-19" tabindex="-1"></a>                      <span class="fu">sum</span>(y <span class="sc">-</span> mu.proposed)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>sigma<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb109-20"><a href="markov-chain-monte-carlo.html#cb109-20" tabindex="-1"></a>    log.denominator <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">10</span><span class="sc">*</span>mu <span class="sc">-</span> <span class="fu">sum</span>(y <span class="sc">-</span> mu)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>sigma<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb109-21"><a href="markov-chain-monte-carlo.html#cb109-21" tabindex="-1"></a>    </span>
<span id="cb109-22"><a href="markov-chain-monte-carlo.html#cb109-22" tabindex="-1"></a>    log.p.acc <span class="ot">&lt;-</span> log.numerator <span class="sc">-</span> log.denominator</span>
<span id="cb109-23"><a href="markov-chain-monte-carlo.html#cb109-23" tabindex="-1"></a>    u <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>)</span>
<span id="cb109-24"><a href="markov-chain-monte-carlo.html#cb109-24" tabindex="-1"></a>    </span>
<span id="cb109-25"><a href="markov-chain-monte-carlo.html#cb109-25" tabindex="-1"></a>    <span class="co">#Accept/Reject step</span></span>
<span id="cb109-26"><a href="markov-chain-monte-carlo.html#cb109-26" tabindex="-1"></a>    <span class="cf">if</span>(<span class="fu">log</span>(u) <span class="sc">&lt;</span> log.p.acc){</span>
<span id="cb109-27"><a href="markov-chain-monte-carlo.html#cb109-27" tabindex="-1"></a>      mu <span class="ot">&lt;-</span> mu.proposed</span>
<span id="cb109-28"><a href="markov-chain-monte-carlo.html#cb109-28" tabindex="-1"></a>    }</span>
<span id="cb109-29"><a href="markov-chain-monte-carlo.html#cb109-29" tabindex="-1"></a>  }</span>
<span id="cb109-30"><a href="markov-chain-monte-carlo.html#cb109-30" tabindex="-1"></a>  </span>
<span id="cb109-31"><a href="markov-chain-monte-carlo.html#cb109-31" tabindex="-1"></a>  <span class="co">#Store mu at each iteration</span></span>
<span id="cb109-32"><a href="markov-chain-monte-carlo.html#cb109-32" tabindex="-1"></a>  mu.store[i] <span class="ot">&lt;-</span> mu</span>
<span id="cb109-33"><a href="markov-chain-monte-carlo.html#cb109-33" tabindex="-1"></a>}</span>
<span id="cb109-34"><a href="markov-chain-monte-carlo.html#cb109-34" tabindex="-1"></a><span class="fu">plot</span>(mu.store, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, <span class="at">xlab =</span> <span class="st">&quot;iteration&quot;</span>, </span>
<span id="cb109-35"><a href="markov-chain-monte-carlo.html#cb109-35" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="fu">expression</span>(mu))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-39-1.png" width="672" /></p>
<p>We can see that after about 300 iterations, the Markov chain has converged to its stationary distribution, the posterior distribution. We can see this more clearly by removing the first 300 iterations.</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="markov-chain-monte-carlo.html#cb110-1" tabindex="-1"></a><span class="fu">plot</span>(mu.store[<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">300</span>)], <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, <span class="at">xlab =</span> <span class="st">&quot;iteration&quot;</span>, <span class="at">ylab =</span> <span class="fu">expression</span>(mu))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-40-1.png" width="672" /></p>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="markov-chain-monte-carlo.html#cb111-1" tabindex="-1"></a><span class="fu">hist</span>(mu.store[<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">300</span>)], <span class="at">xlab =</span> <span class="fu">expression</span>(mu), <span class="at">main =</span> <span class="st">&quot;Posterior distribution&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-40-2.png" width="672" /></p>
<p>The 95% credible interval for <span class="math inline">\(\mu\)</span> using this prior distribution is</p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="markov-chain-monte-carlo.html#cb112-1" tabindex="-1"></a><span class="fu">quantile</span>(mu.store[<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">300</span>)], <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>))</span></code></pre></div>
<pre><code>##      2.5%     97.5% 
## 0.4831480 0.4960669</code></pre>
<p>Using a normal prior distribution, it was</p>
<pre><code>0.486 0.493</code></pre>
<p>It seems that the posterior distribution is very similar when using these two prior distributions. This is because the data are very informative.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-109" class="example"><strong>Example 5.7  </strong></span>In Lab 3.9, we computed the MAP estimate for a parameter from the Pareto distribution. The density function of this distribution is
<span class="math display">\[
\pi(x \mid \beta) = \frac{\beta}{x^{\beta + 1}}, \quad x &gt; 1.
\]</span>
Placing a Gamma prior distribution on <span class="math inline">\(\beta\)</span> such that <span class="math inline">\(\beta \sim \Gamma(a, b)\)</span>. The posterior distribution given the data <span class="math inline">\(\boldsymbol{y} = \{y_1, \ldots, y_N\}\)</span> is
<span class="math display">\[
\pi(\beta \mid \boldsymbol{y}) \propto \frac{\beta^{N + a - 1}e^{-b\beta}}{\prod y_i^{\beta + 1}}.
\]</span>
We can’t sample from this directly, so wee need to use a Metropolis–Hastings algorithm to generate samples from the posterior distribution. We will use a normal proposal distribution.</p>
<p>The acceptance probability is
<span class="math display">\[
p_{acc} = \min \left\{1, \frac{\beta&#39;^{N + a - 1}e^{-b\beta&#39;}}{\prod y_i^{\beta&#39; + 1}}\frac{\prod y_i^{\beta + 1}}{\beta^{N + a - 1}e^{-b\beta}} \right\} \\
= \min \left\{1, \left(\frac{\beta&#39;}{\beta}\right)^{N + a - 1}{\prod y_i^{\beta - \beta&#39;}}\exp((\beta - \beta&#39;)b) \right\}.
\]</span>
The MCMC algorithm will be</p>
<ol style="list-style-type: decimal">
<li>Set an initial value <span class="math inline">\(\beta_0\)</span> and set <span class="math inline">\(i =0\)</span>.</li>
<li>Propose a new value <span class="math inline">\(\beta&#39; \sim N(\beta_i, \sigma^2)\)</span></li>
<li>Accept <span class="math inline">\(\beta&#39;\)</span> with probability <span class="math inline">\(p_{acc}\)</span> and set <span class="math inline">\(\beta_{i+1}= \beta&#39;\)</span>, otherwise reject and set <span class="math inline">\(\beta_{i+1}= \beta_i\)</span>.</li>
<li>Repeat steps 2, 3, and 4 for <span class="math inline">\(i = 1, \ldots, n-1\)</span>.</li>
</ol>
<p>We fix <span class="math inline">\(b=0.01\)</span> and use the data</p>
<pre><code>x &lt;- c(1.019844, 1.043574, 1.360953, 1.049228, 1.491926, 1.192943, 1.323738, 1.262572, 2.034768, 1.451654)</code></pre>
<p>to code up our algorithm.</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="markov-chain-monte-carlo.html#cb116-1" tabindex="-1"></a><span class="co">#Function that evaluates Pareto loglikelihood</span></span>
<span id="cb116-2"><a href="markov-chain-monte-carlo.html#cb116-2" tabindex="-1"></a>log.likelihood <span class="ot">&lt;-</span> <span class="cf">function</span>(x, beta){</span>
<span id="cb116-3"><a href="markov-chain-monte-carlo.html#cb116-3" tabindex="-1"></a></span>
<span id="cb116-4"><a href="markov-chain-monte-carlo.html#cb116-4" tabindex="-1"></a>  log.value <span class="ot">&lt;-</span> <span class="fu">length</span>(x)<span class="sc">*</span><span class="fu">log</span>(beta) <span class="sc">-</span> (beta <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">*</span><span class="fu">sum</span>(<span class="fu">log</span>(x))</span>
<span id="cb116-5"><a href="markov-chain-monte-carlo.html#cb116-5" tabindex="-1"></a></span>
<span id="cb116-6"><a href="markov-chain-monte-carlo.html#cb116-6" tabindex="-1"></a>  <span class="fu">return</span>(log.value)</span>
<span id="cb116-7"><a href="markov-chain-monte-carlo.html#cb116-7" tabindex="-1"></a></span>
<span id="cb116-8"><a href="markov-chain-monte-carlo.html#cb116-8" tabindex="-1"></a>}</span>
<span id="cb116-9"><a href="markov-chain-monte-carlo.html#cb116-9" tabindex="-1"></a></span>
<span id="cb116-10"><a href="markov-chain-monte-carlo.html#cb116-10" tabindex="-1"></a><span class="co"># MCMC Sampler ------------------------------------------------------------</span></span>
<span id="cb116-11"><a href="markov-chain-monte-carlo.html#cb116-11" tabindex="-1"></a></span>
<span id="cb116-12"><a href="markov-chain-monte-carlo.html#cb116-12" tabindex="-1"></a><span class="co">#Initialise Values</span></span>
<span id="cb116-13"><a href="markov-chain-monte-carlo.html#cb116-13" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">1.019844</span>, <span class="fl">1.043574</span>, <span class="fl">1.360953</span>, <span class="fl">1.049228</span>, <span class="fl">1.491926</span>, <span class="fl">1.192943</span>, <span class="fl">1.323738</span>, <span class="fl">1.262572</span>, <span class="fl">2.034768</span>, <span class="fl">1.451654</span>)</span>
<span id="cb116-14"><a href="markov-chain-monte-carlo.html#cb116-14" tabindex="-1"></a>n.iter <span class="ot">&lt;-</span> <span class="dv">10000</span> <span class="co">#number of iterations</span></span>
<span id="cb116-15"><a href="markov-chain-monte-carlo.html#cb116-15" tabindex="-1"></a>beta.current <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="co">#initial value for beta</span></span>
<span id="cb116-16"><a href="markov-chain-monte-carlo.html#cb116-16" tabindex="-1"></a>beta.store <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n.iter) <span class="co">#empty vecotr to store beta at each iteration</span></span>
<span id="cb116-17"><a href="markov-chain-monte-carlo.html#cb116-17" tabindex="-1"></a></span>
<span id="cb116-18"><a href="markov-chain-monte-carlo.html#cb116-18" tabindex="-1"></a><span class="co">#Run MCMC For Loop</span></span>
<span id="cb116-19"><a href="markov-chain-monte-carlo.html#cb116-19" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n.iter){</span>
<span id="cb116-20"><a href="markov-chain-monte-carlo.html#cb116-20" tabindex="-1"></a></span>
<span id="cb116-21"><a href="markov-chain-monte-carlo.html#cb116-21" tabindex="-1"></a>  <span class="co">#Propose prop value for beta</span></span>
<span id="cb116-22"><a href="markov-chain-monte-carlo.html#cb116-22" tabindex="-1"></a>  beta.prop <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, beta.current, <span class="fl">0.5</span>)</span>
<span id="cb116-23"><a href="markov-chain-monte-carlo.html#cb116-23" tabindex="-1"></a></span>
<span id="cb116-24"><a href="markov-chain-monte-carlo.html#cb116-24" tabindex="-1"></a>  <span class="co">#Compute current and prop loglikelihood</span></span>
<span id="cb116-25"><a href="markov-chain-monte-carlo.html#cb116-25" tabindex="-1"></a>  loglike.prop     <span class="ot">&lt;-</span> <span class="fu">log.likelihood</span>(x, beta.prop)</span>
<span id="cb116-26"><a href="markov-chain-monte-carlo.html#cb116-26" tabindex="-1"></a>  loglike.current <span class="ot">&lt;-</span> <span class="fu">log.likelihood</span>(x, beta.current)</span>
<span id="cb116-27"><a href="markov-chain-monte-carlo.html#cb116-27" tabindex="-1"></a></span>
<span id="cb116-28"><a href="markov-chain-monte-carlo.html#cb116-28" tabindex="-1"></a>  <span class="co">#Compute Log acceptance probability</span></span>
<span id="cb116-29"><a href="markov-chain-monte-carlo.html#cb116-29" tabindex="-1"></a>  log.p.acc <span class="ot">&lt;-</span> loglike.prop <span class="sc">-</span> loglike.current <span class="sc">+</span></span>
<span id="cb116-30"><a href="markov-chain-monte-carlo.html#cb116-30" tabindex="-1"></a>    <span class="fu">dgamma</span>(beta.prop, <span class="dv">1</span>, <span class="fl">0.01</span>, <span class="at">log =</span> <span class="cn">TRUE</span>) <span class="sc">-</span> <span class="fu">dgamma</span>(beta.current, <span class="dv">1</span>, <span class="fl">0.01</span>, <span class="at">log =</span> <span class="cn">TRUE</span>)</span>
<span id="cb116-31"><a href="markov-chain-monte-carlo.html#cb116-31" tabindex="-1"></a></span>
<span id="cb116-32"><a href="markov-chain-monte-carlo.html#cb116-32" tabindex="-1"></a>  <span class="co">#Accept/Reject</span></span>
<span id="cb116-33"><a href="markov-chain-monte-carlo.html#cb116-33" tabindex="-1"></a>  u <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>)</span>
<span id="cb116-34"><a href="markov-chain-monte-carlo.html#cb116-34" tabindex="-1"></a>  <span class="cf">if</span>(<span class="fu">log</span>(u) <span class="sc">&lt;</span> log.p.acc){</span>
<span id="cb116-35"><a href="markov-chain-monte-carlo.html#cb116-35" tabindex="-1"></a>    beta.current <span class="ot">&lt;-</span> beta.prop</span>
<span id="cb116-36"><a href="markov-chain-monte-carlo.html#cb116-36" tabindex="-1"></a>  }</span>
<span id="cb116-37"><a href="markov-chain-monte-carlo.html#cb116-37" tabindex="-1"></a></span>
<span id="cb116-38"><a href="markov-chain-monte-carlo.html#cb116-38" tabindex="-1"></a>  <span class="co">#Store Current Value</span></span>
<span id="cb116-39"><a href="markov-chain-monte-carlo.html#cb116-39" tabindex="-1"></a>  beta.store[i] <span class="ot">&lt;-</span> beta.current</span>
<span id="cb116-40"><a href="markov-chain-monte-carlo.html#cb116-40" tabindex="-1"></a></span>
<span id="cb116-41"><a href="markov-chain-monte-carlo.html#cb116-41" tabindex="-1"></a></span>
<span id="cb116-42"><a href="markov-chain-monte-carlo.html#cb116-42" tabindex="-1"></a>}</span>
<span id="cb116-43"><a href="markov-chain-monte-carlo.html#cb116-43" tabindex="-1"></a></span>
<span id="cb116-44"><a href="markov-chain-monte-carlo.html#cb116-44" tabindex="-1"></a><span class="co">#Plot trace plots</span></span>
<span id="cb116-45"><a href="markov-chain-monte-carlo.html#cb116-45" tabindex="-1"></a><span class="fu">plot</span>(beta.store, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-42-1.png" width="672" /></p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="markov-chain-monte-carlo.html#cb117-1" tabindex="-1"></a><span class="co">#Investigate posterior</span></span>
<span id="cb117-2"><a href="markov-chain-monte-carlo.html#cb117-2" tabindex="-1"></a><span class="fu">hist</span>(beta.store, <span class="at">freq =</span> <span class="cn">FALSE</span>, <span class="at">main =</span> <span class="st">&quot;&quot;</span>, <span class="at">xlab =</span> <span class="fu">expression</span>(beta))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-42-2.png" width="672" /></p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="markov-chain-monte-carlo.html#cb118-1" tabindex="-1"></a><span class="fu">quantile</span>(beta.store, <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>))</span></code></pre></div>
<pre><code>##     2.5%    97.5% 
## 2.102268 7.027826</code></pre>
<div id="gibbs-sampler" class="section level2 hasAnchor" number="5.3">
<h2><span class="header-section-number">5.3</span> Gibbs Sampler<a href="markov-chain-monte-carlo.html#gibbs-sampler" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When we can sample directly from full conditional distributions, we can use a Gibbs sampler. Suppose we have a distribution with parameters <span class="math inline">\(\{\theta_1, \ldots, \theta_N\}\)</span>, a Gibbs sampler works as follows:</p>
<ol style="list-style-type: decimal">
<li><p>Set initial values <span class="math inline">\(\{\theta_1^{(0)}, \ldots, \theta_N^{(0)}\}\)</span></p></li>
<li><p>Set <span class="math inline">\(i = 1\)</span>.</p>
<p>Draw a value for <span class="math inline">\(\theta_1^{(i)}\)</span> from <span class="math inline">\(\pi(\theta_1 \mid \theta_2^{(i-1)}, \ldots, \theta_N^{(i-1)})\)</span>.</p>
<p>Draw a value for <span class="math inline">\(\theta_2^{(i)}\)</span> from <span class="math inline">\(\pi(\theta_2 \mid \theta_1^{(i)}, \theta_3^{(i-1)}, \ldots, \theta_N^{(i-1)})\)</span>.</p>
<p><span class="math inline">\(\vdots\)</span></p>
<p>Draw a value for <span class="math inline">\(\theta_N^{(i)}\)</span> from <span class="math inline">\(\pi(\theta_N \mid \theta_1^{(i)}, \theta_2^{(i)}, \ldots, \theta_{N-1}^{(i)})\)</span>.</p></li>
<li><p>Repeat step 2 for <span class="math inline">\(i = 2, \ldots M\)</span>.</p></li>
</ol>
<p>In code, this might look like</p>
<pre><code>M #number of iterations
N #number of parameters
theta.store   &lt;- matrix(NA, N, M)
theta         &lt;- numeric(N)

for(j in 1:M){
  for(j in 1:N){
    theta[i] &lt;- #sample from conditional with theta[-i]
  }
  theta.store[, j] &lt;- theta.current #store current values
}</code></pre>
<div class="example">
<p><span id="exm:unlabeled-div-104" class="example"><strong>Example 5.8  </strong></span>In Example 3.5, we had a hierarchical model with<br />
<span class="math display">\[\begin{align*}
\boldsymbol{y} \mid \lambda &amp;\sim \hbox{Exp}(\lambda) &amp; \textrm{(likelihood)} \\
\lambda \mid \gamma &amp;\sim \hbox{Exp}(\gamma) &amp; \textrm{(prior distribution)} \\
\gamma \mid \nu &amp;\sim \hbox{Exp}(\nu) &amp; \textrm{(hyperprior distribution)}  \\
\end{align*}\]</span>.
To derive the full conditional distributions, we only consider the terms in the posterior distributions that depends on the parameters we are interested in. The full conditional distribution for <span class="math inline">\(\lambda\)</span> is
<span class="math display">\[
\pi(\lambda \mid \boldsymbol{y}, \,\gamma) \propto \lambda^{10}e^{-\lambda(95 + \gamma)}.
\]</span>
This is unchanged and shows that <span class="math inline">\(\lambda \mid \boldsymbol{y}, \gamma \sim \textrm{Gamma}(11, 95 + \gamma)\)</span>. The full conditional distribution for <span class="math inline">\(\gamma\)</span> is
<span class="math display">\[
\pi(\gamma \mid \boldsymbol{y}, \,\lambda) \propto e^{-\nu\gamma}.
\]</span>
Therefore the full conditional distribution of <span class="math inline">\(\gamma\)</span> is <span class="math inline">\(\gamma \mid \boldsymbol{y}, \,\lambda \sim \hbox{Exp}(\lambda + \nu)\)</span>.</p>
<p>We can set up a Metropolis–Hastings algorithm using Gibbs samplers to generate samples for <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\gamma\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p>Set initial values <span class="math inline">\(\{\lambda^{(0)}, \gamma^{(0)}\}\)</span></p></li>
<li><p>Set <span class="math inline">\(i = 1\)</span>.</p></li>
<li><p>Draw a value for <span class="math inline">\(\lambda^{(i)} \mid \boldsymbol{y}, \gamma^{(i-1)} \sim \textrm{Gamma}(10, 95 + \gamma^{(i-1)})\)</span></p></li>
<li><p>Draw a value for <span class="math inline">\(\gamma^{(i)} \mid \boldsymbol{y}, \,\lambda^{(i)} \sim \hbox{Exp}(\lambda^{(i)} + \nu)\)</span>.</p></li>
<li><p>Repeat steps 3 and 4 for <span class="math inline">\(i = 2, \ldots M\)</span>.</p></li>
</ol>
<p>We can now code this up and run the algorithm.</p>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="markov-chain-monte-carlo.html#cb121-1" tabindex="-1"></a><span class="co"># Set Up MCMC Algorithm ---------------------------------------------------</span></span>
<span id="cb121-2"><a href="markov-chain-monte-carlo.html#cb121-2" tabindex="-1"></a></span>
<span id="cb121-3"><a href="markov-chain-monte-carlo.html#cb121-3" tabindex="-1"></a>n.iter <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb121-4"><a href="markov-chain-monte-carlo.html#cb121-4" tabindex="-1"></a>lambda.store <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n.iter) <span class="co">#Store value of Markov chain at end of every iteration</span></span>
<span id="cb121-5"><a href="markov-chain-monte-carlo.html#cb121-5" tabindex="-1"></a>gamma.store <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n.iter) <span class="co">#Store value of Markov chain at end of every iteration</span></span>
<span id="cb121-6"><a href="markov-chain-monte-carlo.html#cb121-6" tabindex="-1"></a></span>
<span id="cb121-7"><a href="markov-chain-monte-carlo.html#cb121-7" tabindex="-1"></a></span>
<span id="cb121-8"><a href="markov-chain-monte-carlo.html#cb121-8" tabindex="-1"></a></span>
<span id="cb121-9"><a href="markov-chain-monte-carlo.html#cb121-9" tabindex="-1"></a><span class="co"># Run MCMC Algorithm ------------------------------------------------------</span></span>
<span id="cb121-10"><a href="markov-chain-monte-carlo.html#cb121-10" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>n.iter){</span>
<span id="cb121-11"><a href="markov-chain-monte-carlo.html#cb121-11" tabindex="-1"></a>  </span>
<span id="cb121-12"><a href="markov-chain-monte-carlo.html#cb121-12" tabindex="-1"></a>  <span class="co">#Store current value of Markov Chain</span></span>
<span id="cb121-13"><a href="markov-chain-monte-carlo.html#cb121-13" tabindex="-1"></a>  lambda.store[i] <span class="ot">&lt;-</span> <span class="fu">rgamma</span>(<span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">95</span> <span class="sc">+</span> gamma.store[i<span class="dv">-1</span>])</span>
<span id="cb121-14"><a href="markov-chain-monte-carlo.html#cb121-14" tabindex="-1"></a>  gamma.store[i]  <span class="ot">&lt;-</span> <span class="fu">rexp</span>(<span class="dv">1</span>, <span class="fl">0.01</span> <span class="sc">+</span> lambda.store[i])</span>
<span id="cb121-15"><a href="markov-chain-monte-carlo.html#cb121-15" tabindex="-1"></a>  </span>
<span id="cb121-16"><a href="markov-chain-monte-carlo.html#cb121-16" tabindex="-1"></a>}</span>
<span id="cb121-17"><a href="markov-chain-monte-carlo.html#cb121-17" tabindex="-1"></a></span>
<span id="cb121-18"><a href="markov-chain-monte-carlo.html#cb121-18" tabindex="-1"></a><span class="co">#Plot trace plot (Markov chain values)</span></span>
<span id="cb121-19"><a href="markov-chain-monte-carlo.html#cb121-19" tabindex="-1"></a><span class="fu">plot</span>(lambda.store, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, <span class="at">xlab =</span> <span class="st">&quot;iteration&quot;</span>, <span class="at">ylab =</span> <span class="fu">expression</span>(lambda))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-43-1.png" width="672" /></p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="markov-chain-monte-carlo.html#cb122-1" tabindex="-1"></a><span class="fu">plot</span>(gamma.store, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, <span class="at">xlab =</span> <span class="st">&quot;iteration&quot;</span>, <span class="at">ylab =</span> <span class="fu">expression</span>(gamma))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-43-2.png" width="672" /></p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="markov-chain-monte-carlo.html#cb123-1" tabindex="-1"></a><span class="co">#Plot posterior density</span></span>
<span id="cb123-2"><a href="markov-chain-monte-carlo.html#cb123-2" tabindex="-1"></a><span class="fu">hist</span>(lambda.store, <span class="at">prob =</span> <span class="cn">TRUE</span>, <span class="at">xlab =</span> <span class="fu">expression</span>(lambda), <span class="at">main =</span> <span class="st">&quot;Posterior density&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-43-3.png" width="672" /></p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="markov-chain-monte-carlo.html#cb124-1" tabindex="-1"></a><span class="fu">mean</span>(lambda.store) <span class="co">#posterior mean</span></span></code></pre></div>
<pre><code>## [1] 0.09561264</code></pre>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb126-1"><a href="markov-chain-monte-carlo.html#cb126-1" tabindex="-1"></a><span class="fu">quantile</span>(lambda.store, <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>)) <span class="co">#95% CI</span></span></code></pre></div>
<pre><code>##      2.5%     97.5% 
## 0.0454177 0.1658069</code></pre>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="markov-chain-monte-carlo.html#cb128-1" tabindex="-1"></a><span class="fu">hist</span>(gamma.store, <span class="at">prob =</span> <span class="cn">TRUE</span>, <span class="at">xlab =</span> <span class="fu">expression</span>(lambda), <span class="at">main =</span> <span class="st">&quot;Posterior density&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-43-4.png" width="672" /></p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="markov-chain-monte-carlo.html#cb129-1" tabindex="-1"></a><span class="fu">mean</span>(gamma.store) <span class="co">#posterior mean</span></span></code></pre></div>
<pre><code>## [1] 10.4679</code></pre>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="markov-chain-monte-carlo.html#cb131-1" tabindex="-1"></a><span class="fu">quantile</span>(gamma.store, <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>)) <span class="co">#95% CI</span></span></code></pre></div>
<pre><code>##       2.5%      97.5% 
##  0.2437085 40.4251992</code></pre>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="markov-chain-monte-carlo.html#cb133-1" tabindex="-1"></a><span class="co">#Investogate correlation between parameters</span></span>
<span id="cb133-2"><a href="markov-chain-monte-carlo.html#cb133-2" tabindex="-1"></a><span class="fu">plot</span>(lambda.store, gamma.store)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-43-5.png" width="672" /></p>
</div>
</div>
<div id="metropolis-within-gibbs" class="section level2 hasAnchor" number="5.4">
<h2><span class="header-section-number">5.4</span> Metropolis-within-Gibbs<a href="markov-chain-monte-carlo.html#metropolis-within-gibbs" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Now we have both the Metropolis–Hastings algorithm and Gibbs sampler, we can combine them to create a generic MCMC algorithm for essentially any posterior distribution with any number of parameters. To create our MCMC algorithm, we update any parameters where the full conditional distribution has closed form with a Gibbs sampler. For parameters where the full conditional distribution does not have a closed form, we use a Metropolis–Hastings algorithm to update the parameters.</p>
<div class="example">
<p><span id="exm:unlabeled-div-105" class="example"><strong>Example 5.9  </strong></span>Suppose <span class="math inline">\(X_1, \ldots, X_N \sim \hbox{Weibull}(\beta, \theta)\)</span>, where
<span class="math display">\[
\pi(x \mid\beta,\theta) = \frac{\beta}{\theta}x^{\beta - 1}\exp\left(-\frac{x^\beta}{\theta}\right), \qquad x, \beta, \theta &gt; 0.
\]</span>
We use an Exponential prior distribution with rate <span class="math inline">\(\lambda\)</span> on <span class="math inline">\(\beta\)</span> and an inverse gamma prior distribution on <span class="math inline">\(\theta\)</span> such that
<span class="math display">\[
\pi(\theta) = \frac{1}{\theta^{a - 1}}\exp\left(-\frac{b}{\theta}\right).
\]</span>
The posterior distribution is therefore
<span class="math display">\[\begin{align*}
\pi(\beta, \theta \mid \boldsymbol{x}) &amp;\propto \pi(\boldsymbol{x} \mid \beta, \theta)\pi(\beta)\pi(\theta) \\
&amp;\propto \frac{\beta^N}{\theta^N}\prod x_i^{\beta - 1}\exp\left(-\frac{1}{\theta}\sum x_i^\beta\right)
\\
&amp;\times\exp(-\lambda\beta) \frac{1}{\theta^{a - 1}}\exp\left(-\frac{b}{\theta}\right)
\end{align*}\]</span></p>
<p>The full conditional distributions are therefore
<span class="math display">\[\begin{align*}
\pi(\beta \mid \theta, \boldsymbol{x}) &amp;\propto\beta^N\prod x_i^{\beta - 1}\exp\left(-\frac{1}{\theta}\sum x_i^\beta\right)\exp(-\lambda\beta) \\
\pi(\theta \mid \beta, \boldsymbol{x}) &amp; \frac{1}{\theta^{N + a -1}}\exp\left(-\frac{1}{\theta}(b + \sum x_i^\beta)\right)
\end{align*}\]</span></p>
<p>There is no closed form for the full conditional distribution for <span class="math inline">\(\beta\)</span>, so we will need to use a Metropolis–Hastings algorithm to update this parameter in our MCMC algorithm. The full conditional distribution for <span class="math inline">\(\theta\)</span> is closed as it is proportional to an inverse Gamma distribution with shape <span class="math inline">\(N + a\)</span> and scale <span class="math inline">\(b + \sum x_i^\beta\)</span>. We can use a Gibbs sampler to update value for <span class="math inline">\(\theta\)</span>. A suitable MCMC algorithm will look like</p>
<ol style="list-style-type: decimal">
<li>Set initial values for <span class="math inline">\(\beta^{(0)}\)</span> and <span class="math inline">\(\theta^{(0)}\)</span> and <span class="math inline">\(i = 1\)</span>.</li>
<li>Propose a new value for <span class="math inline">\(\beta\)</span>, <span class="math inline">\(\beta&#39; \sim U[\beta^{(i-1)} + \varepsilon, \beta^{(i-1)} - \varepsilon]\)</span></li>
<li>Accept <span class="math inline">\(\beta&#39; = \beta^{(i)}\)</span> with probability
<span class="math display">\[
p_{\textrm{acc}} = \min\left\{\frac{\pi(\beta&#39;, \theta^{(i-1)} \mid \boldsymbol{x})}{\pi(\beta, \theta^{(i-1)} \mid \boldsymbol{x})}\frac{q(\beta^{(i-1)} \mid \beta&#39;)}{q(\beta&#39; \mid \beta^{(i-1)})} , 1\right\}
\]</span>
Otherwise reject <span class="math inline">\(\beta&#39;\)</span> and set <span class="math inline">\(\beta^{(i)} = \beta^{(i-1)}\)</span>.</li>
<li>Sample <span class="math inline">\(\theta^{(i)} \sim \hbox{inv}-\Gamma(N + a,\, b + \sum x_i^{\beta^{(i)}})\)</span>.</li>
<li>Repeat steps 2-4.</li>
</ol>
<p>The acceptance probability in step 3 is the ratio of the full conditional distributions for <span class="math inline">\(\beta\)</span>.</p>
</div>
</div>
<div id="mcmc-diagnostics" class="section level2 hasAnchor" number="5.5">
<h2><span class="header-section-number">5.5</span> MCMC Diagnostics<a href="markov-chain-monte-carlo.html#mcmc-diagnostics" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When running an MCMC algorithm, it is always important to check that the Markov chain has converged and is mixing well. For our purposes, mixing well means the chain is exploring the space of possible values of <span class="math inline">\(\theta\)</span> effectively and effectively and not getting stuck on the same value for a long time.</p>
<p>A key way of doing this is by looking at the trace plot, which is a time series of the posterior samples simulated by the algorithm. The trace plot should look like it has converged to the stationary distribution and exploring the stationary distribution efficiently. What it shouldn’t look like is a long series of small steps, or being stuck in one spot for a long time. There are two definitions that help us isolate an efficient Markov chain.</p>
<div class="definition">
<p><span id="def:unlabeled-div-106" class="definition"><strong>Definition 5.11  </strong></span>The <strong>burn-in period</strong> is the number of iterations the Markov chain takes to reach the stationary distribution.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-107" class="definition"><strong>Definition 5.12  </strong></span>The <strong>thinning parameter</strong> is the period of iterations of the Markov chain that are stored.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-108" class="example"><strong>Example 5.10  </strong></span>In Example <a href="markov-chain-monte-carlo.html#exm:norm">5.6</a>, we saw a Markov chain that mixes well. We took the burn-in period to be 3,000 iterations, which was how long it took to for the chain to converge. Although the posterior distribution is invariant to the choice of the proposal distribution, it still has a large effect of the efficiency of the algorithm and how well the chain mixes. The ideal trace plot looks like white noise, or a hairy caterpillar.</p>
<p>In a Metropolis–Hastings random walk algorithm, the proposal distribution often has a large impact on how well the Markov chain mixes. The variance, or step size, of the proposal distribution can be tuned to ensure the chain mixes well.</p>
<p>The following two examples show poorly mixing Markov chains. The first is where the step size is too big and the chain frequently gets stuck for several hundred iterations.</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="markov-chain-monte-carlo.html#cb134-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>) <span class="co">#to reproduce</span></span>
<span id="cb134-2"><a href="markov-chain-monte-carlo.html#cb134-2" tabindex="-1"></a>n.iter   <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb134-3"><a href="markov-chain-monte-carlo.html#cb134-3" tabindex="-1"></a>mu.store <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n.iter)</span>
<span id="cb134-4"><a href="markov-chain-monte-carlo.html#cb134-4" tabindex="-1"></a></span>
<span id="cb134-5"><a href="markov-chain-monte-carlo.html#cb134-5" tabindex="-1"></a><span class="co">#Initial values</span></span>
<span id="cb134-6"><a href="markov-chain-monte-carlo.html#cb134-6" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="dv">1</span> </span>
<span id="cb134-7"><a href="markov-chain-monte-carlo.html#cb134-7" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="fl">0.1</span> <span class="co">#known</span></span>
<span id="cb134-8"><a href="markov-chain-monte-carlo.html#cb134-8" tabindex="-1"></a></span>
<span id="cb134-9"><a href="markov-chain-monte-carlo.html#cb134-9" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n.iter){</span>
<span id="cb134-10"><a href="markov-chain-monte-carlo.html#cb134-10" tabindex="-1"></a>  </span>
<span id="cb134-11"><a href="markov-chain-monte-carlo.html#cb134-11" tabindex="-1"></a>  <span class="co">#Propose value for mu</span></span>
<span id="cb134-12"><a href="markov-chain-monte-carlo.html#cb134-12" tabindex="-1"></a>  mu.proposed <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>, mu <span class="sc">-</span> <span class="fl">0.1</span>, mu <span class="sc">+</span> <span class="fl">0.1</span>) <span class="co">#Step size too big</span></span>
<span id="cb134-13"><a href="markov-chain-monte-carlo.html#cb134-13" tabindex="-1"></a>  </span>
<span id="cb134-14"><a href="markov-chain-monte-carlo.html#cb134-14" tabindex="-1"></a>  <span class="cf">if</span>(mu.proposed <span class="sc">&gt;</span> <span class="dv">0</span>){ <span class="co">#If mu &lt; 0 we can reject straight away</span></span>
<span id="cb134-15"><a href="markov-chain-monte-carlo.html#cb134-15" tabindex="-1"></a>    </span>
<span id="cb134-16"><a href="markov-chain-monte-carlo.html#cb134-16" tabindex="-1"></a>    <span class="co">#Compute (log) acceptance probability</span></span>
<span id="cb134-17"><a href="markov-chain-monte-carlo.html#cb134-17" tabindex="-1"></a>    log.numerator   <span class="ot">&lt;-</span> <span class="sc">-</span><span class="fl">0.01</span><span class="sc">*</span>mu.proposed <span class="sc">-</span> </span>
<span id="cb134-18"><a href="markov-chain-monte-carlo.html#cb134-18" tabindex="-1"></a>                        <span class="fu">sum</span>(y <span class="sc">-</span> mu.proposed)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>sigma<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb134-19"><a href="markov-chain-monte-carlo.html#cb134-19" tabindex="-1"></a>    log.denominator <span class="ot">&lt;-</span> <span class="sc">-</span><span class="fl">0.01</span><span class="sc">*</span>mu <span class="sc">-</span> <span class="fu">sum</span>(y <span class="sc">-</span> mu)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>sigma<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb134-20"><a href="markov-chain-monte-carlo.html#cb134-20" tabindex="-1"></a>    </span>
<span id="cb134-21"><a href="markov-chain-monte-carlo.html#cb134-21" tabindex="-1"></a>    log.p.acc <span class="ot">&lt;-</span> log.numerator <span class="sc">-</span> log.denominator</span>
<span id="cb134-22"><a href="markov-chain-monte-carlo.html#cb134-22" tabindex="-1"></a>    u <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>)</span>
<span id="cb134-23"><a href="markov-chain-monte-carlo.html#cb134-23" tabindex="-1"></a>    </span>
<span id="cb134-24"><a href="markov-chain-monte-carlo.html#cb134-24" tabindex="-1"></a>    <span class="co">#Accept/Reject step</span></span>
<span id="cb134-25"><a href="markov-chain-monte-carlo.html#cb134-25" tabindex="-1"></a>    <span class="cf">if</span>(<span class="fu">log</span>(u) <span class="sc">&lt;</span> log.p.acc){</span>
<span id="cb134-26"><a href="markov-chain-monte-carlo.html#cb134-26" tabindex="-1"></a>      mu <span class="ot">&lt;-</span> mu.proposed</span>
<span id="cb134-27"><a href="markov-chain-monte-carlo.html#cb134-27" tabindex="-1"></a>    }</span>
<span id="cb134-28"><a href="markov-chain-monte-carlo.html#cb134-28" tabindex="-1"></a>  }</span>
<span id="cb134-29"><a href="markov-chain-monte-carlo.html#cb134-29" tabindex="-1"></a>  </span>
<span id="cb134-30"><a href="markov-chain-monte-carlo.html#cb134-30" tabindex="-1"></a>  <span class="co">#Store mu at each iteration</span></span>
<span id="cb134-31"><a href="markov-chain-monte-carlo.html#cb134-31" tabindex="-1"></a>  mu.store[i] <span class="ot">&lt;-</span> mu</span>
<span id="cb134-32"><a href="markov-chain-monte-carlo.html#cb134-32" tabindex="-1"></a>}</span>
<span id="cb134-33"><a href="markov-chain-monte-carlo.html#cb134-33" tabindex="-1"></a><span class="fu">plot</span>(mu.store[<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3000</span>)], <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, <span class="at">xlab =</span> <span class="st">&quot;iteration&quot;</span>, </span>
<span id="cb134-34"><a href="markov-chain-monte-carlo.html#cb134-34" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="fu">expression</span>(mu))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-44-1.png" width="672" /></p>
<p>The next is where the step size is too small. It takes a long time for the chain to converge (~50% of the run time). When the chain does converge, it is inefficient at exploring the space.</p>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb135-1"><a href="markov-chain-monte-carlo.html#cb135-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>) <span class="co">#to reproduce</span></span>
<span id="cb135-2"><a href="markov-chain-monte-carlo.html#cb135-2" tabindex="-1"></a>n.iter   <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb135-3"><a href="markov-chain-monte-carlo.html#cb135-3" tabindex="-1"></a>mu.store <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n.iter)</span>
<span id="cb135-4"><a href="markov-chain-monte-carlo.html#cb135-4" tabindex="-1"></a></span>
<span id="cb135-5"><a href="markov-chain-monte-carlo.html#cb135-5" tabindex="-1"></a><span class="co">#Initial values</span></span>
<span id="cb135-6"><a href="markov-chain-monte-carlo.html#cb135-6" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="dv">1</span> </span>
<span id="cb135-7"><a href="markov-chain-monte-carlo.html#cb135-7" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="fl">0.1</span> <span class="co">#known</span></span>
<span id="cb135-8"><a href="markov-chain-monte-carlo.html#cb135-8" tabindex="-1"></a></span>
<span id="cb135-9"><a href="markov-chain-monte-carlo.html#cb135-9" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n.iter){</span>
<span id="cb135-10"><a href="markov-chain-monte-carlo.html#cb135-10" tabindex="-1"></a>  </span>
<span id="cb135-11"><a href="markov-chain-monte-carlo.html#cb135-11" tabindex="-1"></a>  <span class="co">#Propose value for mu</span></span>
<span id="cb135-12"><a href="markov-chain-monte-carlo.html#cb135-12" tabindex="-1"></a>  mu.proposed <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>, mu <span class="sc">-</span> <span class="fl">0.0005</span>, mu <span class="sc">+</span> <span class="fl">0.0005</span>) <span class="co">#Step size too small</span></span>
<span id="cb135-13"><a href="markov-chain-monte-carlo.html#cb135-13" tabindex="-1"></a>  </span>
<span id="cb135-14"><a href="markov-chain-monte-carlo.html#cb135-14" tabindex="-1"></a>  <span class="cf">if</span>(mu.proposed <span class="sc">&gt;</span> <span class="dv">0</span>){ <span class="co">#If mu &lt; 0 we can reject straight away</span></span>
<span id="cb135-15"><a href="markov-chain-monte-carlo.html#cb135-15" tabindex="-1"></a>    </span>
<span id="cb135-16"><a href="markov-chain-monte-carlo.html#cb135-16" tabindex="-1"></a>    <span class="co">#Compute (log) acceptance probability</span></span>
<span id="cb135-17"><a href="markov-chain-monte-carlo.html#cb135-17" tabindex="-1"></a>    log.numerator   <span class="ot">&lt;-</span> <span class="sc">-</span><span class="fl">0.01</span><span class="sc">*</span>mu.proposed <span class="sc">-</span></span>
<span id="cb135-18"><a href="markov-chain-monte-carlo.html#cb135-18" tabindex="-1"></a>                        <span class="fu">sum</span>(y <span class="sc">-</span> mu.proposed)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>sigma<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb135-19"><a href="markov-chain-monte-carlo.html#cb135-19" tabindex="-1"></a>    log.denominator <span class="ot">&lt;-</span> <span class="sc">-</span><span class="fl">0.01</span><span class="sc">*</span>mu <span class="sc">-</span> <span class="fu">sum</span>(y <span class="sc">-</span> mu)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>sigma<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb135-20"><a href="markov-chain-monte-carlo.html#cb135-20" tabindex="-1"></a>    </span>
<span id="cb135-21"><a href="markov-chain-monte-carlo.html#cb135-21" tabindex="-1"></a>    log.p.acc <span class="ot">&lt;-</span> log.numerator <span class="sc">-</span> log.denominator</span>
<span id="cb135-22"><a href="markov-chain-monte-carlo.html#cb135-22" tabindex="-1"></a>    u <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>)</span>
<span id="cb135-23"><a href="markov-chain-monte-carlo.html#cb135-23" tabindex="-1"></a>    </span>
<span id="cb135-24"><a href="markov-chain-monte-carlo.html#cb135-24" tabindex="-1"></a>    <span class="co">#Accept/Reject step</span></span>
<span id="cb135-25"><a href="markov-chain-monte-carlo.html#cb135-25" tabindex="-1"></a>    <span class="cf">if</span>(<span class="fu">log</span>(u) <span class="sc">&lt;</span> log.p.acc){</span>
<span id="cb135-26"><a href="markov-chain-monte-carlo.html#cb135-26" tabindex="-1"></a>      mu <span class="ot">&lt;-</span> mu.proposed</span>
<span id="cb135-27"><a href="markov-chain-monte-carlo.html#cb135-27" tabindex="-1"></a>    }</span>
<span id="cb135-28"><a href="markov-chain-monte-carlo.html#cb135-28" tabindex="-1"></a>  }</span>
<span id="cb135-29"><a href="markov-chain-monte-carlo.html#cb135-29" tabindex="-1"></a>  </span>
<span id="cb135-30"><a href="markov-chain-monte-carlo.html#cb135-30" tabindex="-1"></a>  <span class="co">#Store mu at each iteration</span></span>
<span id="cb135-31"><a href="markov-chain-monte-carlo.html#cb135-31" tabindex="-1"></a>  mu.store[i] <span class="ot">&lt;-</span> mu</span>
<span id="cb135-32"><a href="markov-chain-monte-carlo.html#cb135-32" tabindex="-1"></a>}</span>
<span id="cb135-33"><a href="markov-chain-monte-carlo.html#cb135-33" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb135-34"><a href="markov-chain-monte-carlo.html#cb135-34" tabindex="-1"></a><span class="fu">plot</span>(mu.store, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, <span class="at">xlab =</span> <span class="st">&quot;iteration&quot;</span>, <span class="at">ylab =</span> <span class="fu">expression</span>(mu))</span>
<span id="cb135-35"><a href="markov-chain-monte-carlo.html#cb135-35" tabindex="-1"></a><span class="fu">plot</span>(mu.store[<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">5000</span>)], <span class="at">type =</span> <span class="st">&#39;l&#39;</span>, <span class="at">xlab =</span> <span class="st">&quot;iteration&quot;</span>,</span>
<span id="cb135-36"><a href="markov-chain-monte-carlo.html#cb135-36" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="fu">expression</span>(mu))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-45-1.png" width="672" /></p>
</div>
<p>The final diagnostic issue we are going to think about is the <strong>curse of dimensionality</strong>. In general, the more parameters we try and update, the less likely we are to accept them. This makes exploring the proposal distribution hard if we are trying to update lots of parameters simultaneously. We can see this if we consider a hypersphere
:::{.example}
Suppose we have a density function that is uniformly distributed over the area of a sphere in <span class="math inline">\(N\)</span> dimensions with radius <span class="math inline">\(r\)</span>. The sphere has volume
<span class="math display">\[
V = \frac{\pi^{N/2}}{\Gamma(\frac{N}{2} + 1)}r^N.
\]</span>
Now consider a smaller sphere inside of our original sphere. This still has dimension <span class="math inline">\(N\)</span> but has radius <span class="math inline">\(r_1 &lt; r\)</span>. This small sphere has volume
<span class="math display">\[
V_1 = \frac{\pi^{N/2}}{\Gamma(\frac{N}{2} + 1)}r_1^N.
\]</span>
The difference between these two volumes is
<span class="math display">\[
V - V_1 = \frac{\pi^{N/2}}{\Gamma(\frac{N}{2} + 1)}(r^N - r_1^N).
\]</span>
For large <span class="math inline">\(N\)</span>, even when <span class="math inline">\(r - r_1\)</span> is small, <span class="math inline">\((r^N - r_1^N)\)</span> is large. This means that lots of the probability mass is concentrated away from the mode into the outer shell of the sphere.</p>
</div>
</div>
<p>The hypersphere example shows that in large dimensions we need our Markov chain to spend lots of time away from the posterior mode and in the tails of the distribution, but this is where the proposal distribution has lowest mass. We can avoid this by updating each parameter individually, but this means we need to use the full conditional distributions, which have highest mass near the mode. This ‘curse’ makes MCMC algorithms inefficient for large dimensions.</p>
</div>
<div id="beyond-mcmc" class="section level2 hasAnchor" number="5.6">
<h2><span class="header-section-number">5.6</span> Beyond MCMC<a href="markov-chain-monte-carlo.html#beyond-mcmc" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>MCMC is not the only method available to generate samples from the posterior distribution. MCMC is often slow and inefficient. Much of the work in computational statistics research is about developing fast and efficient methods for sampling from posterior distributions. We are going to look at another method, called approximate Bayesian computation, in the next chapter. Two other methods, beyound the scope of this module, are Sequential Monte Carlo and Hamiltonian Monte Carlo.</p>
<p>Sequential Monte Carlo (SMC) methods for Bayesian inference aim to estimate the posterior distribution of a state space model recursively based on the observed data. Initially, a set of particles representing possible states is sampled from the prior distribution. Then particles are propagated forward using the system dynamics and updated according to their likelihood given the observed data. This update step involves reweighting particles based on how well they explain the observed data. To ensure that the particle set accurately represents the posterior distribution, resampling is performed, where particles with higher weights are more likely to be retained. By iteratively repeating these steps, SMC effectively tracks the evolution of the posterior distribution over time, providing a flexible and computationally efficient framework for Bayesian inference in dynamic systems.</p>
<p>Hamiltonian Monte Carlo (HMC) is a sophisticated Markov chain Monte Carlo (MCMC) method for sampling from complex, high-dimensional target distributions, commonly used in Bayesian inference. Unlike traditional MCMC methods, which rely on random walk proposals, HMC employs Hamiltonian dynamics to guide the exploration of the state space. By introducing auxiliary momentum variables, HMC constructs a joint distribution over the original target variables and the momentum variables. This augmented space enables the use of Hamiltonian dynamics, which can efficiently explore the target distribution by simulating the evolution of the system’s energy function. The key idea is to use the gradient of the target distribution’s log-probability to determine the momentum dynamics, leading to more effective proposals that can traverse the state space more efficiently. HMC samples are obtained by simulating Hamiltonian dynamics over a trajectory and then accepting or rejecting the proposed states based on Metropolis–Hastings criteria. Overall, HMC offers significant improvements in exploration efficiency compared to traditional MCMC methods, particularly in high-dimensional spaces, making it a powerful tool for Bayesian inference.</p>
</div>
<div id="lab-2" class="section level2 hasAnchor" number="5.7">
<h2><span class="header-section-number">5.7</span> Lab<a href="markov-chain-monte-carlo.html#lab-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="exercise">
<p><span id="exr:unlabeled-div-110" class="exercise"><strong>Exercise 5.1  </strong></span>Consider the Langevin Markov chain with potential <span class="math inline">\(V(x) = \exp(-\lvert x \rvert^3)\)</span> in dimension <span class="math inline">\(d=1\)</span>. Simulate <span class="math inline">\(10,000\)</span> iterates of the Markov chain for different step sizes <span class="math inline">\(h\)</span> and compare the histogram to a plot of the target density <span class="math inline">\(\pi(x) \propto \exp(-V(x))\)</span>. Also experiment with different choices for <span class="math inline">\(V\)</span>.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-111" class="exercise"><strong>Exercise 5.2  </strong></span>You observe the following draws from a Binomial distribution with 25 trials and probability of success <span class="math inline">\(p\)</span>.</p>
<pre><code>y &lt;- c(20, 16, 20, 17, 18, 19, 19, 18, 21, 20, 19, 22, 23, 19, 20, 19, 21, 20, 25, 15)</code></pre>
<p>Use a normal prior distribution with mean 0.5 and variance <span class="math inline">\(0.1^2\)</span>. Write a Metropolis–Hastings Random Walk algorithm to obtain samples from the posterior distribution (you can use R’s built in function for the likelihood function and prior distribution, but if you don’t take logs you will run into small number errors).</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-112" class="exercise"><strong>Exercise 5.3  </strong></span>In a medical trial, to investigate the proportion <span class="math inline">\(p\)</span> of the population who have a particular disease a random sample of 20 individuals is taken. Ten of these are subject to a diagnostic test (Test A) which detects the disease when it is present with 100% certainty. The remaining 10 are given a test (test B) which only detects the disease with probability 0.8 when it is present. Neither test can give a false positive result. before collecting the data your prior belief about <span class="math inline">\(p\)</span> is represented by a U(0,1) distribution. Suppose that, for Test A, 5 out of 10 test positive while for test B, 3 out of 10 test positive. Use an MCMC algorithm to investigate the posterior density of <span class="math inline">\(p\)</span> and estimate its posterior mean and variance.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-113" class="exercise"><strong>Exercise 5.4  </strong></span>Code up an MCMC algorithm for Example 3.5 using Gibbs samplers.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-114" class="exercise"><strong>Exercise 5.5  </strong></span>The density function for the inverse-gamma distribution is
<span class="math display">\[
\pi(x\mid \alpha, \beta) = \frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha - 1}e^{-\beta/x}
\]</span>
Using independent Gamma prior distributions on the model parameters, <span class="math inline">\(\alpha \sim \Gamma(a, b)\)</span> and <span class="math inline">\(\beta \sim Gamma(c, d)\)</span>, write down the posterior distribution for the model parameters. Only one will have a closed form.</p>
<p>Develop a code an MCMC algorithm to sample from the posterior distribution by alternating between sampling <span class="math inline">\(\alpha\)</span> and then <span class="math inline">\(\beta\)</span>.</p>
<p>Simulate some data from the inverse-gamma distribution and see if you can recover the parameters used to simulate the data. Is there any correlation between the samples for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>.</p>
</div>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-hairer11" class="csl-entry">
Hairer, Martin, and Jonathan C. Mattingly. 2011. <span>“Yet Another Look at <span>H</span>arris’ Ergodic Theorem for <span>M</span>arkov Chains.”</span> In <em>Seminar on <span>S</span>tochastic <span>A</span>nalysis, <span>R</span>andom <span>F</span>ields and <span>A</span>pplications <span>VI</span></em>, 63:109–17. Progr. Probab. Birkhäuser/Springer Basel AG, Basel. <a href="https://doi.org/10.1007/978-3-0348-0021-1_7">https://doi.org/10.1007/978-3-0348-0021-1_7</a>.
</div>
<div id="ref-meyntweedie" class="csl-entry">
Meyn, Sean, and Richard L. Tweedie. 2009. <em>Markov Chains and Stochastic Stability</em>. Second. Cambridge University Press, Cambridge. <a href="https://doi.org/10.1017/CBO9780511626630">https://doi.org/10.1017/CBO9780511626630</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sampling.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="advanced-computation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
