[["index.html", "Bayesian Inference and Computation Practicalities 0.1 Module Aims 0.2 Module Structure 0.3 Assessment 0.4 Recommended Books and Videos 0.5 Common Distributions", " Bayesian Inference and Computation Dr Mengchu Li and Dr Lukas Trottner (based on lecture notes by Dr Rowland Seymour) Semester 2, 2025 Practicalities 0.1 Module Aims Bayesian inference is a set of methods where the probability of an event occurring can be updated as more information becomes available. It is fundamentally different from frequentist methods, which are based on long running relative frequencies. This module gives an introduction to the Bayesian approach to statistical analysis and the theory that underpins it. Students will be able to explain the distinctive features of Bayesian methodology, understand and appreciate the role of prior distributions and compute posterior distributions. It will cover the derivation of posterior distributions, the construction of prior distributions, and inference for missing data. Extensions are considered to models with more than a single parameter and how these can be used to analyse data. Computational methods have greatly advanced the use of Bayesian methods and this module covers, and allows students to apply, procedures for the sampling and analysis of intractable Bayesian problems. By the end of the course, students should be able to: Demonstrate a full and rigorous understanding of all definitions associated with Bayesian inference and understand the differences between the Bayesian and frequentist approaches to inference Demonstrate a sound understanding of the fundamental concepts of Bayesian inference and computational sampling methods Understand how to make inferences assuming various population distributions while taking into account expert opinion and the implications of weak prior knowledge and large samples Demonstrate an understanding of the principles of Markov Chain Monte Carlo and be able to programme an MCMC algorithm Engage in Bayesian data analysis in diverse situations drawn from physics, biological, engineering and other mathematical contexts. 0.2 Module Structure The module is split between theory and computation. Each week will have three lectures, one computer lab and one guided study. In the labs, you will need to bring your own laptop. 0.3 Assessment Assessment for this module is 50% via an exam and 50% via coursework assignments during the semester. The exam will last 1h 30m and take place during the summer exam period. There will be three coursework assignment – assignment 1 will be worth 10% of the final mark, with assignments 2 and 3 counting for 20% each. More details about the assignments will be made available during the semester. 0.4 Recommended Books and Videos No books are required for this course and the whole material is contained in these notes. However, you may find it useful to use other resources in your studies. I recommend the following: A First Course in Bayesian Statistical Methods - Peter D. Hoff. This is a short book that covers the basics of Bayesian inference and computation. To the point and well written, it’s a useful place to look topics up. Bayesian Data Analysis - Andrew Gelman, John Carlin, Hal Stern, David Dunson, Aki Vehtari, and Donald Rubin. This is a thorough book explaining everything you’d need to know to carry out Bayesian data analysis. It’s a fairly long and in-depth book, but the authors are authoritative and give good advice throughout. Example code on the website is in R, Python and Stan. Statistical Rethinking - Richard McElrath. This book provides a friendly intuitive understanding of Bayesian inference and computation. Aimed at social and natural scientists, it has less theory that the other two books but is perhaps more approachable. A set of video lectures for this book can be found on YouTube. 0.5 Common Distributions For many Bayesian inference problems, it is useful to be able to identify probability density functions (for continuous random variables) and probability mass functions (for discrete random variables) up to proportionality. Some common density/mass functions are given below. Normal distribution \\(N(\\mu,\\sigma^2)\\) \\[ \\pi(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\right\\} \\qquad x \\in\\mathbb{R}, \\] where \\(\\mu \\in \\mathbb{R}\\) and \\(\\sigma &gt; 0\\). Beta distribution \\(\\text{Beta}(\\alpha,\\beta)\\) \\[ \\pi(x\\mid \\alpha, \\beta) = \\frac{1}{B(\\alpha, \\beta)}x^{\\alpha-1}(1-x)^{\\beta - 1} \\qquad x \\in (0, 1), \\] where \\(\\alpha, \\beta &gt; 0\\) and \\(B(\\alpha, \\beta)\\) is the Beta function. Gamma distribution \\(\\text{Gamma}(\\alpha,\\beta)\\) \\[ \\pi(x\\mid \\alpha, \\beta) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha - 1}e^{-\\beta x} \\qquad x &gt; 0, \\] where \\(\\alpha, \\beta &gt; 0\\) and \\(\\Gamma(\\alpha)\\) is the Gamma function. Exponential distribution \\(\\text{Exp}(\\lambda)\\) \\[ \\pi(x \\mid \\lambda) = \\lambda e^{-\\lambda x} \\qquad x &gt; 0, \\] where \\(\\lambda &gt; 0\\). Poisson distribution \\(\\text{Pois}(\\lambda)\\) \\[ \\pi(x = k \\mid \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!} \\qquad k \\in \\{0, 1, 2, \\ldots\\}, \\] where \\(\\lambda &gt; 0\\). Binomial distribution \\(\\text{Bin}(N,p)\\) \\[ \\pi(x = k \\mid N, p) = \\begin{pmatrix} N \\\\ k\\end{pmatrix} p^k (1-p)^{N-k} \\qquad k \\in \\{1, \\ldots, N\\} \\] where \\(p \\in [0, 1]\\). "],["fundamentals.html", "Chapter 1 Fundamentals Concepts 1.1 Statistical Inference 1.2 Frequentist Theory 1.3 Bayesian Paradigm 1.4 Bayes’ Theorem", " Chapter 1 Fundamentals Concepts Bayesian inference is built on a different way of thinking about parameters of probability distributions than methods you have learnt so far. In the past 30 years or so, Bayesian inference has become much more popular. This is partly due to increased computational power becoming available. In this first chapter, we are going to set out to answer: What are the fundamental principles of Bayesian inference? What makes Bayesian inference different from other methods? 1.1 Statistical Inference The purpose of statistical inference is to “draw conclusions, from numerical data, about quantities that are not observed” (Bayesian Data Analysis, chapter 1). Generally speaking, there are two kinds of inference: Inference for quantities that are unobserved or haven’t happened yet. Examples of this might be the size of a payout an insurance company has to make, or a patients outcome in a clinical trial had they been received a certain treatment. Inference for quantities that are not possible to observe. This is usual because they are part of modelling process, like parameters in a linear model. 1.2 Frequentist Theory Frequentist interpretation of probability is built upon the theory on long run events. Probabilities must be interpretable as frequencies over multiple repetitions of the experiment that is being analysed, and are calculated from the sampling distributions of measured quantities. Definition 1.1 In the frequentist world, the long run relative frequency of an event is the probability of that event. Example 1.1 If a frequentist wanted to assign a probability to rolling a 6 on a particular dice, then they would roll the dice a large number of times and compute the relative frequency. Typical frequentist analysis starts with a statistical model \\(\\{P_\\theta: \\theta \\in \\Theta\\}\\), where \\(\\Theta\\) denotes some parameter space. E.g. \\(\\{N(\\theta,1) : \\theta \\in \\Theta = \\mathbb{R}\\}\\). The data are assumed to be generated from some statistical model, and we often consider the case that they are independent and identically distributed (i.i.d), i.e. \\[ Y_1,\\dotsc,Y_n \\overset{i.i.d}{\\sim} P_{\\theta}, \\; \\theta \\in \\Theta. \\] Without going much into the measure-theoretic formulation, we write \\[ Y_1,\\dotsc,Y_n \\overset{i.i.d}{\\sim} \\pi(y\\mid {\\theta}),\\; \\theta \\in \\Theta, \\] where \\(\\pi(x\\mid {\\theta})\\) is the probability density or mass function depending on whether \\(P_{\\theta}\\) is continuous or discrete. We shall simply refer to it as the density function of \\(P_{\\theta}\\). The actual observed data \\(\\boldsymbol{y} = (y_1,\\dotsc,y_n)\\) are considered to be realisations of the random variable \\(Y = (Y_1,\\dotsc,Y_n)\\). The fundamental difference between frequentist and Bayesian statistical analysis is that \\(\\theta\\) is viewed a deterministic (non-random) quantity by a frequentist but a Bayesian statistician would view it as a random quantity. To estimate \\(\\theta\\) in the frequentist paradigm, one would consider some estimator \\(\\hat{\\theta}(Y)\\), and analyse its finite-sample distribution or asymptotic distribution. The most common way to estimate the value of \\(\\theta\\) is using maximum likelihood estimation (MLE). Although other methods do exist (e.g. method of moments). Recall that the likelihood function \\(\\pi(\\boldsymbol{y} \\mid \\theta)\\) is simply the joint density of \\(Y_1,\\dotsc,Y_n\\). Under the i.i.d assumption, it is \\[ \\pi(\\boldsymbol{y} \\mid \\theta) = \\prod_{i=1}^n \\pi(y_i\\mid \\theta). \\] Definition 1.2 The maximum likelihood estimator of \\(\\theta\\), \\(\\hat{\\theta}\\), is the value such that \\(\\hat{\\theta} = \\mathop{\\mathrm{arg\\,max}}_{\\theta} \\pi(Y\\mid \\theta)\\). The key to quantify uncertainty in the frequentist paradim is using confidence intervals. Definition 1.3 An interval \\([L_n(Y), U_n(Y)] \\subseteq \\mathbb{R}\\), is called a confidence interval (CI) at level \\(1 - \\alpha \\in (0,1)\\) for \\(\\theta\\), if \\[ \\pi\\big(L_n(Y) \\leq \\theta \\leq U_n(Y)\\big) = 1 - \\alpha. \\] It is important to stress that the confidence interval (CI) \\([L_n(Y), U_n(Y)]\\) is a random quantity and the parameter \\(\\theta\\) is fixed (non-random) in the frequentist paradigm! The correct interpretation of CIs is that if we construct many confidence intervals from repeated random samples, \\(100(1-\\alpha)\\%\\) of these intervals would contain the true parameter \\(\\theta\\). It does not mean that a particular interval contains the true value of \\(\\theta\\) with probability \\(1-\\alpha\\). Note that the CI interpretation is based on the previous definition of the probability of an event and this is why CIs based inference are considered to be frequentist. Note that, in many cases, one can only hope to obtain intervals that contains \\(\\theta\\) at level \\(1-\\alpha\\) when \\(n\\) is sufficiently large. For example, asymptotic properties of MLE \\[ \\sqrt{n} (\\hat{\\theta} - \\theta) \\overset{d}{\\longrightarrow} N(0, I(\\theta)^{-1}), \\] allows one to use \\[ \\hat{\\theta} \\pm \\Phi^{-1}{(1-\\alpha / 2)} \\sqrt{\\frac{1}{n I(\\hat{\\theta})}}, \\] as an approxiamte CI at level \\(1-\\alpha\\), under mild conditions, where \\(\\Phi(x)\\) is the cumulative distribution function of \\(N(0,1)\\) and \\[ I(\\theta) = \\mathrm{Var} \\left[ \\frac{\\partial}{\\partial \\theta} \\log \\pi(Y\\mid \\theta) \\right] = -\\mathbb{E} \\left[ \\frac{\\partial^2}{\\partial \\theta^2} \\log \\pi(Y \\mid \\theta) \\right], \\qquad Y \\sim \\pi(y\\mid\\theta) \\] is the Fisher information. 1.3 Bayesian Paradigm Given that we want to understand the properties of \\(\\theta\\) given the data we have observed \\(\\boldsymbol{y}\\), then you might think it makes sense to investigate the distribution \\(\\pi(\\theta \\mid \\boldsymbol{y})\\). This distribution says what are the likely values of \\(\\theta\\) given the information we have observed from the data \\(\\boldsymbol{y}\\). We will talk about Bayes’ theorem in more detail later on in this chapter, but, for now, we will use it to write down this distribution \\[ \\pi(\\theta \\mid \\boldsymbol{y}) = \\frac{\\pi(\\boldsymbol{y} \\mid \\theta)\\pi(\\theta)}{\\pi(\\boldsymbol{y})}. \\] This is where frequentist theory cannot help us, particularly the term \\(\\pi(\\theta)\\). Randomness can only come from the data, so how can we assign a probability distribution to a constant \\(\\theta\\)? The term \\(\\pi(\\theta)\\) is meaningless under this philosophy. Instead, we turn to a different philosophy where we can assign a probability distribution to \\(\\theta\\). The Bayesian paradigm is built around a different interpretation of probability. This allows us to generate probability distributions for parameters values. Definition 1.4 In the Bayesian world, the subjective belief of an event is the probability of that event. This definition means we can assign probabilities to events that frequentists do not recognise as valid. Example 1.2 Consider the following events: Lukas’s height is above 192cm. Mengchu’s weight is above 74kg. Man United will lose against Fulham on 26 Jan 2025. There is more than 90% chance that a particular experiment is going to fail Probabilities can be assigned to any events in the Bayesian paradigm, but they are necessarily subjective. The key in Bayesian inference is to understand how does subject beliefs change when some data/evidence become available. This is essentially captured in the Bayes’ theorem. Before we discuss Bayes’ theorem, we recap some basic facts in probability. Definition 1.5 Given two events \\(A\\) and \\(B\\), the conditional probability that event \\(A\\) occurs given the event \\(B\\) has already occurred is \\[ \\pi(A \\mid B) = \\frac{\\pi(A \\cap B)}{\\pi(B)}, \\] when \\(\\pi(B) &gt; 0\\). Definition 1.6 Two events \\(A\\) and \\(B\\) are conditionally independent given event \\(C\\) if and only if \\[ \\pi(A \\cap B \\mid C) = \\pi(A \\mid C)\\pi(B \\mid C).\\] Definition 1.7 For two random variables \\(X,Y\\) that have a joint density function \\(\\pi(x,y)\\). The marginal density function for \\(X\\) is \\(\\pi(x) = \\int \\pi(x,y)dy\\). The conditional density function of \\(Y\\mid X\\) is \\(\\pi(y\\mid x) = \\pi(x,y)/\\pi(x)\\), although it is actually the density function of \\(Y\\mid X = x\\). If \\(X\\) and \\(Y\\) are independent \\(\\pi(y \\mid x) = \\pi(y)\\). We can factorise a joint density function in different ways since \\(\\pi(x,y) = \\pi(x)\\pi(y\\mid x) = \\pi(x\\mid y)\\pi(y)\\). Combining 1, 2 and 3, we have the partition theorem, also known as law of total probability \\(\\pi(x) = \\int\\pi(x\\mid y)\\pi(y) dy\\). An extension to 4 is that suppose \\(Z\\) is another random variable, then we have \\[ \\pi(y \\mid x) = \\int \\pi(y \\mid x, z) \\, \\pi(z\\mid x)dz. \\] If \\(Y\\) and \\(X\\) are conditionally independent given \\(Z\\) then \\(\\pi(y \\mid x, z) = \\pi(y \\mid z)\\). For a sequence of random variables \\(X_1,\\dotsc,X_n\\), we can factorise the joint density function in many ways, e.g.  \\[ \\pi(x_1,\\dotsc,x_n) = \\pi(x_1\\mid x_2,\\dotsc,x_n)\\pi(x_2,x_3\\mid x_4,\\dotsc,x_n)\\pi(x_4,\\dotsc,x_n). \\] In practice, we may wish to factorise the joint density in a way that exploits the conditionally independence structure between variables. We say \\(X_1,\\dotsc,X_n\\) are conditionally independent and identically distributed given some random variable \\(Y\\) if \\[ \\pi(x_1,\\dotsc,x_n\\mid y) = \\prod_{i=1}^n \\pi(x_i\\mid y). \\] Definition 1.8 Let \\(\\pi(y_1, \\ldots, y_N)\\) be the joint density of \\(Y_1, \\ldots, Y_N\\). If \\(\\pi(y_1, \\ldots, y_N) = \\pi(y_{\\sigma_1}, \\ldots, y_{\\sigma_N})\\) for any permutations \\(\\sigma\\) of \\(\\{1, \\ldots, N\\}\\), then \\(Y_1, \\ldots, Y_N\\) are exchangeable. Exchangeability means that the labels of the random variables don’t contain any information about the outcomes. This is an important idea in many areas of probability and statistics, and it is a weaker assumption compared to saying \\(Y_1,\\dotsc,Y_N\\) are independent. Example 1.3 If \\(Y_i \\sim \\text{Bin}(n, p)\\) are independent and identically distributed for \\(i = 1, 2, 3\\), then \\(\\pi(y_1, y_2, y_3) = \\pi(y_3, y_1, y_2)\\). Therefore, independence implies exchangeability. Example 1.4 Let \\((X, Y)\\) follow a bivariate normal distribution with mean 0, variances \\(\\sigma_x^2 = \\sigma_y^2 = 1\\) and a correlation parameter \\(\\rho \\in [-1, 1]\\). \\((X, Y)\\) are exchangeable, but only independent if \\(\\rho = 0\\). Proposition 1.1 If \\(\\theta \\sim \\pi(\\theta)\\) and \\((Y_1, \\ldots, Y_N)\\) are conditionally iid given the random variable \\(\\theta\\), then marginally \\(Y_1, \\ldots, Y_N\\) are exchangeable. Proof. Suppose \\((Y_1, \\ldots, Y_N)\\) are conditionally iid given some parameter \\(\\theta\\). Then for any permutation \\(\\sigma\\) of \\(\\{1, \\ldots, N\\}\\) and observations \\(\\{y_1, \\ldots, y_N\\}\\) \\[\\begin{equation} \\begin{split} \\pi(y_1, \\ldots, y_N) &amp;= \\int \\pi(y_1, \\ldots, y_N \\mid \\theta) \\pi(\\theta)\\, d\\theta \\qquad \\\\ &amp; = \\int \\left\\{\\prod_{i=1}^N\\pi(y_i \\mid \\theta)\\right\\} \\pi(\\theta)\\, d\\theta \\qquad \\textrm{(conditionally iid)}\\\\ &amp; = \\int \\left\\{\\prod_{i=1}^N\\pi(y_{\\sigma_i} \\mid \\theta)\\right\\} \\pi(\\theta)\\, d\\theta \\qquad \\textrm{(product is commutative)} \\\\ &amp; = \\pi(y_{\\sigma_1}, \\ldots, y_{\\sigma_N}) \\qquad \\end{split} \\end{equation}\\] This tells us that if we have some conditionally iid random variables and a subjective prior belief about some parameter \\(\\theta\\), then we have exchangeability. This is nice to have, but the implication in the other direction is much more interesting and powerful. Theorem 1.1 (de Finetti, informal) If a sequence of random variables \\((Y_1, \\ldots, Y_N)\\) is exchangeable, then its joint distribution can be written as \\[ \\pi(y_1, \\ldots, y_N) = \\int \\left\\{\\prod_{i=1}^N\\pi(y_i \\mid \\theta)\\right\\} \\pi(\\theta)\\, d\\theta \\] for some parameter \\(\\theta\\), some distribution on \\(\\theta\\), and some sampling model \\(\\pi(y_i \\mid \\theta)\\). This is a kind of existence theorem for Bayesian inference. It says that if we have exchangeable random varibales, then a parameter \\(\\theta\\) must exist and a subjective probability distribution \\(\\pi(\\theta)\\) must also exist. The argument against Bayesian inference is that it doesn’t guarantee a good subjective probability distribution \\(\\pi(\\theta)\\) exists. 1.4 Bayes’ Theorem Now we have an understanding of conditional probability and exchangeability, we can put these two together to understand Bayes’ Theorem. Bayes’ theorem is concerned with the distribution of the parameter \\(\\theta\\) given some observed data \\(y\\). It tries to answer the question: what does the data tell us about the model parameters? Theorem 1.2 (Bayes) The conditional distribution of \\(\\theta\\mid Y\\) has density \\[ \\pi(\\theta \\mid y) = \\frac{\\pi(y \\mid \\theta)\\pi(\\theta)}{\\pi(y)} \\] Proof. \\[\\begin{align} \\pi(\\theta \\mid y) &amp;= \\frac{\\pi(\\theta, y)}{\\pi(y)}\\\\ \\implies \\pi(\\theta, y) &amp;= \\pi(\\theta \\mid y)\\pi(y) \\end{align}\\] Analogously, using \\(\\pi(y \\mid \\theta)\\) we can derive \\[ \\pi(\\theta, y) = \\pi(y \\mid \\theta)\\pi(\\theta) \\] Putting these two terms equal to each other and dividing by \\(\\pi(y)\\) gives \\[ \\pi(\\theta \\mid y) = \\frac{\\pi(y \\mid \\theta)\\pi(\\theta)}{\\pi(y)} \\] There are four terms in Bayes’ theorem: The posterior distribution \\(\\pi(\\theta \\mid y)\\). This tells us our belief about the model parameter \\(\\theta\\) given the data we have observed \\(y\\). The likelihood function \\(\\pi(y \\mid \\theta)\\). The likelihood function is common to both frequentist and Bayesian methods. The prior distribution \\(\\pi(\\theta)\\). This is the distribution that describes our prior beliefs about the value of \\(\\theta\\). The form of \\(\\theta\\) should be decided before we see the data. It may be a vague distribution (e.g. \\(\\theta \\sim N(0, 10^2)\\)) or a specific distribution based on prior information from experts (e.g. \\(\\theta \\sim N(5.5, 1.3^2)\\)). The evidence of the data \\(\\pi(y)\\). This is sometimes called the average probability of the data or the marginal likelihood. In practice, we do not need to derive this term as it can be back computed to ensure the posterior distribution sums/integrates to one. A consequence of point four is that posterior distributions are usually derived proportionality, and (up to proportionality) Bayes’ theorem says \\[ \\pi(\\theta \\mid y) \\propto \\pi(y\\mid\\theta)\\pi(\\theta). \\] For simple distributions, knowing the density up to constant is sufficient to identify the distributions, e.g.  If \\(\\theta \\in \\mathbb{R}\\) has density \\(\\pi(\\theta) \\propto\\exp(-\\theta^2/2)\\) then \\(\\theta \\sim N(0,1)\\) If \\(\\theta\\in \\{1,2,3\\}\\) with density (or proabibilty mass function) \\(\\pi(1)\\propto 2, \\pi(2) \\propto 4, \\pi(3) \\propto 7\\), then since \\(\\pi(1)+\\pi(2)+\\pi(3) = 1\\), it must be the case \\(\\pi(1) = 2/13,\\pi(2) = 4/13,\\pi(3) = 7/13\\). For more complicated distributions, even if we only know them up to some constant, which may be very difficult to compute, we can still sample from such distributions using MCMC algorithms. Some history of Thomas Bayes. Thomas Bayes was an English theologean born in 1702. His “Essay towards solving a problem in the doctrine of chances” was published posthumously. It introduces theroems on conditional probability and the idea of prior probability. He discusses an experiment where the data can be modelled using the Binomial distribution and he guesses (places a prior distribution) on the probability of success. Richard Price sent Bayes’ work to the Royal Society two years after Bayes had died. In his commentary on Bayes’ work, he suggested that the Bayesian way of thinking proves the existance of God, stating: The purpose I mean is, to show what reason we have for believing that there are in the constitution of things fixt laws according to which things happen, and that, therefore, the frame of the world must be the effect of the wisdom and power of an intelligent cause; and thus to confirm the argument taken from final causes for the existence of the Deity. It’s not clear how Bayesian Thomas Bayes actually was, as his work was mainly about specific forms of probability theory and not his intepretation of it. The Bayesian way of thinking was really popularised by Laplace, who wrote about deductive probability in the early 19th century. "],["programming-in-r.html", "Chapter 2 Programming in R 2.1 Random Numbers, For Loops and R 2.2 Functions in R 2.3 Good Coding Practices", " Chapter 2 Programming in R 2.1 Random Numbers, For Loops and R This first computer lab is about getting used to R. The first step is to download R and Rstudio. Download R Download RStudio IDE The easiest way to learn R is by using it to solve problems. The lab contains four exercises and three ways of approaching the exercise (easy, medium and hard). If you’re new to R, use the easy approach and copy and paste the code straight into R – you’ll need to fill in a few blanks though. If you’ve used R before, or a similar programming language, stick to the medium and hard approaches. This is also an exercise in using Google. Googling around a problem of for specific commands can allow you to quickly find examples (most likely on Stack Overflow) with code you can use. There are three aims of this lab: Getting used to programming in R. Generating random numbers in R. Creating for loops in R. Example 2.1 Computationally verify that the Poisson distribution with rate \\(\\lambda = 100\\) can be approximated by a normal distribution with mean and variance 100. To do this, we can generate lots of samples from a Poisson(100) distribution and plot them on top of the density function of the normal distribution with mean and variance 100. R has four built-in functions for working with distributions. They take the form rdist, ddist, pdist, and qdist. You replace the dist part with the name of the distribution you want to work with, for example unif for the uniform distribution or norm for the normal distribution. As we are working with the the Poisson distribution, we will use pois. The prefixes allow you to work with the distribution in different ways: r gives you random numbers sampled form the distribution, d evaluates the density function, p evaluates the distribution function, and q evaluates the inverse distribution function (or quantile function). The function rpois allows us to generate samples from a Poisson distribution. We store 10,000 samples in a vector y by calling y &lt;- rpois(n = 10000, lambda = 100) We can generate a histogram of y using the hist command. Setting freq = FALSE, makes R plot a density histogram instead of a frequency histogram. Typing ?hist will give you more information about this hist(y, freq = FALSE, xlab = &quot;y&quot;, main = &quot;&quot;) The last thing to do is to plot the normal density on top. There are a couple of ways of doing this. The way below generates a uniform grid of points and then evaluates the density at each point. Finally, it adds a line graph of these densities on top. x &lt;- seq(from = 50, to = 150, by = 1) #create uniform grid on [50, 150] density &lt;- dnorm(x, mean = 100, sd = sqrt(100)) #compute density #plot together hist(y, freq = FALSE, xlab = &quot;y&quot;, main = &quot;&quot;) lines(x, density) The two match up well, showing the normal distribution is a suitable approximation here. Over the next two sessions, you will need to solve the following four problems in R. You can type ? before any function in R (e.g. ?rnorm) to bring up R’s helpage on the function. Googling can also bring up lots of information, possible solutions and support. Exercise 2.1 The changes in the Birmingham stock exchange each day can be modelled using a normal distribution. The price on day \\(i \\geq 1\\), \\(X_i\\) is given by \\[ X_i = \\alpha_i X_{i-1}, \\qquad \\alpha_i \\overset{i.i.d}{\\sim} N(1.001, 0.005^2). \\] The index begins at \\(X_0 = 100\\). Investigate the distribution of the value of the stock market on days 50 and 100. Hard. Use a simulation method to generate the relevant distributions. Medium. Simulate the value of \\(\\alpha_i\\) and use the cumprod command to plot a trajectory of the market index over 100 days. Use a for loop to repeat this 100 times and investigate the distribution of the value of the stock market on days 50 and 100. Easy. Fill in the blanks in the following code. # Plot one ---------------------------------------------------------------- x &lt;- rnorm(n = , mean = , sd = ) #Simulate daily change for 100 days plot(, type = &#39;l&#39;) #multiply each day by the previous days # Plot 100 realisations --------------------------------------------------- market.index &lt;- matrix(NA, 100, 100) #Initialise a matrix to store trajectories for(i in 1:100){ x &lt;- rnorm(n = , mean = , sd = ) market.index [, i] &lt;- } #Plot all trajectories matplot(market.index, type = &#39;l&#39;) #Get distribution of days 50 and 100 hist() hist() quantile(, ) quantile(, ) Exercise 2.2 You are an avid lottery player and play the lottery twice a week, every week for 50 years (a total of 5,200 times). The lottery has 50 balls labeled 1, …, 50 and you play the same 6 numbers each time. Six out of the 50 balls are chosen uniformly at random and the prize money is shown in the table below. Numbers Matched Prize Amount 0-2 £0 3 £30 4 £140 5 £1,750 6 £1,000,000 It costs you £2 to play each time. Simulate one set of 5,200 draws. How much do you win? What is your total profit/loss? Hard. Use a for loop and sequence of if else statements to generate your prize winnings. Medium. Use a for loop to generate the lottery numbers and prize winnings for each draw. Use the sample function to generate a set of lottery numbers and check they match against your numbers using the %in% function. Finally, use if else statements to check how much you have won each time. Easy. Fill in the blanks in the following code. my.numbers &lt;- #For loop to generate lottery numbers and prize winnings prize &lt;- numeric(5200) for(i in 1:5200){ #Generate lottery numbers draw &lt;- sample(, ) #Check how many match my numbers numbers.matched &lt;- #use %in% function #Compute prize winings if(numbers.matched &lt; 3) prize[i] &lt;- 0 else if() prize[i] &lt;- 30 else if() prize[i] &lt;- 140 else if() prize[i] &lt;- 1750 else prize[i] &lt;- 1000000 } #Summarise prize winnings table(prize) hist(prize) sum(prize) - 2*5200 Exercise 2.3 Estimate \\(\\pi\\). Hard. Use a rejection sampling algorithm. Medium. Generate lots of points \\((x, y)\\) on the unit square \\([0, 1]^2\\). Check each point to see if it lies within the unit circle. Use the proportion of points that lie within the unit circle to estimate \\(\\pi\\). Easy. Fill in the blanks in the following code. #Sample on unit square N &lt;- 10000 #number of points x &lt;- #sample N points uniformly at random on [0, 1] y &lt;- #sample N points uniformly at random on [0, 1] #Estimate pi r.sq &lt;- x^2 + y^2 #check how far from origin number.inside.circle &lt;- #count how many points inside unit cirlce pi.estimate &lt;- #Plot points par(pty = &quot;s&quot;) #make sure plot is square plot(x, y, cex = 0.1) #plot points theta &lt;- seq(0, pi/2, 0.01) #plot unit circle lines(x = cos(theta), y = sin(theta), col = &quot;red&quot;) Extra. Use a for loop to repeat this for \\(N = \\{1, \\ldots, 10000\\}\\). Record the estimate for \\(\\pi\\) for each value and the relative error. Exercise 2.4 A linear congruential generator (LCG) is a simple algorithm for generating random integers. Given a starting value \\(X_0\\), it generates a sequence of integers according to \\[ X_{i+1} = a X_i + c \\mod m. \\] Software that generates numbers using an LCG Setting \\(a = 3\\), \\(c = 2\\), \\(m = 7\\) and \\(X_0 = 0\\), generate 20 samples from this generator. Investigate the ‘randomness’ of this generator by creating the delay plot, where \\(X_{i-1}\\) is plotted against \\(X_{i}\\) One way to improve the quality of these generators is to shuffle the sequence generated. Generate two sequences \\(X\\) and \\(Y\\) from two different LCGs, and report the shuffled sequence \\(Z_j = X_{Y_j}\\). For the sequence \\(Y\\) use the values \\(a = 5\\), \\(c = 1\\), \\(m = 8\\) and \\(Y_0 = 2\\). As the past two exercises show, LCGs are notoriously poor. in the 1960s and 70s, RANDU was a widely used LCG developed by IBM. According to Wikipedia &gt; IBM’s RANDU is widely considered to be one of the most ill-conceived random number generators ever designed, and was described as “truly horrible” by Donald Knuth. The RANDU LCG uses \\(a = 2^{16} + 3\\), \\(c = 0\\), \\(m = 2^{31}\\) and \\(Y_0 = 1\\). Generate a sequence of 10,000 pseudorandom variables from the RANDU LCG and create the delay plot. The delay plot seems to show little relationship between \\(X_{i}\\) and \\(X_{i+1}\\). The third order delay plot is a 3d-plot with coordinate \\((X_i, X_{i+1}, X_{i+2})\\) and this plot shows a different picture. Create this plot using the code #install.packages(&quot;scatterplot3d&quot;) #you may need to install this package scatterplot3d::scatterplot3d(X[1:9998], X[2:9999], X[3:10000], angle=154, xlab = expression(X[i]), ylab = expression(X[i+1]), zlab = expression(X[i+2])) This is what makes the RANDU LCG so poor. Write down \\(X_{i+1}\\) and \\(X_{i+2}\\) in terms of \\(X_i\\). Show that \\(X_{i+2} = \\alpha X_{i+1} + \\beta X_{i}\\). Hard. Use a for loop to construct sequences from the LCGs \\(X\\) and \\(Y\\). Medium. Create a for loop to generate the value for the sequence \\(X_i\\) for \\(i = 1, \\ldots, 20\\). Modular arithmetic can be performed using the %% function. Create a new for loop to construct the sequence \\(Y\\). To shuffle the sequence \\(X\\) using \\(Y\\), you will need to subset \\(X\\) by \\(Y\\) in R. Easy. Fill in the blanks in the code below # 1. Shuffling --------------------------------------------------------------- X &lt;- numeric(21) #initialise vector to store X #Set values for LCG a &lt;- c &lt;- m &lt;- X[1]&lt;- #Run Generator for(i in 2:21){ X[i] &lt;- } X #Delay plot plot( , , xlab = expression(X[i-1]), ylab = expression(X[i]), type = &#39;l&#39;) # 2. Shuffling --------------------------------------------------------------- Y &lt;- numeric(21) #initialise vector to store Y #Set values for LCG a &lt;- c &lt;- m &lt;- Y[1]&lt;- #Run Generator for(i in 2:50){ Y[i] &lt;- } #report sequence Y X[Y] #Plot delay plot plot(x = ,y = , xlab = expression(Z[i-1]), ylab = expression(Z[i]), type = &#39;l&#39;) 2.2 Functions in R The purpose of this lab is to learn how to write functions is R. Functions are wrappers that allow you to easily repeat commands, as well as customise specific pieces of code. 2.2.1 Built in commands R has many build in commands and you used these in Computer Lab I. An example is the runif command from the second exercise. This function generates random numbers from an interval. The code chunk below shows it in action: u &lt;- runif(n = 10, min = -1, max = 1) u ## [1] 0.639640738 0.589739083 -0.229209739 -0.996186689 0.003693592 ## [6] -0.075658012 0.236584073 0.562933128 -0.294804496 0.064979338 The functions has three arguments: n the number of samples to be generated, min the lower limit of the interval, max the upper limit of the interval. In the code chunk above 10 random numbers were generated from the interval [-1, 1]. In R, you don’t need to label the arguments, so the following will sample the same number of samples from the same interval: u &lt;- runif(10, -1, 1) Although in most cases it helps to label the arguments for readability and avoiding undefined behaviour. Note that if you decide to omit the argument names in the function call, the arguments must appear exactly in the order defined by the function prototype (check the documentation ?function for specific cases). 2.2.2 User defined functions In many cases, we will need to repeat the same piece of code over and over again, or we will need to run it again with different values. In this case, we can write our own function. In R, there are two ways to type your own function. The first is to write a full function definition. The basic template is name.of.function &lt;- function(arguments){ #do something #produce result return(result) } The second way is an in-line function, which is sometimes useful for short functions. The template is name.of.function &lt;- function(arguments) #do something In this module, we’re going to use the full function way of writing functions. Example 2.2 In this example, we’re going to write a function to evaluate the normal density function. The density function is given by \\[ \\pi(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{\\left\\{-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\right\\}}. \\] We will need our function to take three arguments, the value at which the density function needs to be evaluated, and the mean and standard deviation of the distribution. normal.density &lt;- function(x, mu, sigma){ fraction.term &lt;- 1/sqrt(2*pi*sigma^2) exponent.term &lt;- -1/(2*sigma^2)*(x-mu)^2 result &lt;- fraction.term*exp(exponent.term) return(result) } We have split up the density into two parts to make it easier to code up and read. R has its own inbuilt normal density function dnorm and we can compare our function against R’s. Although R’s is faster and more reliable, we should get the same results. normal.density(x = 0.5, mu = 1, sigma = 0.5) ## [1] 0.4839414 dnorm(x = 0.5, mean = 1, sd = 0.5) ## [1] 0.4839414 Why might R’s function be faster and more reliable than ours? Exercise 2.5 Write a function to evaluate the log of the probability mass function of a Poisson distribution with rate \\(\\lambda\\). Exercise 2.6 Consider the stock exchange problem in Exercise 2.1. Write a function that simulates 100 days of the stock exchange. Use the replicate function to call this function 10,000 times. 2.3 Good Coding Practices We’ve written code to solve different problems. In this lab, we’re going to take a step back and think about what good R code does and doesn’t look like. 2.3.1 Code Style Code should be both efficient and easy to read. In most cases it’s better to write code that’s easy to read and less efficient, than highly efficient code that’s difficult to read. Some basic principles to make code easy to read are: Write short functions names, e.g. buy.loot.box is better than player.buys.one.loot.boox or blb. Document and comment code. In R comments start with #. Multiple short functions are better than long functions that do multiple things. Be consistent. Example 2.3 Review the tidyverse Style Guide. Example 2.4 Review Google’s R Style Guide. One way to ensure code style is consistent and bug free is to carry out code reviews. These are common both in academia and industry. A code review is where someone else goes through your code line-by-line ensuring it conforms to the company style and doesn’t have any bugs. Exercise 2.7 The following code is for Exercises 2.1 about the stock exchange. Restyle the code so it is easy to read. # Plot one ---------------------------------------------------------------- rnorm(100,1.001,0.5) -&gt; x plot(100*cumprod(x),type =&#39;l&#39;) # Plot 100 realisations --------------------------------------------------- X &lt;- matrix(NA, 100, 100) for(i in 1:100){ x &lt;- rnorm(100, 1.001, 0.005) X[,i] &lt;- 100*cumprod(x) } matplot(X,type =&#39;l&#39;) hist(X[50,]);hist(X[100,]);quantile(X[50,], c(0.25, 0.5, 0.75));quantile(X[100,], c(0.25, 0.5, 0.75)) Exercise 2.8 In pairs or groups, carry out a code review for one of your solutions to an exercise from a previous lab. Remember to Make sure the coding style is consistent. Identify any bugs. Be respectful and constructive in your feedback. "],["bayesian-inference.html", "Chapter 3 Bayesian Inference 3.1 Simple Examples 3.2 Reporting Conclusions from Bayesian Inference 3.3 Conjugate Prior and Posterior Analysis 3.4 Prediction 3.5 Non-informative Prior Distibrutions 3.6 Frequentist analysis of Bayesian methods 3.7 Hierarchical Models 3.8 Lab", " Chapter 3 Bayesian Inference 3.1 Simple Examples We start this chapter with two basic examples that only have one data point. They illustrate the point of prior distributions and motivate our discussions on conjugate priors later. Example 3.1 Suppose we have a model \\(Y \\mid \\theta \\sim N(\\theta, 1)\\) and we want to derive the posterior distribution \\(\\pi(\\theta\\mid y)\\). By Bayes’ theorem, \\[ \\pi(\\theta \\mid y) \\propto \\pi(y \\mid \\theta) \\pi(\\theta). \\] We know the form of \\(\\pi(y \\mid \\theta) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}(y - \\theta)^2}\\), but how should we describe our prior beliefs about \\(\\theta\\)? Here are three options: We can be very vague about \\(\\theta\\) – we genuinely don’t know about its value. We assign a uniform prior distribution to \\(\\theta\\) that takes values between -1,000 and +1,000, i.e. \\(\\theta \\sim U[-1000, 1000]\\). We can write explicitly its distribution as \\[ \\pi(\\theta) = \\begin{cases} \\frac{1}{2000}&amp; q \\in [-1000, 1000] \\\\ 0 &amp; \\textrm{otherwise.} \\end{cases} \\] Up to proportionality/constant, we have \\(\\pi(\\theta) \\propto 1\\) for \\(\\theta \\in [-1000, 1000]\\). After thinking hard about the problem, or talking to an expert, we decide that the only thing we know about \\(\\theta\\) is that it can’t be negative. We adjust our prior distribution from 1. to be \\(\\theta \\sim U[0, 1000]\\). Up to proportionality \\(\\pi(\\theta) \\propto 1\\) for \\(\\theta \\in [0, 1000]\\). We decide to talk to a series of experts about \\(\\theta\\) asking for their views on likely values of \\(\\theta\\). Averaging the experts opinions gives \\(\\theta \\sim N(3, 0.7^2)\\). This is a method known as prior elicitation. We now go and observe some data. After a lot of time and effort, we collect one data point: \\(y = 0\\). Now we have all the ingredients to construct the posterior distribution. We multiply the likelihood function evaluated at \\(y = 0\\) by each of the three prior distributions. This gives us the posterior distributions. These are For the first uniform prior distribution, the posterior distribution is \\(\\pi(\\theta \\mid {y}) \\propto \\exp\\left(-\\frac{1}{2}\\theta^2\\right)\\) for \\(\\theta \\in [-1000, 1000]\\). For the second uniform prior distribution, the posterior distribution is \\(\\pi(\\theta \\mid {y}) \\propto \\exp\\left(-\\frac{1}{2}\\theta^2\\right)\\) for \\(\\theta \\in [0, 1000]\\). For the normal prior distribution, the posterior distribution is \\(\\pi(\\theta \\mid {y}) \\propto \\exp\\left(-\\frac{1}{2}\\theta^2\\right)\\exp\\left(-\\frac{1}{2}\\left(\\frac{\\theta - 3}{0.7}\\right)^2\\right)\\). Combining like terms, we have \\(\\pi(\\theta \\mid {y}) \\propto \\exp\\left(-\\frac{1}{2}\\left(\\frac{1.49\\theta^2 - 6\\theta}{0.7^2}\\right)\\right)\\) for \\(\\theta \\in \\mathbb{R}\\). By further completing the square and comparing to the normal density, one can see the posterior distribution is actually normal with mean \\(300/149 \\approx 2\\) and variance \\(49/149 \\approx 0.33\\). #The likelihood function is the normal PDF #To illustrate this, we evaluate this from [-5, 5]. x &lt;- seq(-5, 5, 0.01) likelihood &lt;- dnorm(x, mean = 0, sd = 1) #The first prior distribution we try is a #uniform [-1000, 1000] distribution. This is a #vague prior distribution. uniform.prior &lt;- rep(1, length(x)) posterior1 &lt;- likelihood*uniform.prior #The second prior distribution we try is a uniform #[0, 1000] distribution, i.e. theta is non-negative. step.prior &lt;- ifelse(x &gt;= 0, 1, 0) posterior2 &lt;- likelihood*step.prior #The third prior distribution we try is a #specific normal prior distribution. It #has mean 3 and variance 0.7. normal.prior &lt;- dnorm(x, mean = 3, sd = 0.7) posterior3 &lt;- likelihood*normal.prior #Now we plot the likelihoods, prior and posterior distributions. #Each row corresponds to a different prior distribution. Each #column corresponds to a part in Bayes&#39; theorem. par(mfrow = c(3, 3)) plot(x, likelihood, type = &#39;l&#39;, xlab = &quot;&quot;, ylab = &quot;&quot;, yaxt = &quot;n&quot;, main = &quot;Likelihood&quot;) plot(x, uniform.prior, type = &#39;l&#39;, yaxt = &quot;n&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, main = &quot;Prior&quot;) plot(x, posterior1, type = &#39;l&#39;, yaxt = &quot;n&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, main = &quot;Posterior&quot;) plot(x, likelihood, type = &#39;l&#39;, xlab = &quot;&quot;, ylab = &quot;&quot;, yaxt = &quot;n&quot;) plot(x, step.prior, type = &#39;l&#39;, yaxt = &quot;n&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;) plot(x, posterior2, type = &#39;l&#39;, yaxt = &quot;n&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;) plot(x, likelihood, type = &#39;l&#39;, xlab = &quot;&quot;, ylab = &quot;&quot;, yaxt = &quot;n&quot;) plot(x, normal.prior, type = &#39;l&#39;, yaxt = &quot;n&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;) plot(x, posterior3, type = &#39;l&#39;, yaxt = &quot;n&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;) The posterior distribution is proportional to the likelihood function. The posterior distribution closely matches frequentist inference. Both the MLE and posterior mean are 0. We get a lopsided posterior distribution, that is proportional to the likelihood function for positive values of \\(\\theta\\), but is 0 for negative values of \\(\\theta\\). We get a normal posterior distribution. Example 3.2 (Binomial likelihood) A social media company wants to determine how many of its users are bots. A software engineer collects a random sample of 200 accounts and finds that three are bots. Assuming that any two accounts being a bot are independent of one another, she decides to model the outcome as \\(Y\\mid \\theta \\sim \\text{Bin}(200,\\theta)\\), and the observation is \\(y = 3\\). By Bayes’ theorem, we have \\[ \\pi(\\theta \\mid {y}) \\propto \\pi({y}\\mid \\theta) \\pi(\\theta). \\] Likelihood function \\(\\pi({y}\\mid \\theta)\\). The Binomial likelihood function is given by \\[ \\pi({y}\\mid \\theta) = \\begin{pmatrix} 200 \\\\ 3 \\end{pmatrix} \\theta^3(1-\\theta)^{197} \\propto \\theta^3(1-\\theta)^{197}. \\] Prior distribution \\(\\pi(\\theta)\\). We now need to describe our prior beliefs about \\(\\theta\\). We have no reason to suggest \\(\\theta\\) takes any specific value, so we use a uniform prior distribution \\(\\theta \\sim U[0, 1]\\), where \\(\\pi(\\theta) = 1\\) for \\(\\theta \\in [0, 1]\\). Posterior distribution \\(\\pi(\\theta \\mid {y})\\). We can now derive the posterior distribution up to proportionality \\[ \\pi(\\theta \\mid {y}) \\propto \\theta^3(1-\\theta)^{197}, \\qquad \\theta \\in (0,1). \\] This functional dependence on \\(\\theta\\) identifies that the posterior distribution \\(\\pi(\\theta \\mid {y})\\) is a Beta distribution. Recall that the density function for the Beta distribution with shape parameters \\(\\alpha\\) and \\(\\beta\\) is \\[ \\pi(x \\mid \\alpha, \\beta) = \\frac{1}{B(\\alpha,\\beta)}x^{\\alpha - 1}(1-x)^{\\beta - 1}, \\qquad x\\in(0,1). \\] Therefore, the posterior distribution is \\(\\textrm{Beta}(4, 198)\\). We also note that the uniform distribution on \\([0,1]\\), \\(U[0, 1]\\), is a special case of Beta distribution with \\(\\alpha = 1\\) and \\(\\beta = 1\\). 3.2 Reporting Conclusions from Bayesian Inference Posterior distribution \\(\\pi(\\theta \\mid y)\\) summaries all the information and uncertainty regarding the parameter \\(\\theta\\), given the data \\(y\\), e.g. it can be used to compute \\(\\pi( \\theta \\in A \\mid y) = \\int_{A} \\pi(\\theta \\mid y) d\\theta\\) for any given set \\(A\\), although computing this exactly also requires knowing \\(\\pi(\\theta\\mid y)\\) exactly. However, we are often also interesting in summarizing the distribution in some way and make our result easy to interpret. We could consider reporting point estimates of \\(\\theta\\). To give a specific estimate of \\(\\theta\\), we may use posterior mean, i.e. \\(\\mathbb{E}(\\theta\\mid y) = \\int \\theta \\, \\pi(\\theta \\mid y) d\\theta\\), which also requires knowing \\(\\pi(\\theta\\mid y)\\) exactly in order to compute it exactly, or posterior mode, defined as \\[ \\hat{\\theta}(y) := \\mathop{\\mathrm{arg\\,max}}_{\\theta \\in \\Theta} \\pi(\\theta\\mid y) = \\mathop{\\mathrm{arg\\,max}}_{\\theta \\in \\Theta} \\pi( y \\mid \\theta) \\pi(\\theta). \\] The posterior mode is also known as maximum a posteriori (MAP) estimate. If \\(\\pi(\\theta) \\propto 1\\), then \\(\\hat{\\theta}_{\\text{MAP}}(y) = \\hat{\\theta}_{\\text{MLE}}(y)= \\mathop{\\mathrm{arg\\,max}}_{\\theta \\in \\Theta} \\pi( y \\mid \\theta).\\) Another ideal property is that finding MAP does not require knowing the posterior exactly. In the previous example, the posterior distribution is \\(\\textrm{Beta}(4, 198)\\). The posterior mean is \\(\\frac{4}{198+4} = \\frac{2}{101}\\) and the posterior mode is \\(\\frac{4-1}{4+198-2} = \\frac{3}{200}\\). In addition to point estimates, it is important to share the uncertainty about \\(\\theta\\). In the frequentist framework, this achieved via confidence intervals. The Bayesian analogue is called credible intervals. Definition 3.1 An interval \\([l,u]\\) is called a credible interval at level \\(1-\\alpha\\), \\(\\alpha \\in (0,1)\\), for a random variable \\(\\theta \\in \\mathbb{R}\\) if \\[ \\pi(l \\leq \\theta \\leq u\\mid y) = \\int_{l}^u \\pi(\\theta\\mid y) = 1-\\alpha. \\] Although this definition does not identify a unique crediable interval at level \\(1-\\alpha\\), the most common choice is choosing \\(l\\) and \\(u\\) such that \\(\\pi(\\theta &lt; l\\mid y) = \\pi(\\theta &gt;u \\mid y) = \\alpha/2\\). This way of specifying creaiable intervals is known as equal-tailed intervals. Note that we can interpret a credible interval at level \\(1-\\alpha\\) as there is \\(100(1-\\alpha)\\%\\) probability that \\(\\theta\\) belongs to that particular interval given the data. The parameter \\(\\theta\\) is random and the interval is fixed in the definition of credible intervals. This is, in some sense, more intuitive than the interpretation of CIs, which relies on repeated sampling. Example 3.3 The 95% credible interval for the Binomial example is given by cred.int.95 &lt;- qbeta(c(0.025, 0.975), 4, 198) round(cred.int.95, 3) ## [1] 0.005 0.043 This says that we believe there is a 95% chance that the probability of an account being a bot lies between 0.005 and 0.043. 3.3 Conjugate Prior and Posterior Analysis We have seen from the two examples discussed so far Normal Prior + Normal Likelihood \\(\\longrightarrow\\) Normal Posterior Uniform Prior (Beta\\((1,1)\\)) + Binomial Likelihood \\(\\longrightarrow\\) Beta Posterior Notice that in both cases, the prior and posterior distribution belong to the same family of distributions. In this case, we say the prior is conjugate with respect to the likelihood function. Definition 3.2 (Conjugate Prior) For a given likelihood, \\(\\pi(y \\mid \\theta)\\), if the prior distribution \\(\\pi(\\theta)\\) and the posterior distribution \\(\\pi(\\theta \\mid {y})\\) are in the same family of distributions, then \\(\\pi(\\theta)\\) is a conjugate prior/ is conjugate with respect to \\(\\pi(y \\mid \\theta)\\). Example 3.4 (Exponential likelihood) Suppose \\(Y_1,\\dotsc,Y_n\\mid \\lambda \\overset{i.i.d.}{\\sim} \\mathrm{Exp}(\\lambda)\\), and consider a Gamma prior on the parameter \\(\\lambda \\sim \\text{Gamma}(\\alpha,\\beta)\\). Let \\(Y = (Y_1,\\dotsc,Y_n).\\) By Bayes’ Theorem, we can derive the posterior distribution of \\(\\theta \\mid Y\\) as \\[\\begin{align*} \\pi(\\lambda\\mid y) \\propto \\pi(y\\mid \\lambda) \\pi(\\lambda) &amp;= \\prod_{i=1}^n\\pi(y_i\\mid \\lambda)\\pi(\\lambda) \\\\ &amp;= \\lambda^n e^{-\\lambda\\sum_{i=1}^n y_i}\\pi(\\lambda) \\\\ &amp; \\propto \\lambda^n e^{-\\lambda\\sum_{i=1}^n y_i} \\lambda^{\\alpha-1}e^{-\\beta \\lambda}\\\\ &amp; = \\lambda^{\\alpha+n-1} e^{-\\lambda (\\beta+\\sum_{i=1}^ny_i)}. \\end{align*}\\] This means that the posterior distribution is Gamma\\((\\alpha+n, \\beta+\\sum_{i=1}^ny_i)\\). Therefore, we conclude that Gamma distribution is a conjugate prior for Exponential likelihood. Using basic properties of Gamma distribution, we can then obtain \\[ \\mathbb{E}(\\lambda\\mid y) = \\frac{\\alpha+n}{\\beta+\\sum_{i=1}^ny_i}. \\] Note that the mean of the prior distribution is \\(\\mathbb{E}(\\lambda) = \\alpha/\\beta\\) and the MLE for \\(\\lambda\\) is \\(\\hat{\\lambda}_{\\text{MLE}} = \\frac{n}{\\sum_{i=1}^ny_i}\\). We can interpret the posterior mean as a weighted average of prior mean and the MLE by noticing \\[ \\mathbb{E}(\\lambda\\mid y) = \\frac{\\beta}{\\beta+\\sum_{i=1}^ny_i} \\cdot \\frac{\\alpha}{\\beta} + \\frac{\\sum_{i=1}^ny_i}{\\beta+\\sum_{i=1}^ny_i} \\cdot \\frac{n}{\\sum_{i=1}^ny_i}. \\] If either \\(n\\) is large, i.e. we have abundant data, or \\(\\beta\\) is small, i.e. we have a vague prior (vague in the sense \\(\\mathrm{Var}(\\lambda) = \\alpha/\\beta^2\\) is large), the weight on MLE is close to \\(1\\), and we would have \\(\\mathbb{E}(\\lambda\\mid y) \\approx \\hat{\\lambda}_{\\text{MLE}}\\). On a slightly more technical level (don’t worry if you are confused about this), it is not hard to show that \\(\\mathbb{E}(\\lambda\\mid Y) - \\hat{\\lambda}_{\\text{MLE}}(Y)\\) converges in probability to \\(0\\), under \\(Y_1, \\dotsc,Y_n \\overset{i.i.d}{\\sim} \\text{Exp}(\\lambda)\\). We discuss a real data example now, which also illustrates the effects of choosing different \\(\\beta\\) in the prior distribution. An insurance company wants to estimate the average time until a claim is made on a specific policy using Bayesian inference. The data \\(\\boldsymbol{y} = \\{14, 10, 6, 7, 13, 9, 12, 7, 9, 8\\}\\) are collected, where each entry represents the number of months until a claimed is made. Likelihood function The exponential distribution is a good way of modelling lifetimes or the length of time until an event happens. Therefore, the company decides to model the observed data as realizations from \\(\\text{Exp}(\\lambda)\\), where \\(\\lambda\\) represents the number of claims per month. Assuming all the claims are independent, the likelihood function is given by \\[\\begin{align*} \\pi(\\boldsymbol{y} \\mid \\lambda) &amp;= \\prod_{i=1}^{10} \\lambda e^{-\\lambda y_i} \\\\ &amp; = \\lambda^{10}e^{-\\lambda \\sum_{i=1}^{10} y_i} \\\\ &amp; = \\lambda^{10} e^{-95\\lambda}. \\end{align*}\\] Prior distribution \\(\\pi(\\lambda)\\). As we are modelling a rate parameter, we know it must be positive. We decide to use an exponential prior distribution for \\(\\lambda\\), but leave the choice of the rate parameter up to the insurance professionals at the insurance company. The prior distribution is given by \\(\\lambda \\sim \\textrm{Exp}(\\beta),\\) which is the same as \\(\\lambda \\sim \\text{Gamma}(1, \\beta)\\). Posterior distribution Using the formula we obtained before, the posterior distribution is \\(\\textrm{Gamma}(11, 95 + \\beta)\\). The posterior mean months until a claim is \\(\\frac{11}{95 + \\beta}\\). We can see the effect of the choice of rate parameter \\(\\beta\\) in this mean. Small values of \\(\\beta\\) yield vague prior distribution, since \\(\\text{Var}(\\lambda) = 1/\\beta^2\\), which plays a minimal role in the posterior distribution. Large values of \\(\\beta\\) result in specific prior distributions that contribute a lot to the posterior distribution. The plots below show the prior and posterior distributions for \\(\\beta = 0.01\\), \\(\\beta = 50\\) and \\(\\beta = 150\\). plot.distributions &lt;- function(gamma.prior){ #evaluate at selected values of lambda lambda &lt;- seq(0.001, 0.3, 0.001) #evaluate prior density prior &lt;- dexp(lambda, rate = gamma.prior) #evaluate posterior density posterior &lt;- dgamma(lambda, shape = 11, rate = 95 + gamma.prior) #plot plot(lambda, posterior, type= &#39;l&#39;, ylim = c(0, 50), xlab = expression(lambda), ylab = &quot;density&quot;) lines(lambda, prior, lty = 2) legend(&#39;topright&#39;, lty = c(1, 2), legend = c(&quot;Posterior&quot;, &quot;Prior&quot;), bty = &quot;n&quot;) } plot.distributions(0.01) plot.distributions(50) plot.distributions(150) The insurance managers recommend that because this is a new premium, a vague prior distribution be used and \\(\\gamma = 0.01\\). The posterior mean is \\(\\frac{11}{95.01} \\approx 0.116\\) and the 95% credible interval is round(qgamma(c(0.025, 0.975), 11, 95.01), 3) ## [1] 0.058 0.194 Example 3.5 (Normal likelihood) Suppose \\(Y_1,\\dotsc,Y_N \\mid \\mu \\overset{i.i.d}{\\sim} N(\\mu, \\sigma^2)\\) and assume the value of \\(\\sigma &gt;0\\) is known. Let \\(\\boldsymbol{Y} = (Y_1,\\dotsc,Y_n)\\). We impose a Normal prior distribution on the unknown parameter \\(\\mu \\sim N(\\mu_0, \\sigma_0^2)\\). By Bayes’ theorem, the posterior distribution is \\[ \\pi(\\mu \\mid \\boldsymbol{y}) \\propto \\pi(\\boldsymbol{y} \\mid \\mu) \\pi(\\mu) \\] Likelihood function. As the observations are independent, the likelihood function is given by the product of the \\(N\\) normal density functions as follows, \\[\\begin{align*} \\pi(\\boldsymbol{y} \\mid \\mu) &amp;= \\prod_{i=1}^{N} \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{-\\frac{(y_i - \\mu)^2}{2\\sigma^2}\\right\\} \\\\ &amp;= (2\\pi\\sigma^2)^{-\\frac{N}{2}}\\exp\\left\\{-\\sum_{i=1}^{N}\\frac{(y_i - \\mu)^2}{2\\sigma^2}\\right\\}. \\end{align*}\\] Prior distribution \\[ \\pi(\\mu) = \\frac{1}{\\sqrt{2\\pi\\sigma_0^2}}\\exp\\left\\{-\\frac{1}{2\\sigma_0^2}(\\mu - \\mu_0)^2\\right\\}. \\] Posterior distribution. To derive the posterior distribution, up to proportionality, we multiply the prior distribution by the likelihood function. As the fractions out the front of both terms do not depend on \\(\\mu\\), we can ignore these. \\[\\begin{align*} \\pi(\\mu \\mid \\boldsymbol{y}) &amp;\\propto\\exp\\left\\{-\\sum_{i=1}^{N}\\frac{(y_i - \\mu)^2}{2\\sigma^2}\\right\\} \\exp\\left\\{-\\frac{1}{2\\sigma_0^2}(\\mu - \\mu_0)^2\\right\\} \\\\ &amp; = \\exp\\left\\{-\\sum_{i=1}^{N}\\frac{(y_i - \\mu)^2}{2\\sigma^2}-\\frac{1}{2\\sigma_0^2}(\\mu - \\mu_0)^2\\right\\} \\\\ &amp; = \\exp\\left\\{-\\frac{\\sum_{i=1}^{N}y_i^2}{2\\sigma^2} + \\frac{\\mu\\sum_{i=1}^{N}y_i}{\\sigma^2} - \\frac{N\\mu^2}{2\\sigma^2} - \\frac{\\mu^2}{2\\sigma_0^2} + \\frac{\\mu\\mu_0}{\\sigma_0^2} - \\frac{\\mu_0^2}{2\\sigma_0^2}\\right\\}. \\end{align*}\\] We can drop the first and last term as they do not depend on \\(\\mu\\). With some arranging, the equation becomes \\[ \\pi(\\mu \\mid \\boldsymbol{y}) \\propto \\exp\\left\\{-\\mu^2\\left(\\frac{N}{2\\sigma^2} + \\frac{1}{2\\sigma_0^2}\\right) + \\mu\\left(\\frac{\\sum_{i=1}^{N}y_i}{\\sigma^2} + \\frac{\\mu_0}{\\sigma_0^2} \\right) \\right\\} \\] Defining \\(a =\\left(\\frac{\\sum_{i=1}^{N}y_i}{\\sigma^2} + \\frac{\\mu_0}{\\sigma_0^2} \\right)\\) and \\(b^2 = \\left(\\frac{N}{\\sigma^2} + \\frac{1}{\\sigma_0^2}\\right)^{-1} = \\frac{\\sigma^2\\sigma_0^2}{N\\sigma_0^2+\\sigma^2}\\) tidies this up and gives \\[ \\pi(\\mu \\mid \\boldsymbol{y}) \\propto \\exp\\left\\{-\\frac{\\mu^2}{2b^2} + \\mu a \\right\\}. \\] Our last step to turning this into a distribution is completing the square. Consider the exponent term, completing the square becomes \\[ -\\frac{\\mu^2}{2b^2} + \\mu a = -\\frac{1}{2b^2}\\left(\\mu - {a}{b^2} \\right)^2 + \\frac{a^2b^2}{2}. \\] Therefore, the posterior distribution, up to proportionality, is given by \\[ \\pi(\\mu \\mid \\boldsymbol{y}) \\propto \\exp\\left\\{-\\frac{1}{2b^2}\\left(\\mu - ab^2 \\right)^2\\right\\}, \\] and so the posterior distribution of \\(\\mu \\mid \\boldsymbol{Y}\\) is \\(N(ab^2, b^2)\\), where the posterior mean is \\[ \\mu_{\\text{post}}:=ab^2 = \\frac{\\sigma_0^2\\sum_{i=1}^N y_i+\\mu_0\\sigma^2}{N\\sigma_0^2+\\sigma^2} = \\frac{N\\sigma_0^2}{N\\sigma_0^2 + \\sigma^2} \\cdot \\frac{\\sum_{i=1}^Ny_i}{N} + \\frac{\\sigma^2}{N\\sigma_0^2 + \\sigma^2} \\cdot \\mu_0 \\] and the posterior variance is \\(b^2\\). Note that we have again that the posterior mean is a weighted average between the prior mean \\(\\mu_0\\) and the MLE for \\(\\mu\\), \\(\\hat{\\mu}_{\\text{MLE}} = \\overline{y} = \\sum y_i/N\\). If we have either abundant data (large \\(N\\)) or a vague prior (large \\(\\sigma^2_0\\)), the weight on MLE is close to \\(1\\) and we then have the posterior mean \\(\\mathbb{E}(\\mu\\mid \\boldsymbol{y}) \\approx \\sum y_i/N\\). To further interpret the weights, we write the posterior mean as \\[ \\mu_{\\text{post}} = \\frac{N/\\sigma^2}{N/\\sigma^2 + 1/\\sigma_0^2} \\cdot \\frac{\\sum_{i=1}^Ny_i}{N} + \\frac{1/\\sigma^2_0}{N/\\sigma^2 + 1/\\sigma_0^2} \\cdot \\mu_0 \\] Note that the precision of a univariate distribution is the reciprocal of its variance. Therefore, the prior distribution has precision \\(1/\\sigma_0^2\\) and the MLE \\(\\hat{\\mu}_{\\text{MLE}} (\\boldsymbol{Y})= \\sum_{i=1}^NY_i/N \\sim N(\\mu,\\sigma^2/N)\\) has precision \\(N/\\sigma^2\\), if \\(Y_1\\dotsc,Y_N \\overset{i.i.d}\\sim N(\\mu,\\sigma^2)\\). Hence, we conclude that the posterior mean is a weighted average of the prior mean and the sample mean, with weights proportional to the precisions. Furthermore, with the notion of precision, it is very easy to remember the formula for posterior variance \\(b^2\\) since it satisfies \\[ \\frac{1}{b^2} = \\frac{N}{\\sigma^2}+\\frac{1}{\\sigma_0^2}, \\] i.e. the posterior precision is the sum of the prior precision and MLE precision in this Normal-Normal example. Finally, let’s consider constructing (equal-tailed) credible intervals at level \\(1-\\alpha\\), \\(\\alpha &lt; 1/2\\). We need to find \\(l,u \\in \\mathbb{R}\\) such that \\[ \\pi(\\mu &lt; l\\mid \\boldsymbol{y}) = \\pi(\\mu &gt;u\\mid \\boldsymbol{y}) = \\alpha/2 \\] so that \\(\\pi(\\mu \\in [l,u] \\mid \\boldsymbol{y}) = 1-\\alpha\\). To do so, we have \\[ \\pi\\Big(\\frac{\\mu - \\mu_{\\text{post}}}{b} &gt; \\frac{u-\\mu_{\\text{post}}}{b}\\mid \\boldsymbol{y} \\Big) = \\pi\\Big(Z &gt; \\frac{u-\\mu_{\\text{post}}}{b}\\Big) = \\alpha/2, \\] where \\(Z \\sim N(0, 1)\\). Therefore, we can choose \\[ \\frac{u-\\mu_{\\text{post}}}{b} = \\Phi^{-1}(1-\\alpha/2) \\implies u = \\mu_{\\text{post}} + b \\cdot \\Phi^{-1}(1-\\alpha/2) \\] and similarly, choosing \\(l = \\mu_{\\text{post}} - b \\cdot \\Phi^{-1}(1-\\alpha/2)\\) guarantees that \\[ \\mu_{\\text{post}} \\pm b \\cdot \\Phi^{-1}(1-\\alpha/2) \\] is a \\(1-\\alpha\\) level credible interval for \\(\\mu\\). Recall that when \\(N\\), the sample size, is large, \\(\\mu_{\\text{post}} \\approx \\hat{\\mu}_{\\text{MLE}}\\) and \\(b^2 \\approx \\sigma^2/N\\), this credible interval is approximately equal to the confidence interval obtained based on MLE, i.e. \\(\\hat{\\mu}_{\\text{MLE}}\\pm \\frac{\\sigma}{\\sqrt{n}} \\cdot \\Phi^{-1}(1-\\alpha/2)\\). This phenomenon of numerical equivalence between credible interval and confidence interval based on MLE when \\(N\\) is large holds more generally, as we will discuss at the end of this chapter. However, if \\(N\\) is small, or there is a very strong (specific) prior belief, there could be a significant difference between credible intervals and confidence intervals. We now explore the posterior distribution using R. We simulate some data with \\(N = 30\\), \\(\\mu = 5\\) and \\(\\sigma^2 = 1\\). Consider a very vague prior distribution \\(\\mu \\sim N(0,1000^2)\\). #data N &lt;- 30 sigma &lt;- 1 y &lt;- rnorm(N, 5, sigma) #MLE for mu mean(y) ## [1] 5.29357 #prior sigma0 &lt;- 1000 mu0 &lt;- 0 #posterior sigma1.sq &lt;- (1/(sigma0^2) + N/(sigma^2))^-1 mu1 &lt;- sigma1.sq*(sum(y)/(sigma^2) + mu0/(sigma0^2)) c(mu1, sigma1.sq) #output mean and variance ## [1] 5.29356950 0.03333333 #Create plot mu &lt;- seq(4, 6, 0.01) posterior &lt;- dnorm(mu, mean = mu1, sd = sqrt(sigma1.sq)) plot(mu, posterior, type =&#39;l&#39;) The 95% credible interval for the population’s mean reaction time is qnorm(c(0.025, 0.975), mu1, sqrt(sigma1.sq)) ## [1] 4.935731 5.651408 3.4 Prediction In many cases, although we are interested in drawing inference for the model parameters, what we may also be interested in is predicting new values. Suppose we observe some data \\(\\boldsymbol{y}\\) and model them using a statistical model parameterised by \\(\\theta\\), and assign a prior distribution \\(\\pi(\\theta)\\) and hence derive the posterior distribution \\(\\pi(\\theta \\mid \\boldsymbol{y})\\). The quantity we are interested in is some future observation \\(Z\\), we would like to the derive the distribution of \\(Z\\) given the observed data \\(\\boldsymbol{y}\\), which has density \\(\\pi(z \\mid \\boldsymbol{y})\\). This distribution, known as the posterior predictive distribution can be computed using the conditional version of law of total probability, i.e. \\[ \\pi(z \\mid \\boldsymbol{y}) = \\int \\pi(z, \\theta \\mid \\boldsymbol{y}) \\,d\\theta = \\int \\pi(z\\mid \\boldsymbol{y}, \\theta)\\pi(\\theta \\mid \\boldsymbol{y})\\, d\\theta. \\] If we further assume \\(Z\\) and \\(Y\\) are conditionally independent given \\(\\theta\\), meaning that the future data is generated independently from the same model as the observed data \\(\\boldsymbol{y}\\), then \\(\\pi(z\\mid \\boldsymbol{y}, \\theta) = \\pi(z\\mid \\theta)\\), and therefore \\[\\begin{equation} \\pi(z \\mid \\boldsymbol{y}) = \\int \\pi(z \\mid \\theta) \\pi(\\theta \\mid \\boldsymbol{y})\\, d\\theta. \\tag{3.1} \\end{equation}\\] For questions in the problem sheets, summative assignments and the final exam, you may assume the posterior predictive distributions take the form in Equation (3.1). Example 3.6 Students have to submit coursework for a particular statistical module. However, each semester a number of students miss the deadline and hand in their coursework late. Last year, three out of 30 students handed their coursework in late. This year, the course has thirty students in. How many students can we expect to hand in their coursework late? We can model the number of students handing their coursework in late, denoted by \\(Y\\), using a Binomial distribution, i.e. \\(Y \\mid \\theta \\sim \\textrm{Bin}(n, \\theta)\\) where \\(n\\) is the number of students and \\(\\theta\\) is the probability of any particular student handing in their coursework late. As in Example 3.2, we assign a uniform prior distribution to \\(\\theta \\sim U[0, 1]\\). Given the observed data, we can derive \\(\\theta \\mid \\boldsymbol{y} \\sim \\text{Beta}(4, 28)\\) (See problem sheets for the derivation of the general posterior distribution of Beta prior with Binomial likelihood). Now we can derive the posterior predictive distribution of \\(Z\\), the number of students who hand in late. We model \\(Z\\) using a Binomial distribution, \\(Z \\mid \\theta \\sim \\textrm{Bin}(30, \\theta)\\). The distribution of \\(Z\\) given the observed data is \\[\\begin{align*} \\pi(z \\mid \\boldsymbol{y}) &amp;= \\int_0^1 \\pi(z \\mid \\theta) \\pi(\\theta \\mid \\boldsymbol{y})\\, d\\theta \\\\ &amp; = \\int_0^1 \\begin{pmatrix} 30 \\\\ z \\end{pmatrix} \\theta^z (1-\\theta)^{30 - z} \\frac{1}{{B}(4,28)}\\theta^{3}(1-\\theta)^{27}\\, d\\theta \\\\ &amp; = \\begin{pmatrix} 30 \\\\ z \\end{pmatrix}\\frac{1}{{B}(4,28)}\\int_0^1 \\theta^{z + 3}(1-\\theta)^{57 - z}\\, d\\theta \\\\ \\end{align*}\\] This integral is difficult to evaluate immediately. But by multiplying (and dividing outside the integral) by a constant, we can turn it into the density function of Beta\\((4 + z, 58 - z)\\), which integrates to 1. \\[\\begin{align*} \\pi(z \\mid \\boldsymbol{y}) &amp; = \\begin{pmatrix} 30 \\\\ z \\end{pmatrix}\\frac{B(z+4,58-z)}{{B}(4,28)}\\int_0^1 \\frac{1}{B(z+4,58-z)}\\theta^{z + 3}(1-\\theta)^{57 - z}\\, d\\theta \\\\ &amp; = \\begin{pmatrix} 30 \\\\ z \\end{pmatrix} \\frac{B(z+4,58-z)}{{B}(4,28)} \\quad \\textrm{for } z \\in \\{0,1,...,30 \\}. \\end{align*}\\] This is an example of Beta-Binomial distribution. This code implements the distribution using a property of the Beta function \\[ B(a,b) = \\frac{\\Gamma(a)\\Gamma{(b)}}{\\Gamma(a+b)}. \\] beta.binom.posterior.predictive.distribution &lt;- function(z){ numerator &lt;- gamma(32)*gamma(z + 4)*gamma(58-z) denominator &lt;- gamma(4)*gamma(28)*gamma(62) output &lt;- choose(30, z)*numerator/denominator return(output) } We can check that our posterior predictive distribution is a valid probability mass function by checking that the probabilities sum to one. z &lt;- 0:30 ppd &lt;- beta.binom.posterior.predictive.distribution(z) sum(ppd) ## [1] 1 plot(z, ppd, xlab = &quot;z&quot;, ylab = &quot;Posterior predictive mass&quot;) The expected number of students who hand in late is 3.75 and there’s a 95.4% chance that up to 8 students will hand in late. z%*%ppd #expectation ## [,1] ## [1,] 3.75 cbind(z, cumsum(ppd)) #CDF ## z ## [1,] 0 0.06029453 ## [2,] 1 0.18723037 ## [3,] 2 0.35156696 ## [4,] 3 0.51889148 ## [5,] 4 0.66530044 ## [6,] 5 0.78021765 ## [7,] 6 0.86309065 ## [8,] 7 0.91880359 ## [9,] 8 0.95404202 ## [10,] 9 0.97513714 ## [11,] 10 0.98713498 ## [12,] 11 0.99363285 ## [13,] 12 0.99698773 ## [14,] 13 0.99863936 ## [15,] 14 0.99941423 ## [16,] 15 0.99976022 ## [17,] 16 0.99990696 ## [18,] 17 0.99996591 ## [19,] 18 0.99998826 ## [20,] 19 0.99999622 ## [21,] 20 0.99999887 ## [22,] 21 0.99999969 ## [23,] 22 0.99999992 ## [24,] 23 0.99999998 ## [25,] 24 1.00000000 ## [26,] 25 1.00000000 ## [27,] 26 1.00000000 ## [28,] 27 1.00000000 ## [29,] 28 1.00000000 ## [30,] 29 1.00000000 ## [31,] 30 1.00000000 Example 3.7 (Bayesian Linear Model) We consider performing Bayesian inference and prediction for the linear model in this example. For \\(i = 1,\\dotsc,n\\), let \\((x_i, Y_i) \\in \\mathbb{R}^{p+1}\\) be generated from \\[ Y_i = x_i^{\\top}\\beta + \\varepsilon_i, \\quad \\varepsilon_i \\overset{i.i.d}{\\sim} N(0, \\sigma^2), \\] where \\(x_i = (x_{i1}, \\dotsc, x_{ip})^{\\top}\\) is the covariate for the \\(i\\)-th individual, and \\(\\beta = (\\beta_1, \\dotsc, \\beta_{p})^{\\top}\\) is the \\(p\\)-dimensional regression parameter. The notation \\(\\top\\) stands for transpose of a vector and it makes both \\(x_i\\) and \\(\\beta\\) column vectors. We will assume the covariates \\(x_i\\)’s are fixed and the value of \\(\\sigma^2\\) is known. A more compact way of writing the linear model is \\[ Y = X\\beta + \\varepsilon, \\] where \\[\\begin{align*} Y = \\begin{pmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{pmatrix}, \\quad X = \\begin{pmatrix} x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1p} \\\\ x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{np} \\end{pmatrix}, \\quad \\varepsilon = \\begin{pmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n \\end{pmatrix} \\sim N(0, \\sigma^2 I_{n \\times n}). \\end{align*}\\] We are interested in two tasks: Find the posterior distribution \\(\\pi(\\beta\\mid y, X, \\sigma ) = \\pi(\\beta\\mid y)\\), which is a multivariate distribution on \\(\\mathbb{R}^p\\). Note that we can remove \\(X,\\sigma\\) from conditioning since they are assumed to deterministic and hence indepdenet of any random variables. Consider a future observation \\((x&#39;,Y&#39;)\\) such that \\(Y&#39; x&#39;^{\\top}\\beta+\\varepsilon, \\varepsilon \\sim N(0,\\sigma^2)\\). Again, assume \\(x_i\\) and \\(\\sigma\\) are known. Find the posterior predictive distribution \\(\\pi(y&#39; \\mid y, X, x&#39;, \\sigma) = \\pi(y&#39; \\mid y, x&#39;)\\). For the first task, we consider a Normal prior distribution on \\(\\beta\\), \\(\\beta \\sim N(0, c^2 I_{p \\times p})\\). Note that \\(Y\\mid \\beta \\sim N(X\\beta, \\sigma^2I_{n \\times n})\\), so this is again an example with normal prior and normal likelihood and it is reasonable to believe (hopefully) that the posterior distribution is also normal. At a high level, deriving this posterior requires the same procedure (e.g. completing the square) as in Example 3.5. However, in this multivariate setting, the calculation involves matrix multiplication and can be a bit daunting. Luckily, we can apply a general formula, known as the Bayes rule for Gaussians; see Section 3.3.1 in the book “Probabilistic machine learning: an introduction” by Kevin Murphy for more details. The general formula implies that \\[ \\beta \\mid Y \\sim N({\\mu}, \\Sigma), \\quad \\mu = \\Big(X^{\\top}X+\\frac{\\sigma^2}{c^2}I\\Big)^{-1}X^{\\top}Y, \\quad \\Sigma = \\Big(\\frac{1}{\\sigma^2}X^{\\top}X + \\frac{1}{c^2}I\\Big)^{-1}. \\] We point out here an interesting connection to ridge regression. Recall that the ordinary least square (OLS) estimator in the frequentist framework for \\(\\beta\\) is \\[ \\hat{\\beta}_{\\text{OLS}} = (X^{\\top}X)^{-1}X^{\\top}Y = \\mathop{\\mathrm{arg\\,min}}_{\\beta \\in \\mathbb{R}^p} \\|Y-X\\beta\\|_2^2, \\] and for the matrix \\(X^{\\top}X\\) to be invertible, it must hold \\(n &gt; p\\), i.e. the sample size must be larger than the dimension of the covariates. In what is known as the high-dimensional setting, i.e. \\(p &gt; n\\), OLS cannot be applied and we typically add a penalty term in the original optimisation problem. Ridge regression estimator is obtained by \\[ \\hat{\\beta}_{\\text{ridge}}^{\\lambda} = \\mathop{\\mathrm{arg\\,min}}_{\\beta \\in \\mathbb{R}^p} \\Big\\{\\|Y-X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_2^2\\Big\\} = (X^{\\top}X+\\lambda I)^{-1}X^{\\top}Y, \\] where a penalty on the squared \\(\\ell_2\\) norm of the regression parameter \\(\\beta\\) is added to the objective function. Now, we observe that the posterior mean is actually equivalent to the ridge regression estimator \\(\\hat{\\beta}_{\\text{ridge}}^{\\lambda}\\) with \\(\\lambda = \\sigma^2/c^2\\). To further understand why this happens, we note that since posterior distribution is symmetric, the posterior mean and the MAP estimate coincide. Therefore, we have \\[\\begin{align*} \\mu = \\mathop{\\mathrm{arg\\,max}}_{\\beta} \\pi(Y \\mid \\beta) \\pi(\\beta) &amp;= \\mathop{\\mathrm{arg\\,max}}_{\\beta}\\Big\\{ \\log\\pi(Y \\mid \\beta) + \\log \\pi(\\beta) \\Big\\} \\\\ &amp;= \\mathop{\\mathrm{arg\\,min}}_{\\beta} \\Big\\{\\|Y-X\\beta\\|_2^2 + \\frac{\\sigma^2}{c^2}\\|\\beta\\|_2^2\\Big\\}, \\end{align*}\\] since \\(\\pi(Y\\mid \\beta) \\propto \\exp(-\\frac{\\|Y-X\\beta\\|^2}{2\\sigma^2})\\) and \\(\\pi(\\beta) \\propto \\exp(-\\frac{\\|\\beta\\|^2}{2c^2})\\) due to the fact that they both follow multivariate normal distributions. To find the posterior predictive distribution \\(\\pi(y&#39;\\mid y, x&#39;)\\), one can of course find it by computing \\[ \\pi(y&#39;\\mid y, x&#39;) = \\int \\pi(y&#39;\\mid \\beta, x&#39;) \\pi(\\beta \\mid y) d\\beta. \\] Since \\(\\pi(y&#39;\\mid \\beta, x&#39;)\\) and \\(\\pi(\\beta \\mid y)\\) are both normally distributed, this integral can be computed and still yields a normal distribution for the posterior predictive distribution. However, a much simpler argument is to notice that \\(y&#39; = x&#39;^{\\top}\\beta+\\varepsilon\\), where \\(\\varepsilon \\sim N(0,\\sigma^2)\\) is independent of \\(\\beta\\). Since we have the posterior distribution \\(\\beta \\mid Y\\) is a Normal distribution, the posterior predictive distribution just a linear combination of independent Normal distributions, which is also Normal with mean \\(\\mu_{pred}\\) and variance \\(\\sigma^2_{pred}\\) \\[ \\mu_{pred} = x_i&#39;^{\\top}\\mu, \\quad \\sigma^2_{pred} = x_{i}&#39;^{\\top} \\Sigma x_i&#39;+\\sigma^2. \\] Example 11.7.3 on page 406 from the book “Probabilistic machine learning: an introduction” by Kevin Murphy illustrates a simple Bayesian linear regression example using simulation. 3.5 Non-informative Prior Distibrutions We have seen in a few examples how the choice of the prior distribution (and prior parameters) can impact posterior distributions and the resulting conclusions. As the choice of prior distribution is subjective, it is the main criticism of Bayesian inference. A possible way around this is to use a prior distribution that reflects a lack of information about \\(\\theta\\). Consider a very simple setting where \\(X\\mid \\theta \\sim \\text{Bern}(\\theta)\\) with \\(\\pi(x\\mid \\theta) = \\theta^x(1-\\theta)^{1-x}\\), for \\(\\theta \\in [0,1]\\) and \\(x\\in \\{0,1\\}\\). It would appear that the only sensible choice of prior that reflects the lack of information about \\(\\theta\\) is \\(\\theta \\in U[0,1]\\). However, such a uniform prior distribution can lead to unintended side effects. Consider a reparametrisation of the Bernoulli model using \\(\\psi = \\theta^2\\), which leads to \\[ \\pi(x\\mid \\psi) = (\\sqrt{\\psi})^x(1-\\sqrt{\\psi})^{1-x} \\] for \\(\\psi \\in [0,1]\\) and \\(x\\in \\{0,1\\}\\). Now, the issue that arises is if we have uniform prior belief about \\(\\theta\\), then our prior belief about \\(\\psi=\\theta^2\\) is not uniform. Specifically let \\(\\psi = \\theta^2\\), which has density \\[ \\pi(\\psi) = \\pi(\\theta(\\psi))\\Bigg|\\frac{d\\theta(\\psi)}{d\\psi}\\Bigg| = \\frac{1}{2\\sqrt{\\psi}}, \\] and it suggests that we believe \\(\\psi\\) (and hence \\(\\theta\\)) is more likely to take values that are close to 0 than 1. Therefore, the use of uniform prior is not invariant to reparametrisations and it is to some extend logically flawed. Sir Harold Jeffreys argues that if there are two ways of parameterising a model, e.g. via \\(\\theta\\) and \\(\\psi\\), then the priors on these parameters should be equivalent. In other words, the prior distribution should be invariant under sensible transformations/reparametersation. Definition 3.3 (Jeffreys invariant prior) Given \\(Y\\mid \\theta \\sim \\pi(y\\mid \\theta)\\), Jefferys prior is \\[ \\pi(\\theta) \\propto \\sqrt{I_Y(\\theta)}, \\] where \\(I_Y(\\theta)\\) is the Fisher information for \\(\\theta\\) defined as \\[ I_Y(\\theta) = \\mathrm{Var} \\left[ \\frac{\\partial}{\\partial \\theta} \\log \\pi(Y\\mid \\theta) \\right] = -\\mathbb{E} \\left[ \\frac{\\partial^2}{\\partial \\theta^2} \\log \\pi(Y \\mid \\theta) \\right], \\qquad Y \\sim \\pi(y\\mid\\theta). \\] Theorem 3.1 (invariance) Given \\(Y\\mid \\theta \\sim \\pi(y\\mid \\theta)\\), consider a reparametrisation of the model by \\(Y\\mid \\psi \\sim \\pi(y\\mid \\psi)\\) with \\(\\psi = h(\\theta)\\), for some smooth strictly monotonic function \\(h\\) so that the inverse of \\(h\\) exists and we can write \\(\\theta = h^{-1}(\\psi)\\). Jeffreys prior is invariant to such reparametrisation in the sense that \\[ \\pi(\\psi) = \\pi(\\theta) \\left|\\frac{d\\theta}{d\\psi}\\right| \\propto \\sqrt{I_Y(\\psi)}. \\] Proof. Note that by the usual change of variable (also see the problem sheet), we have \\[ \\pi(\\psi) = \\pi(\\theta) \\left|\\frac{d\\theta}{d\\psi}\\right|. \\] Recall that \\(\\theta\\) should be understood as a function of \\(\\psi\\) in the above formula. We prove the claim by directly computing the Fisher information under the \\(\\psi\\)-parameterisation. \\[\\begin{align*} I_Y(\\psi) &amp;= - \\mathbb{E}\\left(\\frac{d^2\\log \\pi({Y} \\mid \\psi)}{d\\psi^2}\\right) \\\\ &amp;= -\\mathbb{E}\\left(\\frac{d}{d\\psi} \\left( \\frac{d \\log \\pi(Y|\\theta)}{d \\theta} \\frac{d\\theta}{d\\psi} \\right) \\right) \\tag{chain rule}\\\\ &amp;= -\\mathbb{E}\\left(\\left(\\frac{d^2 \\log \\pi(Y|\\theta)}{d \\psi d\\theta}\\right)\\left( \\frac{d\\theta}{d\\psi}\\right) + \\left(\\frac{d \\log \\pi(Y|\\theta)}{d \\theta}\\right) \\left( \\frac{d^2\\theta}{d\\psi^2}\\right) \\right)\\tag{prod. rule} \\\\ &amp;= -\\mathbb{E}\\left(\\left(\\frac{d^2 \\log \\pi(Y|\\theta)}{d \\theta^2 }\\right)\\left( \\frac{d\\theta}{d\\psi}\\right)^2 + \\left(\\frac{d \\log \\pi(Y|\\theta)}{d \\theta}\\right) \\left( \\frac{d^2\\theta}{d\\psi^2}\\right) \\right)\\tag{chain rule} \\\\ &amp; = -\\mathbb{E}\\left(\\left(\\frac{d^2 \\log \\pi({Y} \\mid \\theta)}{d\\theta^2}\\left(\\frac{d\\theta}{d\\psi}\\right)^2\\right)\\right) \\\\ &amp; = I_Y({\\theta})\\left(\\frac{d\\theta}{d\\psi}\\right)^2, \\end{align*}\\] where we use the fact that \\(\\mathbb{E}(\\frac{d \\log \\pi(Y|\\theta)}{d \\theta}) = 0\\) in the penultimate step. Thus \\(\\sqrt{I_Y(\\psi)} = \\sqrt{I_Y(\\theta)} \\left|\\frac{d\\theta}{d\\psi}\\right| \\propto \\pi(\\theta) \\left|\\frac{d\\theta}{d\\psi}\\right|\\), which proves our claim. Example 3.8 In Example 3.2, we modelled the number of bot accounts on a social media website by \\(Y \\mid \\theta \\sim \\textrm{Bin}(n, \\theta)\\). To find Jeffreys prior distribution for \\(\\theta\\), we just need to derive the Fisher information \\(I_Y(\\theta)\\). \\[\\begin{align*} &amp;\\pi(y \\mid \\theta) = \\begin{pmatrix} n \\\\ y \\end{pmatrix} \\theta^y (1-\\theta)^{n-y}\\\\ \\implies &amp;\\log \\pi(y \\mid \\theta) = \\log \\begin{pmatrix} n \\\\ y \\end{pmatrix} + y \\log\\theta + (n-y)\\log(1-\\theta) \\\\ \\implies &amp;\\frac{\\partial \\log \\pi(y \\mid \\theta)}{\\partial \\theta} = \\frac{y}{\\theta} - \\frac{n-y}{1-\\theta} \\\\ \\implies &amp;\\frac{\\partial^2 \\log \\pi(y \\mid \\theta)}{\\partial \\theta^2} = -\\frac{y}{\\theta^2} + \\frac{y-n}{(1-\\theta)^2} \\\\ \\mathbb{E}\\left(\\frac{\\partial^2 \\log \\pi(Y \\mid \\theta)}{\\partial \\theta^2}\\right) &amp;= -\\frac{\\mathbb{E}(Y)}{\\theta^2} + \\frac{\\mathbb{E}(Y)-n}{(1-\\theta)^2}\\\\ &amp; = -\\frac{n\\theta}{\\theta^2} + \\frac{n\\theta-n}{(1-\\theta)^2}\\\\ &amp; = -\\frac{n}{\\theta} - \\frac{n}{1-\\theta}\\\\ &amp; = -\\frac{n}{\\theta(1-\\theta)} \\\\ \\implies &amp;I_Y(\\theta) \\propto \\frac{1}{\\theta(1-\\theta)}. \\end{align*}\\] Hence Jeffreys prior is \\(\\pi(\\theta) \\propto \\theta^{-\\frac{1}{2}}(1-\\theta)^{-\\frac{1}{2}}\\). This functional dependency on \\(\\theta\\) shows that \\(\\theta \\sim \\textrm{Beta}(\\frac{1}{2}, \\frac{1}{2})\\). Example 3.9 (imporper prior) Consider \\(Y \\mid \\mu \\sim N(\\mu,\\sigma^2)\\) with \\(\\sigma&gt;0\\) known and \\(\\mu \\in \\mathbb{R}\\) unknown. We leave it as an exercise to show that \\(I_Y(\\mu) = 1/\\sigma^2\\) and hence the Jefferys invariant prior for \\(\\mu\\) is \\[ \\pi(\\mu) \\propto \\frac{1}{\\sigma} \\propto 1. \\] This is because \\(1/\\sigma\\) does not depend on \\(\\mu\\) and hence it can be considered to be a constant. One thing that we notice is choosing \\(\\pi(\\mu) \\propto 1\\) for \\(\\mu \\in \\mathbb{R}\\) does not lead to a proper probability distribution for \\(\\mu\\), since \\(\\int_{-\\infty}^{\\infty} \\pi(\\mu)d\\mu = \\infty\\). Generally speaking, we call a prior distribution on \\(\\theta \\in \\Theta\\) that \\(\\int_{\\Theta} \\pi(\\theta)d\\theta = \\infty\\), an improper prior. Improper priors are commonly used in Bayesian modelling, as long as the posterior distribution is well-defined, and it often conveys the idea that we do not know much about the parameter \\(\\theta\\). 3.6 Frequentist analysis of Bayesian methods A main difference between frequentist and Bayesian inference is that there is a true (deterministic) parameter \\(\\theta^*\\) that generates the data in the frequentist framework, whereas Bayesian believe \\(\\theta \\sim \\pi(\\theta)\\) is random. Many research in Bayesian inference are computational and algorithmic in nature, e.g. how to sample from a posterior distribution, but one theoretical topic is to study the frequentist properties (e.g. unbiasedess, consistent) of some Bayesian estimators (e.g. posterior mean, MAP, credible intervals). Specifically, one could view Bayesian method as an algorithmic tool to derive estimators and analyse their performance under the assumption that the data are generated from a model with true parameter \\(\\theta^*\\). Example 3.10 Consider \\(X_1,\\dotsc,X_n \\mid \\theta \\overset{i.i.d}\\sim N(\\theta,1)\\) with \\(\\theta \\sim N(0,1)\\). Using the formula for posterior distribution that we derived in Example 3.5, \\[\\theta\\mid X \\sim N\\Big(\\frac{n}{n+1}\\overline{X}_n,\\;\\frac{1}{n+1}\\Big),\\] where \\(\\overline{X}_n = \\sum_{i=1}^nX_i/n\\) is the sample average/MLE for \\(\\mu\\). Suppose we report the posterior mean \\(\\frac{n}{n+1}\\overline{X}_n\\) and, in order to study its theoretical property, we consider the hypothetical setting \\[X_1,\\dotsc,X_n \\overset{i.i.d}{\\sim} N(\\theta^*,1).\\] Under this setting, we have a true parameter \\(\\theta^*\\) and we can discuss the bias, variance and mean squared error of the estimator \\(\\hat{\\theta}_n := \\frac{n}{n+1}\\overline{X}_n\\) with respect to \\(\\theta^*\\), e.g.  \\[ \\text{Bias}(\\hat{\\theta}_n) = \\mathbb{E}(\\hat{\\theta}_n) - \\theta^* = -\\frac{\\theta^*}{n+1}, \\quad \\text{Var}(\\hat{\\theta}_n) = \\frac{n}{(n+1)^2}. \\] One can compare this to that of MLE \\(\\overline{X}_n\\), which has bias \\(0\\) and variance \\(1/n\\). We can also consider the difference between the posterior mean as an estimator and the MLE \\[ \\overline{X}_n - \\hat{\\theta}_n = \\frac{1}{n+1}\\overline{X}_n, \\] which convergences to \\(0\\) in probability, since \\(\\overline{X}_n\\) converges to \\(\\theta^*\\) in probability and \\(1/(n+1)\\) converges to \\(0\\), as \\(n \\rightarrow \\infty\\). Moreover, given that \\(\\sqrt{n}(\\overline{X}_n - \\hat{\\theta}_n)\\) also converges to \\(0\\) in probability (You don’t need to be able to show this), and \\(\\sqrt{n} (\\overline{X}_n - \\theta^*) \\sim N(0, 1)\\), it holds that \\[ \\sqrt{n} (\\hat{\\theta}_n - \\theta^*) \\overset{d}{\\longrightarrow} N(0,1), \\] by Slutsky’s Theorem (non-examinable in this course). With this result, we can consider the frequentist coverage of a credible interval. Recall that the \\(1-\\alpha\\) level credible interval \\(C_n(X)\\) that we obtained in Example 3.5 is \\[ \\hat{\\theta}_n \\pm \\frac{1}{\\sqrt{n+1}}\\Phi^{-1}(1-\\alpha/2). \\] By frequentist coverage, we mean computing \\[\\begin{align*} &amp;\\mathbb{P}_{X_1,\\dotsc,X_n \\overset{i.i.d}\\sim N(\\theta^*,1)}(\\theta^* \\in C_n(X)) \\\\ &amp; = \\mathbb{P}_{X_1,\\dotsc,X_n \\overset{i.i.d}\\sim N(\\theta^*,1)}\\Big(\\hat{\\theta}_n - \\frac{1}{\\sqrt{n+1}}\\Phi^{-1}(1-\\alpha/2) \\leq \\theta^* \\leq \\hat{\\theta}_n + \\frac{1}{\\sqrt{n+1}}\\Phi^{-1}(1-\\alpha/2)\\Big)\\\\ &amp; = \\mathbb{P}_{X_1,\\dotsc,X_n \\overset{i.i.d}\\sim N(\\theta^*,1)}\\Big(|\\hat{\\theta}_n - \\theta^*| \\leq \\frac{1}{\\sqrt{n+1}}\\Phi^{-1}(1-\\alpha/2)\\Big) \\\\ &amp; = \\mathbb{P}_{X_1,\\dotsc,X_n \\overset{i.i.d}\\sim N(\\theta^*,1)}\\left(\\sqrt{\\frac{n+1}{n}} \\sqrt{n}|\\hat{\\theta}_n - \\theta^*|\\leq \\Phi^{-1}(1-\\alpha/2)\\right) \\\\ &amp;\\overset{n \\rightarrow \\infty}\\longrightarrow \\mathbb{P}_{Z\\sim N(0,1)} \\left(|Z| \\leq \\Phi^{-1}(1-\\alpha/2)\\right) = 1-\\alpha, \\end{align*}\\] which shows the credible interval derived using Bayesian method is also a valid confidence interval for \\(\\theta^*\\) at level \\(1-\\alpha\\) if \\(X_1,\\dotsc,X_n \\overset{i.i.d}\\sim N(\\theta^*,1)\\). Theorem 3.2 (Bernstein-von-Mises) Let \\(X_1,\\dotsc,X_n \\overset{i.i.d}\\sim P_{\\theta^*}, \\theta^*\\in \\Theta\\subseteq \\mathbb{R}\\). Let \\(\\hat{\\phi}_n(\\theta)\\) denote the (random) density function of \\(N\\Big(\\hat{\\theta}_{\\mathrm{MLE}},(nI(\\theta^*))^{-1}\\Big)\\). Under relatively mild regularity conditions including the prior distribution for \\(\\theta\\) is non-zero around the MLE \\(\\hat{\\theta}\\), then \\[ \\frac{1}{2}\\int \\Big|\\pi(\\theta \\mid X) - \\hat{\\phi}_n(\\theta)\\Big|d\\theta \\; \\overset{a.s.}\\longrightarrow 0. \\] as \\(n \\rightarrow \\infty\\). Technical details of this theorem can be found in Chapter 10 of Asymptotic Statistics (2000) by A. W. van der Vaart. The Berstein-von-Mises theorem says that as the number of data points approaches infinity \\((n \\rightarrow \\infty)\\), the posterior distribution is approximately normal \\[ \\theta\\mid X \\sim N\\Big(\\hat{\\theta}_{\\mathrm{MLE}},(nI(\\theta^*))^{-1}\\Big). \\] In particular, when \\(\\theta \\in \\mathbb{R}\\), an approximate (\\(1-\\alpha\\)) level credible interval can be constructed as \\[ C_n(X) = \\hat{\\theta}_{\\mathrm{MLE}} \\pm \\frac{1}{\\sqrt{nI(\\hat{\\theta}_{\\mathrm{MLE}})}}\\Phi^{-1}(1-\\alpha/2), \\] which is the same as the confidence interval that one would obtain using asymptotic properties of the MLE, and hence also has the frequentist coverage \\(\\mathbb{P}(\\theta^* \\in C_n(X)) \\rightarrow 1-\\alpha\\) as in the example we discussed above. 3.7 Hierarchical Models In many modelling problems, there will be multiple parameters each related to one another. These parameters may be directly related to the model, or they may be parameters we introduce through prior distributions. We can form a hierarchy of these parameters, from closest to further from the data, to construct our model. Example 3.11 Let’s consider Example 3.4 again. We have some data \\(\\boldsymbol{y} = (y_1,\\dotsc,y_n)\\) that are assumed to have been generated from an Exponential distribution with rate parameter \\(\\lambda\\). We place an Exponential prior distribution with rate \\(\\gamma\\) on \\(\\lambda\\) and the posterior distribution was \\(\\lambda \\mid \\boldsymbol{y} \\sim \\textrm{Gamma}(n+1, \\sum_{i=1}^ny_i + \\gamma)\\). In that example, we discussed how the choice of \\(\\gamma\\) can affect the posterior distribution and conclusions presented to the company. One option is to place a prior distribution on \\(\\gamma\\), known as a hyperprior distribution, and form a hierarchy as \\[\\begin{align*} Y_1,\\dotsc,Y_n \\mid \\lambda &amp;\\overset{i.i.d.}\\sim \\hbox{Exp}(\\lambda) &amp; \\textrm{(likelihood)} \\\\ \\lambda \\mid \\gamma &amp;\\sim \\hbox{Exp}(\\gamma) &amp; \\textrm{(prior distribution)} \\\\ \\gamma &amp;\\sim \\hbox{Exp}(\\nu) &amp; \\textrm{(hyperprior distribution)}. \\\\ \\end{align*}\\] This hierarchy can be represented as \\(\\gamma \\rightarrow \\lambda \\rightarrow \\{Y_i\\}_{i=1,\\dotsc,n}\\). We can write the posterior distribution for \\(\\lambda\\) and \\(\\gamma\\) given the data \\(\\boldsymbol{y} = (y_1,\\dotsc,y_n)\\) as \\[\\begin{align*} \\pi(\\lambda, \\gamma \\mid \\boldsymbol{y}) \\propto \\pi(\\lambda, \\gamma, \\boldsymbol{y}) \\propto \\pi(\\boldsymbol{y} \\mid \\lambda)\\pi(\\lambda \\mid \\gamma)\\pi(\\gamma) = \\lambda^{n}\\gamma e^{-\\lambda(\\sum_{i=1}^n y_i + \\gamma)} \\nu e^{-\\nu\\gamma} \\propto \\lambda^{n}\\gamma e^{-\\lambda(\\sum_{i=1}^n y_i + \\gamma)} e^{-\\nu\\gamma}. \\end{align*}\\] This is a joint distribution of \\(\\lambda\\) and \\(\\gamma\\) on \\(\\mathbb{R}^2\\), more specifically on \\((0,\\infty) \\times (0,\\infty)\\) since the rate parameters are positive. We can also use it to find two conditional density \\(\\pi(\\lambda \\mid \\boldsymbol{y}, \\,\\gamma)\\) and \\(\\pi(\\gamma \\mid \\boldsymbol{y}, \\,\\lambda)\\). Recall that \\[ \\pi(\\lambda, \\gamma \\mid \\boldsymbol{y}) = \\pi(\\lambda \\mid \\boldsymbol{y}, \\,\\gamma) \\pi(\\gamma \\mid \\boldsymbol{y}) = \\pi(\\gamma \\mid \\boldsymbol{y}, \\,\\lambda)\\pi(\\lambda \\mid \\boldsymbol{y}). \\] Therefore, in order to find \\(\\pi(\\lambda \\mid \\boldsymbol{y}, \\,\\gamma)\\), we just need to collect the terms in \\(\\pi(\\lambda, \\gamma \\mid \\boldsymbol{y})\\) that only depends on \\(\\lambda\\), i.e.  \\[ \\pi(\\lambda \\mid \\boldsymbol{y}, \\,\\gamma) \\propto \\lambda^{n} e^{-\\lambda(\\sum_{i=1}^n y_i + \\gamma)}, \\] which is the density of \\(\\text{Gamma}(n+1,\\sum_{i=1}^n y_i + \\gamma)\\), the same as the posterior distribution. Similarly, collecting all the terms that depend on \\(\\gamma\\) from \\(\\pi(\\lambda, \\gamma \\mid \\boldsymbol{y})\\) , we obtain \\[ \\pi(\\gamma \\mid \\boldsymbol{y}, \\,\\lambda) \\propto \\gamma e^{-(\\lambda+\\nu)\\gamma}, \\] which is the density function of \\(\\text{Gamma}(2,\\lambda+\\nu)\\). In the next chapter, we will look at how to sample from these distributions, but more importantly, these conditional densities play a crucial role in Gibbs sampling - a particular type of MCMC algorithm. 3.8 Lab The aim of this lab is to work with some posterior distributions in cases when the prior distribution is or is not conjugate. Recall the definition of a conjugate prior distribution: If the prior distribution \\(\\pi(\\theta)\\) has the same distributional family as the posterior distribution \\(\\pi(\\theta \\mid \\boldsymbol{y})\\), then the prior distribution is a conjugate prior distribution. Working with conjugate prior distributions often makes the analytical work much easier, as we can work with the posterior distribution. But sometimes, conjugate prior distributions may not be appropriate. This is where R can help, as we do not need a closed form to carry out computations. Example 3.12 The total number of goals scored in 50 games of a low level football league is shown below. y &lt;- c(2, 6, 2, 3, 4, 3, 4, 3, 1, 2, 3, 2, 6, 6, 2, 3, 5, 1, 2, 2, 4, 2, 5, 3, 6, 4, 1, 2, 7, 8, 4, 3, 7, 3, 3, 5, 2, 6, 1, 3, 7, 4, 2, 6, 8, 8, 4, 5, 7, 4) hist(y, main = &quot;&quot;, xlab = &quot;Number of goals scored&quot;) mean(y) ## [1] 3.92 We can model the number of goals scored using a Poisson distribution \\[ Y_1,\\dotsc,Y_n \\mid \\lambda \\overset{i.i.d}{\\sim} \\hbox{Poisson}(\\lambda). \\] By Bayes’ theorem, the posterior distribution is given by \\[ \\pi(\\lambda \\mid \\boldsymbol{y}) \\propto \\pi(\\boldsymbol{y} \\mid \\lambda)\\pi(\\lambda). \\] The likelihood function is given by \\[\\begin{align*} \\pi(\\boldsymbol{y} \\mid \\lambda) &amp;= \\prod_{i=1}^{50} \\frac{e^{-\\lambda}\\lambda^{y_i}}{y_i!}\\\\ &amp;= \\frac{e^{-50\\lambda}\\lambda^{\\sum y_i}}{\\prod_{i=1}^{50} y_i!} \\end{align*}\\] R has a set of inbuilt functions for working with the Poisson distribution so we can rely on those to write functions for the likelihood and loglikelihood. lambda &lt;- seq(0, 10, 0.01) #grid of lambda values likelihood.function &lt;- function(lambda, y) prod(dpois(y, lambda)) #compute likelihood log.likelihood.function &lt;- function(lambda, y) sum(dpois(y, lambda, log = TRUE)) #compute loglikelihood likelihood &lt;- sapply(lambda, likelihood.function, y) #evaluate at grid of points log.likelihood &lt;- sapply(lambda, log.likelihood.function, y) #evaluate at grid of points #Plot likelihood plot(lambda, likelihood, xlab = expression(lambda), ylab = &quot;likelihood&quot;, type = &#39;l&#39;) plot(lambda, log.likelihood, xlab = expression(lambda), ylab = &quot;loglikelihood&quot;, type = &#39;l&#39;) We sometimes work on the log scale because the numbers can be smaller that R can deal with. The denominator with the factorial can get very large very quickly. After speaking to football experts, we decide to place a normal prior distribution on \\(\\lambda\\) with mean 5 goals and standard deviation one goal, i.e. \\[ \\lambda \\sim N(5, 1). \\] The prior distribution can be plotted by lambda &lt;- seq(0, 10, 0.01) #grid of lambda values prior &lt;- dnorm(lambda, 5, 1) log.prior &lt;- dnorm(lambda, 5, 1, log = TRUE) plot(lambda, prior, type = &#39;l&#39;, xlab = expression(lambda), ylab = &quot;density&quot;) plot(lambda, log.prior, type = &#39;l&#39;, xlab = expression(lambda), ylab = &quot;log density&quot;) Writing the posterior distribution up to proportionality, we get \\[ \\pi(\\lambda \\mid \\boldsymbol{y}) \\propto \\exp\\left(-50\\lambda -\\frac{1}{2}(\\lambda - 5)^2\\right)\\lambda^{\\sum y_i}. \\] There is no closed form for this distribution and it is not that nice to work with. But with R, we can evaluate the posterior distribution at a grid of points with an approximation of the normalizing constant via the Trapezoidal rule. The Trapezoidal rule approximates the integral of a function \\(f(x)\\) over an interval \\([a,b]\\) using a grid of points with spacing \\(h\\) \\[ \\int_a^b f(x) dx \\approx \\frac{h}{2} \\Big(f(x_0)+f(x_n)+2\\sum_{i=1}^{n-1}f(x_i)\\Big), \\] where \\(x_0 = a\\), \\(x_n = b\\) and \\(x_i-x_{i-1} = h\\) for \\(i = 1,\\dotsc,n\\). posterior &lt;- prior*likelihood integrating.factor &lt;- 0.5*0.01*(posterior[1] + posterior[1001] + 2*sum(posterior[-c(1, 1001)])) #Using Trapezoidal rule posterior &lt;- posterior/integrating.factor #normalise plot(lambda, posterior, type = &#39;l&#39;, xlab = expression(lambda), ylab = &quot;posterior density&quot;) We can now visually inspect the posterior distribution and see that it has a strong peak around 4. One important statistic is the maximum a posteriori estimation or MAP estimate, this is the mode of the posterior distribution and it is a similar principle to the maximum likelihood estimate. We can compute this using the command lambda[which.max(posterior)] ## [1] 4 which shows the MAP estimate is exactly 4. Exercise 3.1 Adapt the code in the Example above to use an exponential prior distribution with rate 0.1. Analytically show that the posterior distribution is \\(\\text{Gamma}(\\sum_{i=1}^n y_i+1, n+0.1)\\), where \\(n = 50\\). Plot the posterior distribution for \\(\\lambda \\in [0,10]\\) compare it to the approximation obtained using Trapezoidal rule. Exercise 3.2 You are given that the data are exponentially distributed with rate \\(\\lambda,\\) i.e. \\(Y_1, \\ldots, Y_N \\mid \\lambda \\overset{i.i.d.}\\sim \\hbox{Exp}(\\lambda)\\). Your prior belief is that \\(\\lambda \\in (0, 1)\\). Show that if the prior distribution is \\(\\lambda \\sim \\hbox{Beta}(\\alpha, \\beta)\\), then the posterior is \\[ \\pi(\\lambda \\mid \\boldsymbol{y}) \\propto \\lambda^{N + \\alpha - 1}(1-\\lambda)^{\\beta - 1}\\exp{\\Big(-\\lambda\\sum_{i=1}^Ny_i\\Big)}, \\quad \\lambda \\in (0,1), \\] which does not have a simple closed form expression. Suppose that the data is given by y &lt;- c(1.95, 1.46, 4.81, 1.52, 4.24, 3.00, 0.46, 2.27, 1.76, 0.41) On a grid of points for \\(\\lambda \\in (0,1)\\), evaluate the likelihood function, log-likelihood function, prior density (with \\(\\alpha = \\beta = 1\\)), and the posterior density (with Trapezoidal rule approximation). Plot the posterior distribution. Exercise 3.3 Suppose you have \\(X_1, ..., X_N \\mid p \\overset{i.i.d.}\\sim \\hbox{Binomial}(100, p)\\). Using \\(p \\sim \\hbox{Beta}(\\alpha, \\beta)\\) as the prior distribution, derive the posterior distribution and the posterior mean (Wikipedia is a helpful place for properties of distributions). (Large data scenario) Fix \\(\\alpha = 2\\), \\(N = 150\\) and \\(\\sum_{i=1}^N x_i = 2971\\). Plot the prior and posterior distributions on the same figure for different values of \\(\\beta\\). Plot the posterior mean against \\(\\beta \\in (0, 10)\\). Plot the prior mean against the posterior mean for \\(\\beta \\in (0, 10)\\). (Small data scenario) Fix \\(\\alpha = 2\\), \\(N = 5\\) and \\(\\sum_{i=1}^N x_i = 101\\). Plot the prior and posterior distributions on the same figure for different values of \\(\\beta\\). Plot the posterior mean against \\(\\beta \\in (0, 10)\\). Plot the prior mean against the posterior mean for \\(\\beta \\in (0, 10)\\). "],["sampling.html", "Chapter 4 Sampling 4.1 Uniform Random Numbers 4.2 Inverse Transform Sampling 4.3 Rejection Sampling 4.4 Ziggurat Sampling 4.5 Approximate Bayesian Computation 4.6 Lab", " Chapter 4 Sampling 4.1 Uniform Random Numbers What we won’t be doing in this module is generating true uniform random numbers. This is incredibly difficult and usually requires lots of expensive hardware. This is because computers aren’t good at being random, they require algorithmic instructions. True random number generation often uses physical methods, such as the radioactive decay of atoms, or atmospheric noise. Throughout this module, we will be using R’s built in random number generation. This is a pseudo random number generator that has excellent random properties, but will eventually repeat. A basic random number generation tool that we will repeatedly use in the module involves sampling from a uniform distribution on the unit interval, which can be done in R using runif(1, 0, 1) ## [1] 0.5207633 4.2 Inverse Transform Sampling Suppose we want to sample from a non-uniform one-dimensional distribution. The inverse transform theorem allows us to do this using the distribution’s inverse function. Definition 4.1 The generalised inverse \\(F^{-1}\\colon [0,1] \\to [-\\infty,\\infty]\\) of a cumulative distribution function \\(F\\), is defined for all \\(u \\in [0, 1]\\) by \\[ F^{-1}(u) = \\inf\\{x \\in\\mathbb{R} : F(x) \\geq u\\}, \\] where we use the convention \\(\\inf \\varnothing = +\\infty\\). This definiton entails that \\(F^{-1}\\) is left-continuous (while the quantile function \\(q(u) = \\inf\\{x \\in \\mathbb{R}: F(x) &gt; u\\}\\) is right-continuous). The reason for us to not demand a strict inequality for the generalised inverse is that our definition provides \\[\\begin{equation} \\tag{4.1} \\forall x \\in \\mathbb{R}, u \\in [0,1]: u \\leq F(x) \\iff F^{-1}(u) \\leq x \\end{equation}\\] by right-continuity of \\(F\\). We also note that if \\(F\\) is bijective, its inverse function is identical to the generalised inverse on \\((0,1]\\). Theorem 4.1 Let \\(F\\colon \\mathbb{R} \\rightarrow [0, 1]\\) be a distribution function, \\(U \\sim U[0, 1]\\) and \\(Y = F^{-1}(U)\\). Then \\(Y\\) has distribution function \\(F\\). Proof. From (4.1) we obtain \\[ \\mathbb{P}(Y \\leq x) = \\mathbb{P}(F^{-1}(U) \\leq x) = \\mathbb{P}(U \\leq F(x)) = F(x), \\] where the last equality follows from \\(U \\sim U[0, 1]\\). This theorem says that if we have a random variable \\(U \\sim U[0, 1]\\) and we want to get \\(Y \\sim F\\), then we can use \\(F^{-1}(U)\\). Viewing this theorem graphically can provide a much more intuitive understanding. Example 4.1 We would like to sample from an exponential distribution with rate \\(\\lambda\\), i.e. \\(Y ~ \\sim \\hbox{Exp}(\\lambda)\\). The density function is given by \\[ \\pi(y \\mid \\lambda) = \\begin{cases} \\lambda e^{-\\lambda y} &amp; y \\geq 0 \\\\ 0 &amp; \\text{otherwise.} \\end{cases} \\] The distribution function for \\(y \\geq 0\\) is given by \\[\\begin{align*} F(y \\mid \\lambda) &amp;= \\int_0^y \\lambda e^{-\\lambda t}\\,dt \\\\ &amp; = 1 - e^{-\\lambda y}. \\end{align*}\\] Finally, the inverse function on \\((0,1)\\) is given by \\[ F^{-1}(y \\mid \\lambda) = -\\frac{1}{\\lambda}\\log(1-y). \\] Therefore, if \\(U \\sim U[0, 1]\\), then it follows that \\(-\\frac{1}{\\lambda}\\log(1-U) \\sim \\hbox{Exp}(\\lambda)\\). The R code below generates a plot to show this (with \\(\\lambda = 0.5\\)). We can plot the CDF for most one parameter distributions straightforwardly. We can think of this theorem as allowing us to sample a point on the y-axis and then computing the quantile this corresponds to. set.seed(12345) # to reproduce y &lt;- seq(0, 10, 0.01) #Show on the interval [0, 5] f &lt;- 1 - exp(-0.5*y) #Construct the cumulative density #function (CDF) plot(y, f, type =&#39;l&#39;, xlab = &quot;y&quot;, ylab= &quot;CDF&quot;) #Sample u u &lt;- runif(1) #Get the corresponding y value f.inv &lt;- -2*log(1-u) #plot segments(x0 = 0, y0 = u, x1 = f.inv, y1 = u, lty = 2) segments(x0 = f.inv, y0 = 0, x1 = f.inv, y1 = u, lty = 2) text(x = f.inv, y = -0.01, expression(F[-1](U)), col = 4) text(x = -.1, y = u, &quot;U&quot;, col = 4) Example 4.2 Suppose we want to generate samples from the Cauchy distribution with location 0 and scale 1. This has density function \\[ \\pi(x) = \\frac{1}{\\pi(1+x^2)}, \\quad x \\in \\mathbb{R}. \\] A plot of this function is shown below. x &lt;- seq(-5, 5, 0.01) y &lt;- 1/(pi*(1 + x^2)) plot(x, y, type = &#39;l&#39;) To use the inverse transform method, we first need to find the CDF: \\[ F(x) = \\int_{-\\infty}^x \\frac{1}{\\pi(1+t^2)}dt \\] Letting \\(t = \\tan \\theta\\), we can write \\(dt = \\sec^2(\\theta)d\\theta\\). The integral becomes \\[ F(x) = \\int_{-\\frac{\\pi}{2}}^{\\arctan(x)} \\frac{\\sec^2(\\theta)}{\\pi(1+\\tan^2(\\theta))} d\\theta \\] As \\(1 + \\tan^2(\\theta) = \\sec^2(\\theta)\\), we can write the integral as \\[ F(x) = \\int_{-\\frac{\\pi}{2}}^{\\arctan(x)} \\frac{1}{\\pi}du\\\\ = \\left[\\frac{\\theta}{\\pi}\\right]_{-\\frac{\\pi}{2}}^{\\arctan(x)}\\\\ = \\frac{\\arctan(x)}{\\pi} + \\frac{1}{2}. \\] The inverse of the distribution function is \\[ F^{-1}(x) = \\tan\\left(\\pi\\left(x - \\frac{1}{2}\\right)\\right). \\] Hence, if \\(U \\sim U[0, 1]\\), then \\(\\tan\\left(\\pi\\left(U - \\frac{1}{2}\\right)\\right) \\sim \\hbox{Cauchy}(0, 1)\\). 4.3 Rejection Sampling We now have a way of sampling realisations from distributions where we can analytically derive the inverse distribution function. We can use this to sample from more complex densities, or simple densities more efficiently. Rejection sampling works by sampling according to a density we can sample from and then rejecting or accepting that sample based on the density we’re actually interested in. The plot below shows an example from this. We would like to generate a sample from the distribution with the curved density function, which is challenging. Instead, we find a distribution whose density function \\(q\\) multiplied by some \\(c \\geq 1\\) (call \\(cq\\) an unnormalised density function) bounds the one we wish to sample from. In this case we can use the uniform distribution. Once we have generated our sample from the uniform distribution, we choose to accept or reject it based on the distribution we are interested in. In this case we reject it. Suppose we want to sample from a density \\(\\pi\\), but can only generate samples from a density \\(q\\). If there exists some constant \\(c &gt; 0\\), such that \\(\\frac{\\pi(y)}{q(y)} \\leq c\\) for all \\(y\\), then we can generate samples from \\(\\pi\\) by the following: Sample \\(y \\sim q\\) Sample independently \\(u \\sim U[0, 1]\\) Compute \\(k = \\frac{\\pi(y)}{cq(y)}\\) Accept \\(y\\) if \\(u \\leq k\\). Reject otherwise and repeat. Upon rejection we then go back to step 1 until a sample is accepted. This says draw sample a point \\(y\\) according to the density \\(q\\). Draw a vertical line at \\(y\\) from the \\(x\\)-axis to \\(cq(y)\\). Sample uniformly on this line. If the uniformly random sample is below \\(q\\), then accept it. Otherwise, reject it. Theorem 4.2 Let \\(Y \\sim q\\), \\(U \\sim U[0,1]\\) be independent of \\(Y\\) and suppose that that for some \\(c \\geq 1\\) we have \\(\\pi \\leq cq\\). Then, the acceptance probability of the rejection algorithm is given by \\[\\mathbb{P}\\Big(U \\leq \\frac{\\pi(Y)}{cq(Y)} \\Big) = \\frac{1}{c}, \\] and it holds that \\[\\mathbb{P}\\Big(Y \\in dy \\mid U \\leq \\frac{\\pi(Y)}{cq(Y)}\\Big) = \\pi(y) \\, dy.\\] Proof. Let \\(x \\in \\mathbb{R}\\) be arbitrarily chosen. Since CDFs uniquely characterise distributions, for the second claim it is sufficient to show that \\[\\mathbb{P}\\Big(Y \\leq x \\mid U \\leq \\frac{\\pi(Y)}{cq(Y)}\\Big) = \\int_{-\\infty}^x \\pi(y) \\, dy. \\] By using independence of \\(Y\\) and \\(U\\) and Fubini we can calculate as follows: \\[\\begin{align*} \\mathbb{P}\\Big(Y \\leq x, U \\leq \\frac{\\pi(Y)}{cq(Y)}\\Big) &amp;= \\int_{\\mathbb{R}^2} \\mathbf{1}_{\\{y \\leq x, u \\leq \\pi(y)/(cq(y))\\}} \\, \\mathbb{P}_U(du)\\,\\mathbb{P}_Y(dy)\\\\ &amp;= \\int_{-\\infty}^x \\int_0^{\\frac{\\pi(y)}{cq(y)}} \\,du \\, q(y)\\, dy\\\\ &amp;= \\int_{-\\infty}^x \\frac{\\pi(y)}{cq(y)} q(y) \\, dy\\\\ &amp;= \\frac{1}{c} \\int_{-\\infty}^x \\pi(y) \\, dy. \\end{align*}\\] Note that in the second line we used the assumption \\(\\pi \\leq cq\\), which ensures that \\(\\pi(y)/(cq(y)) \\in [0,1]\\). Letting \\(x \\to \\infty\\) we therefore see that \\[\\mathbb{P}\\Big(U \\leq \\frac{\\pi(Y)}{cq(Y)} \\Big) = \\lim_{x \\to \\infty} \\mathbb{P}\\Big(Y \\leq x, U \\leq \\frac{\\pi(Y)}{cq(Y)}\\Big) = \\frac{1}{c} \\int_{\\mathbb{R}} \\pi(y) dy = \\frac{1}{c}, \\] proving the claim on the acceptance probability. Finally, the previous two calculations yield \\[\\mathbb{P}\\Big(Y \\in dy \\mid U \\leq \\frac{\\pi(Y)}{cq(Y)}\\Big) = \\frac{\\mathbb{P}\\Big(Y \\leq x, U \\leq \\frac{\\pi(Y)}{cq(Y)}\\Big)}{\\mathbb{P}\\Big(U \\leq \\frac{\\pi(Y)}{cq(Y)} \\Big)} = \\int_{-\\infty}^x \\pi(y) \\, dy. \\] Example 4.3 Suppose we want to sample from a distribution that has the density \\[ \\pi(y) = \\begin{cases} \\frac{3}{4}y(2-y), \\qquad y \\in [0, 2] \\\\ 0, \\qquad \\textrm{otherwise} \\end{cases}. \\] The maximum of \\(\\pi\\) is given by \\(\\frac{3}{4}\\). We choose \\(q \\sim U[0, 2] \\sim 2U\\) for \\(U \\sim U[0,1]\\) and for illustrative purposes not choose \\(c\\) optimally (which would be \\(c = \\frac{3}{2}\\) to maximise the acceptance probability) but \\(c = 3\\). The R code below shows a pictorial version of how one sample is generated. set.seed(1234) #to reproduce scaling.c &lt;- 3 #set c y &lt;- runif(1, 0, 2) #sample Y ~ Q p &lt;- 3/4*y*(2-y) #compute pi(y) k &lt;- p/(scaling.c*1/2) #compute k u &lt;- runif(1) #sample U ~ U[0, 1] ifelse(u &lt; k, &#39;accept&#39;, &#39;reject&#39;) #Accept if u &lt; k ## [1] &quot;reject&quot; #Create nice plot a &lt;- seq(0, 2, 0.01) b &lt;- 3/4*a*(2-a) scaling.c &lt;- scaling.c*rep(1, length(a)) plot(a, b, ylim = c(0, 3/2), type = &#39;l&#39;) lines(a, scaling.c/2) segments(x0 = y, y0 = 0, x1 = y, y1 =3/4*y*(2-y) , lty = 2, lwd = 2) segments(x0 = y, y0 =3/4*y*(2-y), x1 = y, y1 = 3/2, lty = 2, col = 2, lwd = 2) points(x = y, y = u, pch = 19) The plot also shows how the choices of \\(c\\) and \\(q\\) can make the sampling more or less efficient. In our example, the rejection space is large, meaning many of our proposed samples will be rejected. Here, we could have chosen a better \\(q\\) or a better \\(c\\) for the given \\(q\\) to minimise this space (compare with the first figure from this subsection). Example 4.4 Suppose we want to sample from a Beta(4, 8) distribution. This distribution looks like x &lt;- seq(0, 1, 0.001) y &lt;- dbeta(x, 4, 8) plot(x, y, type = &#39;l&#39;) A uniform distribution on [0, 1] will cover the Beta distribution and we can then use a rejection sampling algorithm. First, we need to find \\(c\\), the maximum of the Beta distribution. We can find this by differentiating the pdf and setting it equal to 0: \\[ \\pi(x) = \\frac{1}{B(4, 8)}x^3(1-x)^7 \\\\ \\implies \\frac{d \\pi(x)}{d x} = \\frac{1}{B(4, 8)}(3x^2(1-x)^7 - 7x^3(1-x)^6) \\\\ \\implies \\frac{d \\pi(x)}{d x} = \\frac{1}{B(4, 8)}(x^2(1-x)^6(3-10x)). \\] Setting this equal to 0 gives us the maximum at \\(x = \\frac{3}{10}\\). This means we can set \\(c = \\pi(3/10) = \\frac{1}{B(4, 8)} \\frac{3^3 7^{7}}{10^{10}} = \\frac{11! 3^3 7^7}{3! 7! 10^{10}} \\approx 2.935\\). Our rejection sampling algorithm is therefore Sample \\(u \\sim U[0, 1]\\) Compute \\(k = \\pi(u)/c\\). Accept \\(u\\) with probability \\(k\\). If rejected, repeat. Example 4.5 In this example, we want to sample from a Gamma(3, 2) distribution. The density function is shown below. x &lt;- seq(0, 6, 0.01) y &lt;- dgamma(x, 3, 2) plot(x, y, type = &#39;l&#39;) We will use an Exp(1) distribution as our proposal distribution. To find the value of \\(c\\), consider \\(R(x) = \\frac{\\pi(x)}{q(x)}\\) \\[ R(x) = \\frac{\\frac{2^3}{\\Gamma(3)}x^2\\exp(-2x)}{\\exp(-x)} \\\\ = \\frac{2^3}{\\Gamma(3)}x^2 \\exp(-x) \\] To find the maximum of this ratio, we differentiate and set the result equal to 0. \\[ \\frac{dR(x)}{dx} = 2x\\exp(-x) - x^2\\exp(-x)\\\\ = x\\exp(-x)(2 - x) \\] The maximum is therefore at 2. The value of the ratio at \\(x = 2\\) is \\(R(2) = \\frac{2^3}{\\Gamma(3)}4\\exp(-2)\\). This is therefore our value of c. We can see how \\(\\pi(x)\\) and \\(cq(x)\\) look in the graph below. x &lt;- seq(0, 6, 0.01) y &lt;- dgamma(x, 3, 2) scaling.c &lt;- 2^3/gamma(3)*4*exp(-2) q &lt;- dexp(x, 1) plot(x, y, type = &#39;l&#39;) lines(x, q*scaling.c, col = 2, lty = 2) The exponential distribution completely encloses the gamma distribution using this value of \\(c\\), wiht the two densities just touching at \\(x=2\\). Our rejection sampling algorithm is therefore Sample \\(y \\sim \\hbox{exp}(1)\\) Compute \\(k = \\frac{\\pi(y)}{cq(y)}\\) Accept \\(y\\) with probability \\(k\\); else reject and repeat. 4.3.1 Rejection Sampling Efficiency Suppose we are using a rejection sampling algorithm to sample from \\(\\pi(y)\\) using the proposal distribution \\(q(y)\\). How good is our rejection sampling algorithm? What does it mean to be a ‘good’ rejection sampling algorithm. One measure of the efficiency of a sampler is, on average, how many samples do we need to generate until one is accepted. Proposition 4.1 The number of samples proposed in a rejection sampling algorithm before one is accepted is distributed geometrically with success probability \\(\\frac{1}{c}\\). Proof. Since we sample independently until first success with success probability (i.e., acceptance probability) \\(p\\), the number of samples proposed until acceptance follows a \\(\\mathrm{Geo}(p)\\) distribution. As we saw in Theorem 4.2, we have \\(p = 1/c\\), which proves the assertion. Do drive the point home, let’s redo the calculation for this explicitly: \\[\\begin{align*} \\mathbb{P}\\Big(U \\leq \\frac{\\pi(Y)}{cq(Y)}\\Big) &amp;= \\int_{\\mathbb{R}^2} \\mathbf{1}_{\\{u \\leq \\pi(y)/(cq(y))\\}} \\, \\mathbb{P}_U(du) \\, \\mathbb{P}_Y(dy)\\\\ &amp;= \\int_{\\mathbb{R}} \\int_0^{\\pi(y)/cq(y)} \\, du \\, \\mathbb{P}_Y(dy)\\\\ &amp;= \\int_{\\mathbb{R}} \\frac{\\pi(y)}{cq(y)} q(y) \\, dy\\\\ &amp;= \\frac{1}{c} \\int_{\\mathbb{R}} \\pi(y) \\, dy = \\frac{1}{c}. \\end{align*}\\] Remark. In other words, the average number of samples produced until the algorithm terminates is given by the constant \\(c\\). Thus, the tighter we fit the proposal density \\(q\\) to \\(\\pi\\), the more efficient our rejection algorithm will be. 4.4 Ziggurat Sampling The final method we are going to look at in this chapter is Ziggurat sampling. It is generally used to sample from distributions supported on the positive half line with monotone densities, using only uniform samples. It may also be used for sampling from symmetric distributions via a sign flipping argument, as illustrated for a normal distribution below. Denote its density by \\(\\pi\\). A Ziggurat is a kind of stepped pyramid. To start the sampling algorithm, we approximate the normal distribution on the positive half line by a series of horizontal rectangles numbered in decreasing order from \\(n-1\\) to \\(0\\) with bottom right corner being denoted by \\((x_i,y_i) = (x_i, \\pi(x_i))\\), where \\(x_0 = x_1\\). We also set \\(x_n = 0, y_n = \\pi(0)\\). For the base layer \\(0\\) we then add the tail extending to the right of the \\(0\\)-th rectangle, thus obtaining \\(n-1\\) rectangles stacked upon the base layer. Moreover, the layers are chosen in such a way that they all have the same area \\(A\\) (this can be achieved via a numerical root finding algorithm). An example of this is shown below. To generate a sample, we choose a layer at random – we might do this by sampling on the \\(y\\)-axis uniformly at random. Assume first that we have sampled a layer \\(i \\in \\{1,\\ldots,n\\}\\), i.e., a true rectangle. We then sample uniformly at random within the rectangle and accept the sample if it is in the subgraph of the half-normal distribution. This is is done efficiently as follows: First sample uniformly the \\(x\\)-value of the sample in the box. If this value is smaller than \\(x_{i+1}\\), then sampling uniformly in \\([y_i,y_{i+1}]\\) will always return a sample in the subgraph and we can therefore terminate the algorithm and accept \\(x\\). Otherwise we need to sample the \\(y\\)-value as well and check if \\(y &lt; \\pi(x)\\), in which case we accept, or else reject and select a new layer. If the base layer is sampled things are a bit more complicated as we may need to sample uniformly from the tail. There are methods to do this efficiently for the half-normal distribution. We will not go into details on this and just note that for a large number of layers (a typical choice is \\(n=256\\)) this case occurs very rarely and just ignoring this layer would result in a good, albeit compactly. supported, sample approximation. The full Ziggurat algorithm is now Sample a layer \\(i \\in \\{0,\\ldots,n-1\\}\\) uniformly at random If \\(i &gt; 0\\), do: Sample \\(u_0 \\sim U[0, 1]\\) and set \\(x = u_0x_i\\) If \\(x &lt; x_{i+1}\\): accept the sample and terminate; else: Sample \\(u_1 \\sim U[0, 1]\\) and set \\(y = y_i + u_1(y_{i+1} − y_i)\\) If \\(y &lt; \\pi(x)\\): accept the sample. Else: return to Step 1. Else Sample \\(u_2 \\sim U[0,1]\\) and set \\(w = u_2 A\\) If \\(w &lt; x_0y_1\\): sample and accept \\(x \\sim U[0,x_0]\\) Else: sample from the tail Finally, to generate samples from the full normal distribution, we multiply each sample by -1 with probability \\(1/2\\). 4.5 Approximate Bayesian Computation So far, we have always considered models where the likelihood function is easy to work with. By easy, we mean that we can evaluate the likelihood function for lots of different values, and we can evaluate it cheaply. In some cases, it might not be possible to write down the likelihood function, or it might not be possible to evaluate it. In these cases, we refer to methods call likelihood free inference. Example 4.6 Models to predict the weather are notoriously complex. They contain a huge number of parameters, and sometimes it is not possible to write this model down exactly. In cases where the likelihood function for the weather model can be written down, we would have to start the MCMC algorithm from scratch every time we collected new data. Approximate Bayesian Computation (ABC) is a likelihood free algorithm that relies on reject sampling from the prior distribution. When constructing an ABC algorithm, we only need to be able to generate data given a parameter value and not evaluate the likelihood of seeing specific data given a parameter value. We are going to look at two types of ABC. The first is ABC with rejection 4.5.1 ABC with Rejection Definition 4.2 To carry out inference for a parameter \\(\\theta\\) using an Approximate Bayesian Computation algorithm with rejection Sample a value for the parameter \\(\\theta^*\\) from the prior distribution \\(\\pi(\\theta)\\). Generate some data \\(y*\\) from the data generating process using the parameter value \\(\\theta^*\\). Accept \\(\\theta^*\\) as a value from the posterior distribution if \\(||y - y^*|| &lt; \\varepsilon\\) for some \\(\\varepsilon &gt; 0\\). Otherwise reject \\(\\theta^*\\) Repeat steps 1 - 3. Definition 4.3 The approximate posterior distribution using ABC with rejection is \\[ \\pi_\\varepsilon(\\theta \\mid y) \\propto \\int \\pi(y^* \\mid \\theta)\\pi(\\theta)\\mathbf{1}_{A_\\varepsilon(y^*)} dy^*, \\] where \\({A_\\varepsilon(y^*)} = \\{y^* \\mid ||y^* - y||&lt; \\varepsilon\\}\\). Example 4.7 This is a simple example, where we can derive the posterior distribution, but it allows us to see how this method works. Suppose we observe \\(y_1, \\ldots, y_{10}\\) from \\(\\text{Beta}(3, \\beta)\\). We placed a uniform prior on \\(\\beta\\) such that \\(\\beta \\sim \\text{Unif}[0, 5]\\). The ABC algorithm with rejection works as follows: Sample a value \\(\\beta^* \\sim \\text{Unif}[0, 5]\\). Simulate \\(y^*_1, \\ldots, y^*_{10} \\sim \\text{Beta}(3,\\beta^*)\\) Compute \\(D = \\sum_{i=1}^{10}(y_i -y^*_i)^2\\). If \\(D &lt; 0.75\\), accept \\(\\beta^*\\) as a sample from the posterior distribution. Otherwise, reject \\(\\beta^*\\). Repeat steps 1, 2, and 3. The code below carries out this algorithm. #Set Up Example set.seed(1234) n &lt;- 10 y &lt;- rbeta(n, 3, 2) y ## [1] 0.8519237 0.5286251 0.3126172 0.9691679 0.4883547 0.4677043 0.7339799 ## [8] 0.7279578 0.7317827 0.7971786 #Set Up ABC n.iter &lt;- 50000 b.store &lt;- numeric(n.iter) epsilon &lt;- 0.75 #Run ABC for(i in 1:n.iter){ #Propose new beta b &lt;- runif(1, 0, 5) #Simualate data y.star &lt;- rbeta(n, 3, b) #Compute statistic d &lt;- sum((y-y.star)^2) #Accept/Reject if(d &lt; epsilon){ b.store[i] &lt;- b } else{ b.store[i] &lt;- NA } } #Get number of reject samples sum(is.na(b.store)) ## [1] 37886 #Plot Approximate Posterior hist(b.store, freq = FALSE, xlab = expression(beta), main = &quot;&quot;) abline(v = 2, col = &#39;red&#39;) mean(b.store, na.rm = TRUE) ## [1] 2.03304 quantile(b.store, c(0.025, 0.975), na.rm = TRUE) ## 2.5% 97.5% ## 0.5843792 4.1369339 One important question is how to choose the value for \\(\\varepsilon\\)? It turns out this is an incredibly hard question that is specific to each application. Often the approximate posterior distribution \\(\\pi_\\varepsilon(\\theta \\mid y)\\) is very sensitive to the choice of \\(\\varepsilon\\). Example 4.8 Let’s repeat the example, first with \\(\\varepsilon = 0.12\\). In this case, almost all of the proposals are rejected. #Set Up Example set.seed(1234) n &lt;- 10 y &lt;- rbeta(n, 3, 2) y ## [1] 0.8519237 0.5286251 0.3126172 0.9691679 0.4883547 0.4677043 0.7339799 ## [8] 0.7279578 0.7317827 0.7971786 #Set Up ABC n.iter &lt;- 50000 b.store &lt;- numeric(n.iter) epsilon &lt;- 0.12 #Run ABC for(i in 1:n.iter){ #Propose new beta b &lt;- runif(1, 0, 5) #Simualate data y.star &lt;- rbeta(n, 3, b) #Compute statistic d &lt;- sum((y-y.star)^2) #Accept/Reject if(d &lt; epsilon){ b.store[i] &lt;- b } else{ b.store[i] &lt;- NA } } #Get number of reject samples sum(is.na(b.store)) ## [1] 49993 #Plot Approximate Posterior hist(b.store, freq = FALSE, xlab = expression(beta), main = &quot;&quot;) abline(v = 2, col = &#39;red&#39;) mean(b.store, na.rm = TRUE) ## [1] 1.831118 quantile(b.store, c(0.025, 0.975), na.rm = TRUE) ## 2.5% 97.5% ## 1.181349 2.587056 Example 4.9 And now again with \\(\\varepsilon = 2\\). In this case, almost all of the proposals are accepted. #Set Up Example set.seed(1234) n &lt;- 10 y &lt;- rbeta(n, 3, 2) y ## [1] 0.8519237 0.5286251 0.3126172 0.9691679 0.4883547 0.4677043 0.7339799 ## [8] 0.7279578 0.7317827 0.7971786 #Set Up ABC n.iter &lt;- 50000 b.store &lt;- numeric(n.iter) epsilon &lt;- 2 #Run ABC for(i in 1:n.iter){ #Propose new beta b &lt;- runif(1, 0, 5) #Simualate data y.star &lt;- rbeta(n, 3, b) #Compute statistic d &lt;- sum((y-y.star)^2) #Accept/Reject if(d &lt; epsilon){ b.store[i] &lt;- b } else{ b.store[i] &lt;- NA } } #Get number of reject samples sum(is.na(b.store)) ## [1] 471 #Plot Approximate Posterior hist(b.store, freq = FALSE, xlab = expression(beta), main = &quot;&quot;) abline(v = 2, col = &#39;red&#39;) mean(b.store, na.rm = TRUE) ## [1] 2.488073 quantile(b.store, c(0.025, 0.975), na.rm = TRUE) ## 2.5% 97.5% ## 0.1281937 4.8666030 When \\(\\varepsilon = 0.12\\), almost all the proposals are rejected. Although the approximate posterior mean is close to the true value, given we only have 7 samples we cannot say much about the posterior distribution. When \\(\\varepsilon = 2\\), almost all the proposals are accepted. This histogram shows that we are really just sampling from the prior distribution, i.e. \\(\\pi_2(\\beta \\mid y) \\approx \\pi(\\beta)\\). Proposition 4.2 Using an ABC rejection algorithm \\[ \\lim_{\\varepsilon \\rightarrow \\infty} \\pi_\\varepsilon(\\theta \\mid y) \\overset{D}= \\pi(\\theta), \\] and \\[ \\lim_{\\varepsilon \\rightarrow 0} \\pi_\\varepsilon(\\theta \\mid y) \\overset{D}= \\pi(\\theta \\mid y). \\] This example and the proposition show that if we set \\(\\varepsilon\\) too large, we don’t learn anything about \\(\\theta\\), we just recover the prior distribution. The smaller the value of \\(\\varepsilon\\), the better. But very small values may require very long run times, or have such few samples that the noise from the sampling generator is larger than the signal in the accepted samples. The only diagnostic tools we have are the proportion of samples accepted and the histograms of the approximate posterior and prior distributions. 4.5.2 Summary ABC with Rejection ABC with rejection suffers from the curse of dimensionality (see Chapter 5). As the number of data points increases, the probability we get a ‘close’ match decreases. This means we have to increase \\(\\varepsilon\\) and degrade the quality of our approximation. Example 4.10 Let’s repeat the Beta example with \\(n = 200\\) observed data points. We need \\(\\varepsilon &gt; 15\\) for any proposals to be accepted. We can avoid the curse of dimensionality by comparing summary statistics instead. This leads us to the Summary ABC algorithm. Definition 4.4 To carry out inference for a parameter \\(\\theta\\) using an Summary Approximate Bayesian Computation algorithm with rejection Sample a value for the parameter \\(\\theta^*\\) from the prior distribution \\(\\pi(\\theta)\\). Generate some data \\(y*\\) from the data generating process using the parameter value \\(\\theta^*\\). Accept \\(\\theta^*\\) as a value from the posterior distribution if \\(||S(y) - S(y^*)|| &lt; \\varepsilon\\) for some \\(\\varepsilon &gt; 0\\) and summary summary statistic \\(S\\). Otherwise reject \\(\\theta^*\\) Repeat steps 1 - 3. Similar to the ABC algorithm with rejection, we also have the following proposition. Proposition 4.3 The approximate posterior distribution using ABC with rejection is \\[ \\pi_\\varepsilon(\\theta \\mid S(y)) \\propto \\int \\pi(y^* \\mid \\theta)\\pi(\\theta)\\mathbf{1}_{A_\\varepsilon(y^*)} dy^*, \\] where \\({A_\\varepsilon(y^*)} = \\{y^* \\mid ||S(y^*) - S(y)||&lt; \\varepsilon\\}\\). Using summary statistics only increases the level of ‘’approximation’’, as we are approximating the data using a summary of it. The only case when we are not approximating further is when the statistic contains all the information about the underlying sample it is summarising. This is known as a sufficient statistic. Definition 4.5 A statistic \\(S\\) is a sufficient statistic for the parameter \\(\\theta\\) if the conditional distribution \\(\\pi(y | S(y))\\) does not depend on \\(\\theta\\). Proposition 4.4 Using a Summary ABC rejection algorithm with a sufficient statistic \\(S\\) \\[ \\lim_{\\varepsilon \\rightarrow 0} \\pi_\\varepsilon(\\theta \\mid S(y)) \\overset{D}= \\pi(\\theta \\mid y). \\] The difficulty with sufficient statistics is that they only exist for ‘nice’ distributions, like the Gamma, Beta and Poisson distributions. In these cases, we can work with the posterior distribution directly or use MCMC algorithms. Example 4.11 Let’s repeat the beta distribution example using the mean as the summary (but not sufficient) statistic. We can set \\(\\varepsilon = 0.001\\). set.seed(1234) n &lt;- 200 y &lt;- rbeta(n, 3, 2) n.iter &lt;- 50000 b.store &lt;- numeric(n.iter) epsilon &lt;- 0.001 for(i in 1:n.iter){ b &lt;- runif(1, 0, 5) y.star &lt;- rbeta(n, 3, b) d &lt;- sum((mean(y)-mean(y.star))^2) if(d &lt; epsilon){ b.store[i] &lt;- b } else{ b.store[i] &lt;- NA } } #Get number of reject samples sum(is.na(b.store)) ## [1] 45028 #Plot Approximate Posterior hist(b.store, freq = FALSE, xlab = expression(beta), main = &quot;&quot;) abline(v = 2, col = &#39;red&#39;) mean(b.store, na.rm = TRUE) ## [1] 1.930908 quantile(b.store, c(0.025, 0.975), na.rm = TRUE) ## 2.5% 97.5% ## 1.593348 2.302256 4.6 Lab The aim of this lab is to code up some sampling methods. You have already done some similar work in lab 1 (e.g. estimating \\(\\pi\\)). Exercise 4.1 Visit random.org to generate some truly random numbers. How does this site generate numbers that are truly random? Exercise 4.2 A random variable \\(X\\) has density \\(\\pi(x) = ax^2\\) for \\(x\\in[0,1]\\) and 0 otherwise. Find the value \\(a\\). Use the inverse transform method to generate 10,000 samples from this distribution. Plot a histogram against the true density function. Exercise 4.3 Let \\(X\\) have the density \\(\\pi(x) = \\frac{1}{\\theta}x^{\\frac{1-\\theta}{\\theta}}\\) for \\(x \\in [0, 1]\\). Use the inverse transform method to generate 10,000 samples from this distribution with \\(\\theta = \\{1, 5, 10\\}\\). Exercise 4.4 Let \\(X\\) have the density \\(\\pi(x) = 3x^2\\) for \\(x \\in [0, 1]\\) and 0 otherwise. Use a rejection sampling method to generate 10,000 from this distribution. Plot the results against the true density to check you have the same distribution. Exercise 4.5 The half normal distribution with mean 0 and variance 1 has density function \\[ \\pi(x) = \\frac{2}{\\sqrt{2\\pi}}\\exp{(-x^2/2)} \\] for \\(x \\geq 0\\) and 0 otherwise. Denote the exponential density function by \\(q(x) = \\lambda \\exp(-\\lambda x)\\). Find the smallest \\(c\\) such that \\(\\pi(x)/q(x) &lt; c\\). Use a rejection sampling algorithm with an exponential proposal distribution to generate samples from the half normal distibrution with mean 0 and variance 1. Exercise 4.6 You observe the following data from an \\(N(5, \\sigma^2)\\) distribution. -5.93, 33.12, -21.41, -12.42, -17.64, -5.47, -27.95, -22.25, -20.40, -26.28, -24.57, 3.06, 44.28, 6.02, -21.14, 14.79, -15.10, 53.18, 38.61, 5.71 Use an Exp(0.1) prior distribution on \\(\\sigma^2\\) and develop a summary statistic ABC algorithm to draw samples from the approximate posterior distribution. Exercise 4.7 You observe the following data from an \\(Exp(\\lambda)\\) distribution. 2.6863422, 8.8468112, 8.8781831, 0.2712696, 1.8902442 Use an \\(Beta(1, 3)\\) prior distribution on \\(\\lambda\\) and develop an ABC algorithm to draw samples from the approximate posterior distribution. Write the ABC algorithm as a function so you can run it for different values of \\(\\varepsilon\\). Run your algorithm for \\(\\varepsilon = \\{20, \\ldots, 100\\}\\) and record the approximate posterior median. Plot the relative error in your estimate against the true value of lambda, which is 0.2. "],["markov-chain-monte-carlo.html", "Chapter 5 Markov Chain Monte Carlo 5.1 Properties of Markov Chains 5.2 Metropolis–Hastings 5.3 Gibbs Sampler 5.4 Metropolis-within-Gibbs 5.5 MCMC Diagnostics 5.6 Beyond MCMC 5.7 Lab", " Chapter 5 Markov Chain Monte Carlo Markov Chain Monte Carlo (MCMC) is a class of algorithms that produce samples from a probability distribution. These methods combine the idea of rejection sampling with the theory of Markov chains. Before we set out the theory of Markov chains, we’ll go through an example to show how MCMC works. Example 5.1 (Adapted from Statistical Rethinking 9) Consider an eccentric King whose kingdom consists of a ring of 10 islands. Directly north is island one, the smallest island. Going clockwise around the archipelago, next is island two, which is twice the size of island one, then island three, which is three times as large as island one. Finally, island 10 is next to island one and ten times as large. The King wanted to visit all of his islands, but spending time on each one according to its size. That is he should spend the most time on island ten and the least on island one. Being climate conscious, he also decided that flying from one side of the archipelago to the other was not allowed. Instead, he would only sail from one island to either of its neighbors. So from island one, he could reach islands two and ten. He decided to travel according to these rules: At the end of each week, he decides to stay on the same island or move to a neighboring island according to a coin toss. If it’s heads he proposes moving clockwise, and tails anti-clockwise. The island he is considering moving to is called the proposal island. To decide if he is going to move to the proposal island, the King counts out a number of shells equal to the number of size of the island. So if island five is the proposal island, he counts out five shells. He then counts out a number of stones equal to the size of the current island. If the number of seashells is greater than the number of stones, he moves to the proposed island. If the number of seashells is less than the number of stones, he takes a different strategy. He discards the number of stones equal to the number of seashells. So if there are six stones and five seashells, he ends up with 6-5=1 stones. He then places the stones and seashells into a bag and pulls one at random. If he picks a seashell, he moves to the proposed island, otherwise if he picks a stone, he stays put. This is a complex way of moving around, but it produces the required result; the time he spends on each island is proportionate to the size of the island. The code below shows an example of this over 10,000 weeks. weeks &lt;- 10000 island &lt;- numeric(weeks) current &lt;- 10 for(i in 1:weeks){ ## record current position island[i] &lt;- current #Flip a coin to move to a propose a new island proposed &lt;- current + sample(c(1, -1), size = 1) #Ensure he loops round the island if(proposed &lt; 1) proposed &lt;- 10 if(proposed &gt; 10) proposed &lt;- 1 #Decide to move p &lt;- proposed/current u &lt;- runif(1) if(u &lt; p) current &lt;- proposed } #Plot results plot(island, type = &#39;l&#39;, xlab = &quot;Week&quot;, ylab = &quot;Island&quot;) barplot(table(island)/weeks, xlab = &quot;Island&quot;, ylab = &quot;Proportion of time&quot;) We can recognise several different statistical principles in this example. The King decides to move islands dependent on where he is currently, not based on where he has been previously (Markov property). He proposes an island to move to and accepts or rejects this decision based on some distribution (rejection principle). We are now going to describe some of the properties of Markov chains, including the Markov property. 5.1 Properties of Markov Chains Definition 5.1 A sequence of random variables \\(Y = (Y_n)_{n \\in \\mathbb{N}_0}\\) is a Markov chain if \\(\\mathbb{P}(Y_{n+1} \\in \\cdot \\mid Y_{n}, \\ldots, Y_0) = \\mathbb{P}(Y_{n+1} \\in \\cdot \\mid Y_{n})\\) a.s. That is, the distribution of the next state \\(Y_{n+1}\\) conditional on its entire past only depends on the current state \\(Y_n\\). Definition 5.2 A Markov chain is called homogeneous if for any \\(n,m \\in \\mathbb{N}_0\\) it holds that \\(\\mathbb{P}(Y_{n+ m} \\in \\cdot \\mid Y_n) = \\mathbb{P}(Y_{m} \\in \\cdot \\mid Y_0)\\). That is, transition probabilities only depend on the time differences. From here on, we will only deal with homogeneous Markov chains and will therefore synonymously use the terms Markov chain and homogeneous Markov chain. Definition 5.3 If for some finite set \\(S = \\{i_1,\\ldots,i_N\\}\\) we have \\(\\mathbb{P}(Y_n \\in S) = 1\\) for all \\(n \\in \\mathbb{N}_0\\), we call \\(Y\\) a finite Markov chain and \\(S\\) its state space. By relabelling, we may always assume wlog that \\(S = \\{1,\\ldots,N\\}\\) for a finite Markov chain with \\(N\\) states. Definition 5.4 For a finite Markov chain the probability of transitioning from state \\(i\\) to state \\(j\\) in a Markov chain is given by \\(p_{ij}\\). The transition matrix for a Markov chain with \\(N\\) states is the \\(N \\times N\\) matrix \\(P = (p_{ij})_{i,j=1,\\ldots,N}\\), where the \\(\\{i, j\\}^{th}\\) entry \\(p_{ij}\\) is the probability of moving from state \\(i\\) to state \\(j\\), i.e., \\(\\mathbb{P}(Y_{n+1} = j \\mid Y_n = i) = \\mathbb{P}(Y_{1} = j \\mid Y_0 = i).\\) These properties make Markov chains nice to work with. Two other important definitions are: Definition 5.5 For a finite Markov chain, the period of a state \\(i\\) is given by \\(d_i = \\textrm{gcd}\\{n &gt; 0; P^n(i,i) &gt; 0 \\}\\). A state is aperiodic if \\(d_i = 1\\). An aperiodic chain is a chain where all states are aperiodic. Definition 5.6 A finite Markov chain is irreducible if for any \\(i,j \\in S\\) there exists an \\(n \\in \\mathbb{N}_0\\) such that \\(\\mathbb{P} (Y_n = j \\mid Y_0 = i) &gt; 0\\). In other words, it is possible to move from any state to any other state in a finite number of steps. We can use these definitions to start working with distributions. Suppose, the state we start at is drawn from some distribution \\(Y_0 \\sim q\\). Then the distributions of the second state \\(Y_1\\) depends on the distribution of \\(Y_0\\) and the transition probabilities \\[ \\mathbb{P}(Y_1 = j) = \\sum_{i=1}^N q_ip_{ij}. \\] If we denote the distribution of \\(Y_1 \\sim {q}^{(1)}\\), then we can write it in terms of the transition matrix \\({q}^{(1)} = {q}P\\). Now suppose we would like to determine the distribution of \\(Y_2 \\sim {q}^{(2)}\\); thanks to the Markov property, this is the distribution for \\(Y_1\\) multiplied by the transition matrix from the right, so \\(Y_2 \\sim qP^2\\). Inductively, \\(Y_k \\sim qP^{k}\\). To use Markov chains to sample from distributions, we need to identify the eigenvalues of the transition matrix. Definition 5.7 Suppose that there exists a probability vector \\(\\pi\\) (that is, \\(\\pi_i \\geq 0\\) and \\(\\sum_{i=1}^n \\pi(i) = 1\\)) that is a left-eigenvector with eigenvalue \\(1\\) for the transition matrix \\(P\\), i.e., \\[ \\pi P = \\pi. \\] Then \\(\\pi\\) is called the stationary distribution of the Markov chain corresponding to \\(P\\). The terminology arises from the fact that if the Markov chain is started in \\(\\pi\\), the marginal distributions \\(\\mathbb{P}_{Y_n}\\) remain unchanged (they are invariant under time translation), since then \\[q^{(n)} = \\pi P^n = (\\pi P)P^{n-1} = \\pi P^{n-1} = \\cdots = \\pi.\\] A basic fact is the following. Theorem 5.1 If a finite Markov chain is irreducible, a unique stationary distribution exists. Definition 5.8 A finite Markov chain is called ergodic if for all \\(i,j \\in S\\) it holds that \\[\\lim_{n \\to \\infty} \\mathbb{P}(Y_n = i \\mid Y_0 = j) = \\pi_i.\\] Equivalently, for any initial distribution \\(q\\), \\[\\lim_{n \\to \\infty} qP^n = \\pi.\\] Irreducibility is not sufficient to guarantee ergodicity of a Markov chain: take a two state Markov chain with transition matrix \\[P = \\begin{pmatrix} 0 &amp; 1\\\\ 1 &amp; 0 \\end{pmatrix},\\] that is the chain deterministically moves from state \\(1\\) to \\(2\\) and vice versa. Then \\(Y\\) is clearly irreducible and \\(\\pi = (1/2,1/2)\\) is its unique stationary distribution since \\[(1/2, 1/2) = \\begin{pmatrix} 0 &amp; 1\\\\ 1 &amp; 0 \\end{pmatrix} = (1/2,1/2).\\] However, if started in \\(1\\), the chain does not converge, since then \\(\\mathbb{P}(Y_{2n} = 1) = 1 \\neq 1/2 = \\pi(1).\\) The problem is the lack of aperiodicity, since by its very nature, \\(d_1 = d_2 = 2 &gt; 1\\). In fact, aperiodicity is the missing piece to guarantee convergence to the stationary distribution, irrespectively of the initialisation: Theorem 5.2 A finite Markov chain is ergodic if, and only if, it is irreducible and aperiodic. This important concept underpins MCMC methods. It says that no matter where we start our ergodic chain, we’ll eventually end up sampling states according to the target distribution \\(\\pi\\). It make take a long time to reach the stationary distribution, but it will eventually get there. In order to check whether our finite Markov chain will converge to a stationary distribution, we therefore need to check: the Markov chain is irreducible, and the Markov chain is aperiodic. When we want to sample from a prescribed distribution \\(\\pi\\) we additionally need to check the following condition for a finite Markov chain satisfying the above: \\(\\pi\\) is stationary for the Markov chain. Example 5.2 In Example 5.1, the King wanted to visit the islands according to how large they are. We can think of the islands as the states and the stationary distribution as \\(p(Y = i) \\propto i\\). The eccentric method the King used allowed him to construct a transition matrix for an aperiodic Markov chain. He also never visited islands regularly using this method. When designing a Markov chain, it is usually straightforward to design one that meets conditions one and two. Constructing a chain with prescribed stationary distribution \\(\\pi\\) is more difficult, but a detailed balance criterion can help with this. Definition 5.9 The Markov chain with transition matrix \\(P\\) satisfies the detailed balance equation with respect to the distribution \\(\\pi\\) if \\[ \\pi_i p_{ij} = \\pi_j p_{ji}. \\] Theorem 5.3 (Detailed Balance) Let \\(P\\) be a transition matrix that satisfies detailed balance with respect to the distribution \\(\\pi\\). Then \\(\\pi P = \\pi\\). Proof. The \\(j^{th}\\) entry of \\(\\pi P\\) is \\[\\begin{align*} \\sum_{i} \\pi_i p_{ij} &amp; = \\sum_{i} \\pi_j p_{ji} \\quad \\textrm{(detailed balance)} \\\\ &amp; = \\pi_j \\sum_{i} p_{ji} \\\\ &amp; = \\pi_j.\\qquad \\textrm{(probaility sums to 1)} \\end{align*}\\] Hence \\(\\pi P = \\pi\\). So far, we have shown that we can use finite state Markov chain theory to simulate from a discrete probability distribution \\(\\pi\\) by running a Markov chain with \\(\\pi\\) as a stationary distribution for a sufficently long time. However, for discrete distributions this is of course overkill, since we can easily sample from such distributions via the inverse transformation method. All of the above concepts translate however in a natural way to Markov chains with more complicated state spaces, and we will just briefly comment on this now: The generalisation of the transition matrix is a Markov kernel \\(P \\colon S \\times \\mathcal{B}(S) \\to [0,1]\\) on a general Borel state space \\((S,\\mathcal{B}(S))\\), which satisfies the following properties: \\(\\forall x \\in S: \\quad P(x, \\cdot) \\text{ is a probability measure}\\), \\(\\forall A \\in \\mathcal{B}(S): \\quad x \\mapsto P(x, A) \\text{ is measurable.}\\) For two Markov kernels \\(P,Q\\) we let \\[P\\circ Q(x,A) = \\int_{S} P(x, dy ) Q(y,A), \\quad (x,A) \\in S \\times \\mathcal{B}(S),\\] and for a probability measure \\(q\\) on \\(S\\) let \\(qP\\) be the probability measure \\[qP(A) = \\int_S q(dx) P(x,A), \\quad A \\in \\mathcal{B}(S).\\] If \\(S\\) is nice enough, the Markov property is equivalent to demanding that for some Markov kernel \\(P\\) we have \\[\\forall A \\in \\mathcal{B}(S): \\quad \\mathbb{P}(Y_{n+1} \\in A \\mid Y_n, \\ldots, Y_0) = P(Y_n, A). \\] Then, if we define iteratively the Markov kernels \\(P^1 = P\\) and \\[P^{n+1}(x,A) = P^n \\circ P(x,A) = \\int_{S} P^n(x, dy) P(y, A), \\quad (x,A) \\in S \\times \\mathcal{B}(S),\\] we have the Chapman–Kolmogorov equation \\[ P^{n+m} = P^n \\circ P^m \\] and \\[ \\mathbb{P}(Y_1 \\in A_1, \\ldots, Y_n \\in A_n \\mid Y_0 = x) = \\int_S P(x, dy_0) \\int_{A_1 \\times \\cdots \\times A_n} \\prod_{i=1}^n P(y_{i-1}, dy_i). \\] Example 5.3 Let \\(X= (X_n)_{n\\in \\mathbb{N}}\\) be an i.i.d. sequence of \\(\\mathbb{R}^d\\)-valued random variables with distribution \\(\\mathbb{P}_{X_1} = \\eta\\) and \\(X_0 \\sim \\mu\\) be independent of \\((X_n)_{n\\in \\mathbb{N}}\\). A random walk \\(Y\\) associated to the sequence of innovations \\(X\\) and initialistion \\(X_0\\) is defined by \\(Y_0 = X_0\\) and \\[Y_n = X_0 + \\sum_{i=1}^n X_i, \\quad n \\in \\mathbb{N},\\] which can be equivalently characterised by the recursive relation \\[Y_{n+1} = Y_n + X_{n+1}, \\quad n \\in \\mathbb{N}_0. \\] This immediately yields the Markov property \\[\\mathbb{P}(Y_{n+1} \\in A \\mid Y_0,\\ldots,Y_n) = \\mathbb{P}(Y_{n+1} \\in A \\mid Y_n) \\] and defining for \\(A -x := \\{z = y -x: y \\in A\\}\\) the Markov kernel \\[P(x,A) = \\mathbb{P}(X_1 \\in A - x) = \\eta(A -x) \\] we obtain \\[\\mathbb{P}(Y_{n+1} \\in A \\mid Y_n) = \\mathbb{P}(Y_n + X_{n+1} \\in A \\mid Y_n) = \\mathbb{P}(X_{n+1} \\in A - Y_n \\mid Y_n) = P(Y_n, A). \\] Example 5.4 Let \\((\\xi_n)_{n \\in \\mathbb{N}} \\overset{\\mathrm{i.i.d}}{\\sim} N(0, I)\\) and \\(V \\colon \\mathbb{R}^d \\to \\mathbb{R}\\) be a continuously differentiable function. Then, for \\(Y_0 \\sim \\mu\\) independent of \\((\\xi_n)_{n \\in \\mathbb{N}}\\) and a step size \\(h &gt; 0\\) we define recursively \\[\\begin{align*} Y_{n+1} &amp;= Y_n - h \\nabla V(Y_n) + \\sqrt{2h} \\xi_{n+1}, \\quad n \\in \\mathbb{N}_0. \\end{align*}\\] This is very reminiscent of a gradient descent algorithm with an additional stochastic perturbation that is distributed according to \\(N(0,2hI)\\) and therefore centered and rather small for small step sizes \\(h\\). We call this Markov chain a Langevin Markov Chain with step size \\(h\\) and potential \\(V\\). Because the innovations are i.i.d. the chain is clearly Markov and since \\(Y_{n +1} \\mid Y_n \\sim N(Y_n - h \\nabla V(Y_n), 2h I )\\) its Markov kernel is given by \\[P(x,dy) = \\phi_{x - h\\nabla V(x), 2h I }(y) \\, dy, \\] where \\(\\phi_{\\mu,\\Sigma}\\) denotes the pdf of a normal random vector with mean \\(\\mu\\) and covariance matrix \\(\\Sigma\\). Analogously to the finite state space case we then call a distribution \\(\\pi\\) on \\(S\\) a stationary distribution if \\[\\pi P = \\pi \\qquad (\\iff \\forall A \\in \\mathcal{B}(S): \\int \\pi(dx)\\, P(x,A) = \\pi(A)),\\] and thus iteratively \\[\\mathbb{P}(Y_n \\in \\cdot \\mid Y_0 \\sim \\pi) = \\pi.\\] Definition 5.10 A Markov chain \\(Y\\) with unique stationary distribution \\(\\pi\\) is called ergodic if \\[\\forall x \\in S: \\quad \\lim_{n \\to \\infty} \\lVert P^n(x,\\cdot) - \\pi \\rVert_{\\mathrm{TV}} = 0,\\] where we recall that for two probability measures \\(\\mu,\\nu\\) on \\(S\\) their total variation distance is given by \\[\\lVert \\mu - \\nu \\rVert_{\\mathrm{TV}} = \\sup_{A \\in \\mathcal{B}(S)} \\lvert \\mu(A) - \\nu(A) \\rvert.\\] Example 5.5 Let’s return to the Langevin Markov chain from above, where \\[Y_{n+1} = Y_n - h\\nabla V(Y_n) + \\sqrt{2h} \\xi_{n+1}, \\quad (\\xi_n)_{n\\in \\mathbb{N}} \\overset{\\mathrm{i.i.d.}}{\\sim} N(0,I). \\] Given appropriate conditions on \\(h\\) and the potential \\(V\\), which are similar to those that guarantee convergence of a gradient descent algorithm to obtain minimisers of \\(V\\) (say, strong convexity and Lipschitz-continuity), the Markov chain is ergodic with invariant distribution \\(\\pi^h\\). Here, for sufficiently small step sizes \\(h\\) we have \\(\\pi^h \\approx \\pi\\), where \\(\\pi\\) has density proportional to \\[\\pi(y) \\propto \\mathrm{e}^{-V(y)}, \\quad y \\in \\mathbb{R}^d. \\] This is our first example of an MCMC algorithm for the target density \\(\\pi\\) (called the Langevin Monte-Carlo algorithm) and is especially relevant in high dimensions, where the normalising constant \\(\\int_{\\mathbb{R}^d} \\mathrm{e}^{-V(y)}\\, dy\\) is very expensive to calculate numerically, therefore making rejection algorithms infeasible. Determining whether a general Markov chain is ergodic is a much more demanding task than in the finite state space situation. For a given Markov kernel, verification is based on checking extensions of the notions of irreducibility, aperiodicity and recurrence for countable Markov chains, where the latter property encodes the idea that return times to any state \\(i\\) should be finite almost surely. The main challenge for general state spaces is that there need not be any states \\(x \\in S\\) such that \\(P(y,\\{x\\}) &gt; 0\\) for any \\(y \\in S\\) (e.g. when the transition probabilities are absolutely continuous). Instead of going into technical details, we only cite a quantitative result from (Hairer and Mattingly 2011) that provides exponentially fast convergence to the invariant distribution given explicit criteria on \\(P\\). Theorem 5.4 Suppose that \\(Y\\) satisfies the following conditions: Drift condition: There exists a penalty function \\(V\\colon S \\to [1,\\infty)\\) and constants \\(K \\geq 0, \\gamma \\in (0,1)\\) such that \\[\\forall x \\in S: \\underbrace{\\int_S V(y)\\, P(x,dy)}_{= \\mathbb{E}[V(Y_1) \\mid Y_0 = x]} \\leq \\gamma V(x) + K. \\] Minorisation condition: There exists a constant \\(\\alpha \\in (0,1)\\) and a probability measure \\(\\nu\\) such that \\[\\forall A \\in \\mathcal{B}(S): \\quad \\inf_{x \\in C} P(x,A) \\geq \\alpha \\nu(A)\\] where \\(C = \\{x \\in S: V(x) \\leq R\\}\\) for some \\(R &gt; 2K/(1-\\gamma).\\) Then, \\(Y\\) has a unique stationary distribution \\(\\pi\\) and there exist explicit constants \\(D &gt; 0, \\kappa \\in (0,1)\\) such that \\[\\lVert P^n(x,\\cdot) - \\pi \\rVert_{\\mathrm{TV}} \\leq DV(x) \\kappa^n, \\quad x \\in S.\\] As a corollary we obtain the following result, which is mostly useful for bounded state spaces. Corollary 5.1 Suppose that for some probability measure \\(\\nu\\) and \\(\\alpha \\in (0,1)\\) the following Doeblin minorisation criterion holds: \\[\\forall x \\in S, A \\in \\mathcal{B}(S): \\quad P(x,A) \\geq \\alpha \\nu(A). \\] Then, there exist constants \\(D &gt; 0,\\kappa \\in (0,1)\\) such that \\[\\sup_{x \\in S} \\lVert P^n(x,\\cdot) - \\pi \\rVert_{\\mathrm{TV}} \\leq D\\kappa^n.\\] Proof. Choose \\(\\gamma = 1/2, V \\equiv 1, K = 1/2\\). Then, for arbitrary \\(R &gt; 2 = 2K/(1-\\gamma)\\), we have \\(C = \\{x \\in S: V(x) \\leq R\\} = S\\) and hence by assumption \\[\\inf_{x \\in C} P(x,A) = \\inf_{x \\in S} P(x,A) \\geq \\alpha \\nu.\\] Moreover, for any \\(x \\in S\\) by construction. \\[\\mathbb{E}[V(Y_1) \\mid Y_0 = x] = 1 = \\gamma V(x) +K.\\] Consequently, the drift and minorisation criteria from the previous theorem are satisfied, yielding the assertion. Remark. It can be shown that \\(\\kappa = 1 - \\alpha, D=1\\), see Theorem 16.0.2 in (Meyn and Tweedie 2009). Such results provide the foundation theory for MCMC and allow us to sample approximatively from a posterior distribution \\(\\pi\\) for an appropriately constructed chain. What it doesn’t tell us is how to design the Markov chain, and that is what the next sections deal with. The following extension of the detailed balance condition will be helpful for this purpose: Theorem 5.5 (Detailed Balance) Let \\(P\\) be a Markov kernel and \\(\\pi\\) be a distribution on the state space \\(S\\). If \\(\\pi\\) and \\(P\\) satisfy detailed balance in the sense \\[\\begin{align*} &amp;\\forall x,y \\in S: \\pi(dx)\\, P(x,dy) = \\pi(dy)\\, P(y,dx) \\\\ &amp;(\\text{i.e.,} \\forall B \\in \\mathcal{B}(S) \\otimes \\mathcal{B}(S): \\int \\mathbf{1}_B(x,y)\\, \\pi(dx) \\, P(x,dy) = \\int \\mathbf{1}_B(x,y)\\, \\pi(dy) \\, P(y,dx) \\end{align*}\\] then \\(\\pi P = \\pi\\), i.e., \\(\\pi\\) is stationary. Proof. Using detailed balance and Fubini, we find for any \\(A \\in \\mathcal{B}(S)\\), \\[\\begin{align*} \\pi P(A) &amp;= \\int_{x \\in S} \\int_{y \\in A} \\pi(dx)\\, P(x,dy) =\\int_{x \\in S} \\int_{y \\in A} \\pi(dy) \\, P(y,dx)\\\\ &amp;= \\int_{y \\in A} \\underbrace{\\Big(\\int_{x \\in S} P(y,dx) \\Big)}_{= \\mathbb{P}(Y_1 \\in S \\mid Y_0 = y) = 1} \\, \\pi(dy) = \\pi(A). \\end{align*}\\] 5.2 Metropolis–Hastings We’re now going to look at MCMC algorithms. The first algorithm we are going to look at is the Metropolis–Hastings algorithm. This is a useful algorithm if we cannot sample directly from the posterior distribution and if the conditional distributions do not have a closed form. The Metropolis–Hastings algorithm is like the island example we saw earlier. At each iteration, we propose a new sample and then accept or reject it based on the likelihood function, the prior and how likely we are to propose this new sample given the current one. Suppose we want to sample from the posterior distribution \\(\\pi(\\theta \\mid \\boldsymbol{y})\\). The Metropolis–Hastings works as follows: Set the initial value \\(\\theta^{(0)}\\). Set \\(i = 1\\) Propose a new value of \\(\\theta&#39;\\) from some distribution \\(q(\\cdot \\mid \\theta)\\) for \\(\\theta = \\theta^{(i-1)}\\) Accept \\(\\theta&#39; = \\theta^{(i)}\\) with probability \\[ p_{\\textrm{acc}} = p_{\\textrm{acc}}(\\theta,\\theta^\\prime) = \\min\\left\\{\\frac{\\pi(\\theta&#39; \\mid \\boldsymbol{y})}{\\pi(\\theta \\mid \\boldsymbol{y})}\\frac{q(\\theta \\mid \\theta&#39;)}{q(\\theta&#39; \\mid \\theta)}, 1\\right\\}, \\] otherwise set \\(\\theta^{(i)} = \\theta\\) (in the acceptance probability use the convention \\(x/0 = +\\infty\\) for any \\(x\\)) Set \\(i = i+1\\) and repeat steps 3 to 4 for \\(i = 2, \\ldots, n-1\\). This yields a Markov chain with transition kernel given by \\[P(\\theta,d\\theta^\\prime) = p_{\\textrm{acc}}(\\theta,\\theta^\\prime) q(\\theta^\\prime \\mid \\theta)\\, d\\theta^\\prime + \\delta_{\\theta}(d \\theta^\\prime) \\int (1- p_{\\mathrm{acc}}(\\theta,z)) q(z \\mid \\theta) \\, dz. \\] Let us also note that the specific choice of a posterior as target sampling density is just for the sake of exposition in this chapter. The algorithm can be applied to any target distribution \\(\\pi\\). There are two parts to the acceptance probability in step 4. The first is the posterior ratio, similar to saying the likelihood of \\(\\theta&#39;\\) given the observed data over the likelihood of \\(\\theta\\) given the data. The second is the proposal ratio. It is similar to saying the likelihood of proposing \\(\\theta\\) given the current value \\(\\theta&#39;\\), over the likelihood of proposing \\(\\theta&#39;\\) given the current value \\(\\theta\\). In practice, we don’t need to evaluate the full posterior distribution. Recall \\[ \\pi(\\theta \\mid \\boldsymbol{y}) = \\frac{\\pi(\\boldsymbol{y} \\mid \\theta) \\pi(\\theta)}{\\pi(y)} \\] As the the denominator doesn’t depend on \\(\\theta\\), it cancels in the ratio. The ratio becomes \\[ \\frac{\\pi(\\theta&#39; \\mid \\boldsymbol{y})}{\\pi(\\theta \\mid \\boldsymbol{y})} = \\frac{\\pi(\\boldsymbol{y} \\mid \\theta&#39;) \\pi(\\theta&#39;)}{\\pi(\\boldsymbol{y} \\mid \\theta) \\pi(\\theta)}. \\] This is the likelihood ratio multiplied by the prior ratio. Theorem 5.6 The Markov chain generated by the Metropolis–Hastings algorithm satisfies detailed balance with respect to the posterior distribution and the posterior is stationary. Proof. For detailed balance we need to show that \\[\\pi(\\theta \\mid \\boldsymbol{y}) P(\\theta, d\\theta^\\prime) \\, d\\theta = \\pi(\\theta^\\prime \\mid \\boldsymbol{y}) P(\\theta^\\prime, d\\theta) \\, d\\theta^\\prime, \\] and stationarity will follow from that by Theorem 5.5. The case \\(\\theta = \\theta^\\prime\\) is trivial, so let us consider the case \\(\\theta \\neq \\theta^\\prime\\). We then have \\[\\begin{align*} \\frac{P(\\theta,d\\theta^\\prime)}{d\\theta^\\prime} &amp;= q(\\theta&#39; \\mid \\theta)p_{acc}(\\theta,\\theta^\\prime)\\\\ &amp;= q(\\theta&#39; \\mid \\theta)\\min\\left\\{\\frac{\\pi(\\theta&#39; \\mid \\boldsymbol{y})}{\\pi(\\theta \\mid \\boldsymbol{y})}\\frac{q(\\theta \\mid \\theta&#39;)}{q(\\theta&#39; \\mid \\theta)}, \\, 1\\right\\} \\\\ &amp; = \\min\\left\\{\\frac{\\pi(\\theta&#39; \\mid \\boldsymbol{y})}{\\pi(\\theta \\mid \\boldsymbol{y})}q(\\theta \\mid \\theta&#39;),\\, q(\\theta&#39; \\mid \\theta)\\right\\}, \\end{align*}\\] which shows that the lhs of the desired detailed balance equation is given by \\[ \\pi(\\theta \\mid \\boldsymbol{y})P(\\theta, d\\theta^\\prime) d\\theta = \\min\\{\\pi(\\theta&#39; \\mid \\boldsymbol{y})q(\\theta \\mid \\theta&#39;),\\, \\pi(\\theta \\mid \\boldsymbol{y})q(\\theta&#39; \\mid \\theta)\\} \\, d\\theta^\\prime\\, d\\theta. \\] By symmetric arguments, it follows that the rhs is given by \\[ \\pi(\\theta^\\prime \\mid \\boldsymbol{y})P(\\theta^\\prime, d\\theta) \\, d\\theta^\\prime = \\min\\{\\pi(\\theta \\mid \\boldsymbol{y})q(\\theta^\\prime \\mid \\theta),\\, \\pi(\\theta^\\prime \\mid \\boldsymbol{y})q(\\theta \\mid \\theta&#39;)\\} \\, d\\theta\\, d\\theta^\\prime, \\] which establishes equality of the lhs and rhs, whence detailed balance for \\(\\theta \\neq \\theta^\\prime\\). The speed of convergence of the Metropolis–Hastings algorithm and related MCMC algorithms has been a central research questions for the past 30 years or so, as this is fundamental to evaluate their sampling efficiency. Here, we focus on a simple case, where sampling works particularly well, and which sometimes can be applied to target posteriors \\(\\pi(\\cdot \\mid \\boldsymbol{y}\\) that are uniformly bounded away from zero on its bounded support. Theorem 5.7 Suppose that \\(\\pi(\\theta \\mid \\boldsymbol{y})\\) is a posterior with support \\(S\\) and that there exist finite constants \\(m,M &gt; 0\\) we have \\(m \\leq \\inf_{\\theta \\in S} \\pi(\\theta \\mid \\boldsymbol{y}) \\leq \\sup_{\\theta \\in S} \\pi(\\theta \\mid \\boldsymbol{y}) \\leq M\\). Let also \\(q\\) be a symmetric proposal density on \\(S\\) (that is, \\(q(\\theta^\\prime \\mid \\theta) = q(\\theta \\mid \\theta^\\prime)\\) for all \\(\\theta,\\theta^\\prime \\in S\\)) such that \\(q(\\theta^\\prime \\mid \\theta) \\geq \\rho\\nu(\\theta^\\prime)\\) for all \\(\\theta \\in S\\), some \\(\\rho \\in (0,1)\\) and some probability density \\(\\nu\\) on \\(S\\). Then, for the Metropolis–Hastings chain it holds that \\[\\sup_{x \\in S} \\lVert P^n(x,\\cdot) - \\pi(\\cdot \\mid \\boldsymbol{y}) \\rVert_{\\mathrm{TV}} \\leq \\Big(1 - \\frac{\\rho m}{M}\\Big)^n.\\] Proof. By symmetry of the proposal density and the upper and lower bounds on the posterior, it follows that \\[\\begin{align*} p_{\\mathrm{acc}}(\\theta,\\theta^\\prime) = \\min\\Big\\{\\frac{\\pi(\\theta^\\prime \\mid \\boldsymbol{y})}{\\pi(\\theta \\mid \\boldsymbol{y})}, 1 \\Big\\} \\geq \\frac{m}{M}. \\end{align*}\\] Therefore, for any \\(A \\in \\mathcal{B}(S)\\) it follows that \\[\\begin{align*} \\forall \\theta \\in S: \\quad P(\\theta,A) &amp;= \\int_A p_{\\textrm{acc}}(\\theta,\\theta^\\prime) q(\\theta^\\prime \\mid \\theta)\\, d\\theta^\\prime + \\delta_{\\theta}(A) \\int (1- p_{\\mathrm{acc}}(\\theta,z)) q(z \\mid \\theta) \\, dz \\\\ &amp;\\geq \\int_A p_{\\textrm{acc}}(\\theta,\\theta^\\prime) q(\\theta^\\prime \\mid \\theta)\\, d\\theta^\\prime \\\\ &amp;\\geq \\frac{m\\rho}{M} \\int_A \\nu(\\theta^\\prime) \\,d\\theta^\\prime. \\end{align*}\\] Since the rhs is independent of \\(\\theta\\), the Doeblin minorisation criterion is fulfilled with \\(\\alpha = \\rho m/M\\) and Corollary 5.1 yields \\[\\sup_{x \\in S} \\lVert P^n(x,\\cdot) - \\pi(\\cdot \\mid \\boldsymbol{y}) \\rVert_{\\mathrm{TV}} \\leq (1-\\alpha)^n = \\Big(1 - \\frac{\\rho m}{M}\\Big)^n.\\] Example 5.6 A counter monitors the time until atoms decays. It collects the data \\(Y_1, \\ldots, Y_N\\) and for a decay rate \\(\\lambda\\) we assume \\(Y_1,\\ldots, Y_N \\mid \\lambda \\overset{\\mathrm{iid}}{\\sim} \\hbox{Exp}(\\lambda)\\). The time until these atom decay is long, certainly more than 1 second (or minute, hour etc.), so we assume \\(\\lambda \\sim \\hbox{Beta}(\\alpha, \\beta)\\). The posterior distribution is given by \\[ \\pi(\\lambda \\mid \\boldsymbol{y}) \\propto \\lambda^{N + \\alpha - 1} (1 - \\lambda)^{\\beta - 1}\\exp\\left(-\\lambda \\sum y_i\\right) \\] This doesn’t have a closed form, so we need to use a Metropolis–Hastings algorithm to generate samples from this distribution. A suitable algorithm is Decide on an starting value the Markov chain and denote it by \\(\\lambda^{(0)}\\). Set \\(i = 1\\). Propose a new value for the parameter and denote it \\(\\lambda&#39;\\). In this example, we are going to propose values using a random walk method where \\(\\lambda&#39; \\sim N(\\lambda^{(i-1)}, \\sigma^2)\\). Accept \\(\\lambda&#39;\\) as the value \\(\\lambda^{(i)}\\) with probability \\(p_{\\textrm{acc}}\\). Otherwise reject this value and set \\(\\lambda^{(i)} = \\lambda^{(i-1)}\\). Set \\(i = i + 1\\). Repeat steps 2, 3, 4, for \\(i = 2, \\ldots, n-1\\). The value \\(p_{\\textrm{acc}}\\) is given by \\[ p_{\\textrm{acc}} = \\min \\left\\{1, \\, \\frac{\\lambda&#39;^{N + \\alpha - 1} (1 - \\lambda&#39;)^{\\beta - 1}\\exp\\left(-\\lambda&#39; \\sum y_i\\right)}{\\lambda^{N + \\alpha - 1} (1 - \\lambda)^{\\beta - 1}\\exp\\left(-\\lambda \\sum y_i\\right)}\\right\\} \\\\ = \\min \\left\\{1, \\, \\left(\\frac{\\lambda&#39;}{\\lambda}\\right)^{N + \\alpha - 1} \\left(\\frac{1-\\lambda&#39;}{1-\\lambda}\\right)^{\\beta -1} \\exp\\left((\\lambda- \\lambda&#39;)\\sum y_i\\right)\\right\\} \\] As we our proposal distribution is symmetric, \\(q(\\lambda \\mid \\lambda&#39;) = q(\\lambda&#39; \\mid \\lambda)\\) and this term cancels. The code below shows this algorithm in action # Set Up MCMC Algorithm --------------------------------------------------- n.iter &lt;- 10000 lambda.store &lt;- numeric(n.iter) #Store value of Markov chain at end of every iteration #Initialise Prior Parameters and Data lambda &lt;- 0.5 sum.y &lt;- 67.6 N &lt;- 20 alpha &lt;- 1 beta &lt;- 1 # Run MCMC Algorithm ------------------------------------------------------ for(i in 1:n.iter){ #Propose new value of lambda lambda.prop &lt;- rnorm(1, lambda, 0.1) #Check lambda \\in [0, 1] if(lambda.prop &gt; 0 &amp; lambda.prop &lt; 1){ #Compute p_acc log.p.acc &lt;- (N + alpha - 1)*log(lambda.prop/lambda) + (beta - 1)*log((1-lambda.prop)/(1-lambda)) + (lambda - lambda.prop)*sum.y #Accept/Reject step if(log(runif(1)) &lt; log.p.acc) lambda &lt;- lambda.prop } #Store current value of Markov Chain lambda.store[i] &lt;- lambda } #Plot trace plot (Markov chain values) plot(lambda.store, type = &#39;l&#39;, xlab = &quot;iteration&quot;, ylab = expression(lambda)) abline(h=0.3, col = 2) #the value I used to simulate the data #Plot posterior density hist(lambda.store, prob = TRUE, xlab = expression(lambda), main = &quot;Posterior density&quot;) abline(v=0.3, col = 2) #the value I used to simulate the data mean(lambda.store) #posterior mean ## [1] 0.3086762 quantile(lambda.store, c(0.025, 0.975)) #95% CI ## 2.5% 97.5% ## 0.1933055 0.4555422 Example 5.6 The time until lorry drivers react (in milliseconds) to an obstacle in the road is y &lt;- c(0.34, 0.47, 0.58, 0.27, 0.74, 0.44, 0.46, 0.65, 0.36, 0.55, 0.58, 0.55, 0.53, 0.56, 0.54, 0.61, 0.43, 0.52, 0.45, 0.49, 0.32, 0.33, 0.47, 0.58, 0.34, 0.60, 0.59, 0.43, 0.57, 0.34) hist(y, main = &quot;&quot;, xlab = &quot;Reaction time (ms)&quot;) Assuming that for fixed \\(\\sigma^2 &gt; 0\\), \\(Y_i \\mid \\mu \\sim N(\\mu, \\sigma^2)\\) are independent and identically distributed for \\(i=1,...,n\\), by Bayes’ theorem, the posterior distribution is \\[ \\pi(\\mu \\mid \\boldsymbol{y}, \\sigma^2) \\propto \\pi(\\boldsymbol{y} \\mid \\mu, \\sigma^2) \\pi(\\mu). \\] One of the issues here is that we have assigned a normal prior distribution to the population mean parameter \\(\\mu\\). The advantage previously was that we could derive a posterior distribution with closed form. The disadvantage however is that the choice of prior distribution assigns some positive probability to impossible values of \\(\\mu\\), i.e. reaction times less than zero. Now we have a tool to sample from posterior distributions that don’t have a closed form. We can instead assign an exponential prior distribution, a distribution which only has non-negative support. Letting \\(\\mu \\sim \\textrm{Exp}(10)\\) sets a vague prior distribution on \\(\\mu\\). It can be shown that the posterior distribution (exercise) is therefore \\[ \\pi(\\mu \\mid \\boldsymbol{y}, \\sigma^2) \\propto \\exp\\left\\{-10\\mu -\\sum_{i=1}^{30}\\frac{(y_i - \\mu)^2}{\\sigma^2}\\right\\} \\] We can use the Metropolis–Hastings algorithm to sample from this posterior distribution. But how should we propose new value of \\(\\mu\\)? A common method is a Metropolis–Hastings Random Walk proposal distribution. The proposal distribution is symmetric and centered on \\(\\mu\\). The two most common methods are \\(\\mu&#39; \\mid \\mu \\sim U[\\mu - \\varepsilon, \\mu + \\varepsilon]\\) and \\(\\mu&#39; \\mid \\mu \\sim N(\\mu, \\tau^2)\\). We choose the uniform proposal distribution, with \\[ q(\\mu&#39; \\mid \\mu) = \\frac{1}{2\\varepsilon}. \\] The acceptance probability is therefore \\[ p_\\textrm{acc} = \\min\\left\\{\\frac{\\exp\\left\\{-10\\mu&#39; -\\sum_{i=1}^{30}\\frac{(y_i - \\mu&#39;)^2}{\\sigma^2}\\right\\} }{\\exp\\left\\{-10\\mu -\\sum_{i=1}^{30}\\frac{(y_i - \\mu)^2}{\\sigma^2}\\right\\} }, 1\\right\\} \\] We can implement a sampler in R as follows: #Set up elements for MCMC set.seed(123) #to reproduce n.iter &lt;- 10000 mu.store &lt;- numeric(n.iter) #Initial values mu &lt;- 1 sigma &lt;- 0.1 #known for(i in 1:n.iter){ #Propose value for mu mu.proposed &lt;- runif(1, mu - 0.01, mu + 0.01) if(mu.proposed &gt; 0){ #If mu &lt; 0 we can reject straight away #Compute (log) acceptance probability log.numerator &lt;- -10*mu.proposed - sum(y - mu.proposed)^2/(2*sigma^2) log.denominator &lt;- -10*mu - sum(y - mu)^2/(2*sigma^2) log.p.acc &lt;- log.numerator - log.denominator u &lt;- runif(1) #Accept/Reject step if(log(u) &lt; log.p.acc){ mu &lt;- mu.proposed } } #Store mu at each iteration mu.store[i] &lt;- mu } plot(mu.store, type = &#39;l&#39;, xlab = &quot;iteration&quot;, ylab = expression(mu)) We can see that after about 300 iterations, the Markov chain has converged to its stationary distribution, the posterior distribution. We can see this more clearly by removing the first 300 iterations. plot(mu.store[-c(1:300)], type = &#39;l&#39;, xlab = &quot;iteration&quot;, ylab = expression(mu)) hist(mu.store[-c(1:300)], xlab = expression(mu), main = &quot;Posterior distribution&quot;) The 95% credible interval for \\(\\mu\\) using this prior distribution is quantile(mu.store[-c(1:300)], c(0.025, 0.975)) ## 2.5% 97.5% ## 0.4831480 0.4960669 Using a normal prior distribution, it was 0.486 0.493 It seems that the posterior distribution is very similar when using these two prior distributions. This is because the data are very informative. Example 5.7 In Lab 3.9, we computed the MAP estimate for a parameter from the Pareto distribution. The density function of this distribution is \\[ \\pi(x \\mid \\beta) = \\frac{\\beta}{x^{\\beta + 1}}, \\quad x &gt; 1. \\] Placing a Gamma prior distribution on \\(\\beta\\) such that \\(\\beta \\sim \\Gamma(a, b)\\). The posterior distribution given the data \\(\\boldsymbol{y} = \\{y_1, \\ldots, y_N\\}\\) is \\[ \\pi(\\beta \\mid \\boldsymbol{y}) \\propto \\frac{\\beta^{N + a - 1}e^{-b\\beta}}{\\prod y_i^{\\beta + 1}}. \\] We can’t sample from this directly, so wee need to use a Metropolis–Hastings algorithm to generate samples from the posterior distribution. We will use a normal proposal distribution. The acceptance probability is \\[ p_{acc} = \\min \\left\\{1, \\frac{\\beta&#39;^{N + a - 1}e^{-b\\beta&#39;}}{\\prod y_i^{\\beta&#39; + 1}}\\frac{\\prod y_i^{\\beta + 1}}{\\beta^{N + a - 1}e^{-b\\beta}} \\right\\} \\\\ = \\min \\left\\{1, \\left(\\frac{\\beta&#39;}{\\beta}\\right)^{N + a - 1}{\\prod y_i^{\\beta - \\beta&#39;}}\\exp((\\beta - \\beta&#39;)b) \\right\\}. \\] The MCMC algorithm will be Set an initial value \\(\\beta_0\\) and set \\(i =0\\). Propose a new value \\(\\beta&#39; \\sim N(\\beta_i, \\sigma^2)\\) Accept \\(\\beta&#39;\\) with probability \\(p_{acc}\\) and set \\(\\beta_{i+1}= \\beta&#39;\\), otherwise reject and set \\(\\beta_{i+1}= \\beta_i\\). Repeat steps 2, 3, and 4 for \\(i = 1, \\ldots, n-1\\). We fix \\(b=0.01\\) and use the data x &lt;- c(1.019844, 1.043574, 1.360953, 1.049228, 1.491926, 1.192943, 1.323738, 1.262572, 2.034768, 1.451654) to code up our algorithm. #Function that evaluates Pareto loglikelihood log.likelihood &lt;- function(x, beta){ log.value &lt;- length(x)*log(beta) - (beta + 1)*sum(log(x)) return(log.value) } # MCMC Sampler ------------------------------------------------------------ #Initialise Values x &lt;- c(1.019844, 1.043574, 1.360953, 1.049228, 1.491926, 1.192943, 1.323738, 1.262572, 2.034768, 1.451654) n.iter &lt;- 10000 #number of iterations beta.current &lt;- 2 #initial value for beta beta.store &lt;- numeric(n.iter) #empty vecotr to store beta at each iteration #Run MCMC For Loop for(i in 1:n.iter){ #Propose prop value for beta beta.prop &lt;- rnorm(1, beta.current, 0.5) #Compute current and prop loglikelihood loglike.prop &lt;- log.likelihood(x, beta.prop) loglike.current &lt;- log.likelihood(x, beta.current) #Compute Log acceptance probability log.p.acc &lt;- loglike.prop - loglike.current + dgamma(beta.prop, 1, 0.01, log = TRUE) - dgamma(beta.current, 1, 0.01, log = TRUE) #Accept/Reject u &lt;- runif(1) if(log(u) &lt; log.p.acc){ beta.current &lt;- beta.prop } #Store Current Value beta.store[i] &lt;- beta.current } #Plot trace plots plot(beta.store, type = &#39;l&#39;) #Investigate posterior hist(beta.store, freq = FALSE, main = &quot;&quot;, xlab = expression(beta)) quantile(beta.store, c(0.025, 0.975)) ## 2.5% 97.5% ## 2.102268 7.027826 5.3 Gibbs Sampler When we can sample directly from full conditional distributions, we can use a Gibbs sampler. Suppose we have a distribution with parameters \\(\\{\\theta_1, \\ldots, \\theta_N\\}\\), a Gibbs sampler works as follows: Set initial values \\(\\{\\theta_1^{(0)}, \\ldots, \\theta_N^{(0)}\\}\\) Set \\(i = 1\\). Draw a value for \\(\\theta_1^{(i)}\\) from \\(\\pi(\\theta_1 \\mid \\theta_2^{(i-1)}, \\ldots, \\theta_N^{(i-1)})\\). Draw a value for \\(\\theta_2^{(i)}\\) from \\(\\pi(\\theta_2 \\mid \\theta_1^{(i)}, \\theta_3^{(i-1)}, \\ldots, \\theta_N^{(i-1)})\\). \\(\\vdots\\) Draw a value for \\(\\theta_N^{(i)}\\) from \\(\\pi(\\theta_N \\mid \\theta_1^{(i)}, \\theta_2^{(i)}, \\ldots, \\theta_{N-1}^{(i)})\\). Repeat step 2 for \\(i = 2, \\ldots M\\). In code, this might look like M #number of iterations N #number of parameters theta.store &lt;- matrix(NA, N, M) theta &lt;- numeric(N) for(j in 1:M){ for(j in 1:N){ theta[i] &lt;- #sample from conditional with theta[-i] } theta.store[, j] &lt;- theta.current #store current values } Example 5.8 In Example 3.5, we had a hierarchical model with \\[\\begin{align*} \\boldsymbol{y} \\mid \\lambda &amp;\\sim \\hbox{Exp}(\\lambda) &amp; \\textrm{(likelihood)} \\\\ \\lambda \\mid \\gamma &amp;\\sim \\hbox{Exp}(\\gamma) &amp; \\textrm{(prior distribution)} \\\\ \\gamma \\mid \\nu &amp;\\sim \\hbox{Exp}(\\nu) &amp; \\textrm{(hyperprior distribution)} \\\\ \\end{align*}\\]. To derive the full conditional distributions, we only consider the terms in the posterior distributions that depends on the parameters we are interested in. The full conditional distribution for \\(\\lambda\\) is \\[ \\pi(\\lambda \\mid \\boldsymbol{y}, \\,\\gamma) \\propto \\lambda^{10}e^{-\\lambda(95 + \\gamma)}. \\] This is unchanged and shows that \\(\\lambda \\mid \\boldsymbol{y}, \\gamma \\sim \\textrm{Gamma}(11, 95 + \\gamma)\\). The full conditional distribution for \\(\\gamma\\) is \\[ \\pi(\\gamma \\mid \\boldsymbol{y}, \\,\\lambda) \\propto e^{-\\nu\\gamma}. \\] Therefore the full conditional distribution of \\(\\gamma\\) is \\(\\gamma \\mid \\boldsymbol{y}, \\,\\lambda \\sim \\hbox{Exp}(\\lambda + \\nu)\\). We can set up a Metropolis–Hastings algorithm using Gibbs samplers to generate samples for \\(\\lambda\\) and \\(\\gamma\\). Set initial values \\(\\{\\lambda^{(0)}, \\gamma^{(0)}\\}\\) Set \\(i = 1\\). Draw a value for \\(\\lambda^{(i)} \\mid \\boldsymbol{y}, \\gamma^{(i-1)} \\sim \\textrm{Gamma}(10, 95 + \\gamma^{(i-1)})\\) Draw a value for \\(\\gamma^{(i)} \\mid \\boldsymbol{y}, \\,\\lambda^{(i)} \\sim \\hbox{Exp}(\\lambda^{(i)} + \\nu)\\). Repeat steps 3 and 4 for \\(i = 2, \\ldots M\\). We can now code this up and run the algorithm. # Set Up MCMC Algorithm --------------------------------------------------- n.iter &lt;- 10000 lambda.store &lt;- numeric(n.iter) #Store value of Markov chain at end of every iteration gamma.store &lt;- numeric(n.iter) #Store value of Markov chain at end of every iteration # Run MCMC Algorithm ------------------------------------------------------ for(i in 2:n.iter){ #Store current value of Markov Chain lambda.store[i] &lt;- rgamma(1, 10, 95 + gamma.store[i-1]) gamma.store[i] &lt;- rexp(1, 0.01 + lambda.store[i]) } #Plot trace plot (Markov chain values) plot(lambda.store, type = &#39;l&#39;, xlab = &quot;iteration&quot;, ylab = expression(lambda)) plot(gamma.store, type = &#39;l&#39;, xlab = &quot;iteration&quot;, ylab = expression(gamma)) #Plot posterior density hist(lambda.store, prob = TRUE, xlab = expression(lambda), main = &quot;Posterior density&quot;) mean(lambda.store) #posterior mean ## [1] 0.09561264 quantile(lambda.store, c(0.025, 0.975)) #95% CI ## 2.5% 97.5% ## 0.0454177 0.1658069 hist(gamma.store, prob = TRUE, xlab = expression(lambda), main = &quot;Posterior density&quot;) mean(gamma.store) #posterior mean ## [1] 10.4679 quantile(gamma.store, c(0.025, 0.975)) #95% CI ## 2.5% 97.5% ## 0.2437085 40.4251992 #Investogate correlation between parameters plot(lambda.store, gamma.store) 5.4 Metropolis-within-Gibbs Now we have both the Metropolis–Hastings algorithm and Gibbs sampler, we can combine them to create a generic MCMC algorithm for essentially any posterior distribution with any number of parameters. To create our MCMC algorithm, we update any parameters where the full conditional distribution has closed form with a Gibbs sampler. For parameters where the full conditional distribution does not have a closed form, we use a Metropolis–Hastings algorithm to update the parameters. Example 5.9 Suppose \\(X_1, \\ldots, X_N \\sim \\hbox{Weibull}(\\beta, \\theta)\\), where \\[ \\pi(x \\mid\\beta,\\theta) = \\frac{\\beta}{\\theta}x^{\\beta - 1}\\exp\\left(-\\frac{x^\\beta}{\\theta}\\right), \\qquad x, \\beta, \\theta &gt; 0. \\] We use an Exponential prior distribution with rate \\(\\lambda\\) on \\(\\beta\\) and an inverse gamma prior distribution on \\(\\theta\\) such that \\[ \\pi(\\theta) = \\frac{1}{\\theta^{a - 1}}\\exp\\left(-\\frac{b}{\\theta}\\right). \\] The posterior distribution is therefore \\[\\begin{align*} \\pi(\\beta, \\theta \\mid \\boldsymbol{x}) &amp;\\propto \\pi(\\boldsymbol{x} \\mid \\beta, \\theta)\\pi(\\beta)\\pi(\\theta) \\\\ &amp;\\propto \\frac{\\beta^N}{\\theta^N}\\prod x_i^{\\beta - 1}\\exp\\left(-\\frac{1}{\\theta}\\sum x_i^\\beta\\right) \\\\ &amp;\\times\\exp(-\\lambda\\beta) \\frac{1}{\\theta^{a - 1}}\\exp\\left(-\\frac{b}{\\theta}\\right) \\end{align*}\\] The full conditional distributions are therefore \\[\\begin{align*} \\pi(\\beta \\mid \\theta, \\boldsymbol{x}) &amp;\\propto\\beta^N\\prod x_i^{\\beta - 1}\\exp\\left(-\\frac{1}{\\theta}\\sum x_i^\\beta\\right)\\exp(-\\lambda\\beta) \\\\ \\pi(\\theta \\mid \\beta, \\boldsymbol{x}) &amp; \\frac{1}{\\theta^{N + a -1}}\\exp\\left(-\\frac{1}{\\theta}(b + \\sum x_i^\\beta)\\right) \\end{align*}\\] There is no closed form for the full conditional distribution for \\(\\beta\\), so we will need to use a Metropolis–Hastings algorithm to update this parameter in our MCMC algorithm. The full conditional distribution for \\(\\theta\\) is closed as it is proportional to an inverse Gamma distribution with shape \\(N + a\\) and scale \\(b + \\sum x_i^\\beta\\). We can use a Gibbs sampler to update value for \\(\\theta\\). A suitable MCMC algorithm will look like Set initial values for \\(\\beta^{(0)}\\) and \\(\\theta^{(0)}\\) and \\(i = 1\\). Propose a new value for \\(\\beta\\), \\(\\beta&#39; \\sim U[\\beta^{(i-1)} + \\varepsilon, \\beta^{(i-1)} - \\varepsilon]\\) Accept \\(\\beta&#39; = \\beta^{(i)}\\) with probability \\[ p_{\\textrm{acc}} = \\min\\left\\{\\frac{\\pi(\\beta&#39;, \\theta^{(i-1)} \\mid \\boldsymbol{x})}{\\pi(\\beta, \\theta^{(i-1)} \\mid \\boldsymbol{x})}\\frac{q(\\beta^{(i-1)} \\mid \\beta&#39;)}{q(\\beta&#39; \\mid \\beta^{(i-1)})} , 1\\right\\} \\] Otherwise reject \\(\\beta&#39;\\) and set \\(\\beta^{(i)} = \\beta^{(i-1)}\\). Sample \\(\\theta^{(i)} \\sim \\hbox{inv}-\\Gamma(N + a,\\, b + \\sum x_i^{\\beta^{(i)}})\\). Repeat steps 2-4. The acceptance probability in step 3 is the ratio of the full conditional distributions for \\(\\beta\\). 5.5 MCMC Diagnostics When running an MCMC algorithm, it is always important to check that the Markov chain has converged and is mixing well. For our purposes, mixing well means the chain is exploring the space of possible values of \\(\\theta\\) effectively and effectively and not getting stuck on the same value for a long time. A key way of doing this is by looking at the trace plot, which is a time series of the posterior samples simulated by the algorithm. The trace plot should look like it has converged to the stationary distribution and exploring the stationary distribution efficiently. What it shouldn’t look like is a long series of small steps, or being stuck in one spot for a long time. There are two definitions that help us isolate an efficient Markov chain. Definition 5.11 The burn-in period is the number of iterations the Markov chain takes to reach the stationary distribution. Definition 5.12 The thinning parameter is the period of iterations of the Markov chain that are stored. Example 5.10 In Example 5.6, we saw a Markov chain that mixes well. We took the burn-in period to be 3,000 iterations, which was how long it took to for the chain to converge. Although the posterior distribution is invariant to the choice of the proposal distribution, it still has a large effect of the efficiency of the algorithm and how well the chain mixes. The ideal trace plot looks like white noise, or a hairy caterpillar. In a Metropolis–Hastings random walk algorithm, the proposal distribution often has a large impact on how well the Markov chain mixes. The variance, or step size, of the proposal distribution can be tuned to ensure the chain mixes well. The following two examples show poorly mixing Markov chains. The first is where the step size is too big and the chain frequently gets stuck for several hundred iterations. set.seed(123) #to reproduce n.iter &lt;- 10000 mu.store &lt;- numeric(n.iter) #Initial values mu &lt;- 1 sigma &lt;- 0.1 #known for(i in 1:n.iter){ #Propose value for mu mu.proposed &lt;- runif(1, mu - 0.1, mu + 0.1) #Step size too big if(mu.proposed &gt; 0){ #If mu &lt; 0 we can reject straight away #Compute (log) acceptance probability log.numerator &lt;- -0.01*mu.proposed - sum(y - mu.proposed)^2/(2*sigma^2) log.denominator &lt;- -0.01*mu - sum(y - mu)^2/(2*sigma^2) log.p.acc &lt;- log.numerator - log.denominator u &lt;- runif(1) #Accept/Reject step if(log(u) &lt; log.p.acc){ mu &lt;- mu.proposed } } #Store mu at each iteration mu.store[i] &lt;- mu } plot(mu.store[-c(1:3000)], type = &#39;l&#39;, xlab = &quot;iteration&quot;, ylab = expression(mu)) The next is where the step size is too small. It takes a long time for the chain to converge (~50% of the run time). When the chain does converge, it is inefficient at exploring the space. set.seed(123) #to reproduce n.iter &lt;- 10000 mu.store &lt;- numeric(n.iter) #Initial values mu &lt;- 1 sigma &lt;- 0.1 #known for(i in 1:n.iter){ #Propose value for mu mu.proposed &lt;- runif(1, mu - 0.0005, mu + 0.0005) #Step size too small if(mu.proposed &gt; 0){ #If mu &lt; 0 we can reject straight away #Compute (log) acceptance probability log.numerator &lt;- -0.01*mu.proposed - sum(y - mu.proposed)^2/(2*sigma^2) log.denominator &lt;- -0.01*mu - sum(y - mu)^2/(2*sigma^2) log.p.acc &lt;- log.numerator - log.denominator u &lt;- runif(1) #Accept/Reject step if(log(u) &lt; log.p.acc){ mu &lt;- mu.proposed } } #Store mu at each iteration mu.store[i] &lt;- mu } par(mfrow = c(1, 2)) plot(mu.store, type = &#39;l&#39;, xlab = &quot;iteration&quot;, ylab = expression(mu)) plot(mu.store[-c(1:5000)], type = &#39;l&#39;, xlab = &quot;iteration&quot;, ylab = expression(mu)) The final diagnostic issue we are going to think about is the curse of dimensionality. In general, the more parameters we try and update, the less likely we are to accept them. This makes exploring the proposal distribution hard if we are trying to update lots of parameters simultaneously. We can see this if we consider a hypersphere :::{.example} Suppose we have a density function that is uniformly distributed over the area of a sphere in \\(N\\) dimensions with radius \\(r\\). The sphere has volume \\[ V = \\frac{\\pi^{N/2}}{\\Gamma(\\frac{N}{2} + 1)}r^N. \\] Now consider a smaller sphere inside of our original sphere. This still has dimension \\(N\\) but has radius \\(r_1 &lt; r\\). This small sphere has volume \\[ V_1 = \\frac{\\pi^{N/2}}{\\Gamma(\\frac{N}{2} + 1)}r_1^N. \\] The difference between these two volumes is \\[ V - V_1 = \\frac{\\pi^{N/2}}{\\Gamma(\\frac{N}{2} + 1)}(r^N - r_1^N). \\] For large \\(N\\), even when \\(r - r_1\\) is small, \\((r^N - r_1^N)\\) is large. This means that lots of the probability mass is concentrated away from the mode into the outer shell of the sphere. The hypersphere example shows that in large dimensions we need our Markov chain to spend lots of time away from the posterior mode and in the tails of the distribution, but this is where the proposal distribution has lowest mass. We can avoid this by updating each parameter individually, but this means we need to use the full conditional distributions, which have highest mass near the mode. This ‘curse’ makes MCMC algorithms inefficient for large dimensions. 5.6 Beyond MCMC MCMC is not the only method available to generate samples from the posterior distribution. MCMC is often slow and inefficient. Much of the work in computational statistics research is about developing fast and efficient methods for sampling from posterior distributions. We are going to look at another method, called approximate Bayesian computation, in the next chapter. Two other methods, beyound the scope of this module, are Sequential Monte Carlo and Hamiltonian Monte Carlo. Sequential Monte Carlo (SMC) methods for Bayesian inference aim to estimate the posterior distribution of a state space model recursively based on the observed data. Initially, a set of particles representing possible states is sampled from the prior distribution. Then particles are propagated forward using the system dynamics and updated according to their likelihood given the observed data. This update step involves reweighting particles based on how well they explain the observed data. To ensure that the particle set accurately represents the posterior distribution, resampling is performed, where particles with higher weights are more likely to be retained. By iteratively repeating these steps, SMC effectively tracks the evolution of the posterior distribution over time, providing a flexible and computationally efficient framework for Bayesian inference in dynamic systems. Hamiltonian Monte Carlo (HMC) is a sophisticated Markov chain Monte Carlo (MCMC) method for sampling from complex, high-dimensional target distributions, commonly used in Bayesian inference. Unlike traditional MCMC methods, which rely on random walk proposals, HMC employs Hamiltonian dynamics to guide the exploration of the state space. By introducing auxiliary momentum variables, HMC constructs a joint distribution over the original target variables and the momentum variables. This augmented space enables the use of Hamiltonian dynamics, which can efficiently explore the target distribution by simulating the evolution of the system’s energy function. The key idea is to use the gradient of the target distribution’s log-probability to determine the momentum dynamics, leading to more effective proposals that can traverse the state space more efficiently. HMC samples are obtained by simulating Hamiltonian dynamics over a trajectory and then accepting or rejecting the proposed states based on Metropolis–Hastings criteria. Overall, HMC offers significant improvements in exploration efficiency compared to traditional MCMC methods, particularly in high-dimensional spaces, making it a powerful tool for Bayesian inference. 5.7 Lab Exercise 5.1 Consider the Langevin Markov chain with potential \\(V(x) = \\lvert x \\rvert^3\\) in dimension \\(d=1\\). Simulate \\(10,000\\) iterates of the Markov chain for different step sizes \\(h\\) and compare the histogram to a plot of the target density \\(\\pi(x) \\propto \\exp(-V(x))\\). Also experiment with different choices for \\(V\\). Exercise 5.2 You observe the following draws from a Binomial distribution with 25 trials and probability of success \\(p\\). y &lt;- c(20, 16, 20, 17, 18, 19, 19, 18, 21, 20, 19, 22, 23, 19, 20, 19, 21, 20, 25, 15) Use a normal prior distribution with mean 0.5 and variance \\(0.1^2\\). Write a Metropolis–Hastings Random Walk algorithm to obtain samples from the posterior distribution (you can use R’s built in function for the likelihood function and prior distribution, but if you don’t take logs you will run into small number errors). Exercise 5.3 In a medical trial, to investigate the proportion \\(p\\) of the population who have a particular disease a random sample of 20 individuals is taken. Ten of these are subject to a diagnostic test (Test A) which detects the disease when it is present with 100% certainty. The remaining 10 are given a test (test B) which only detects the disease with probability 0.8 when it is present. Neither test can give a false positive result. before collecting the data your prior belief about \\(p\\) is represented by a U(0,1) distribution. Suppose that, for Test A, 5 out of 10 test positive while for test B, 3 out of 10 test positive. Use an MCMC algorithm to investigate the posterior density of \\(p\\) and estimate its posterior mean and variance. Exercise 5.4 Code up an MCMC algorithm for Example 3.5 using Gibbs samplers. Exercise 5.5 The density function for the inverse-gamma distribution is \\[ \\pi(x\\mid \\alpha, \\beta) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha - 1}e^{-\\beta/x} \\] Using independent Gamma prior distributions on the model parameters, \\(\\alpha \\sim \\Gamma(a, b)\\) and \\(\\beta \\sim Gamma(c, d)\\), write down the posterior distribution for the model parameters. Only one will have a closed form. Develop a code an MCMC algorithm to sample from the posterior distribution by alternating between sampling \\(\\alpha\\) and then \\(\\beta\\). Simulate some data from the inverse-gamma distribution and see if you can recover the parameters used to simulate the data. Is there any correlation between the samples for \\(\\alpha\\) and \\(\\beta\\). References Hairer, Martin, and Jonathan C. Mattingly. 2011. “Yet Another Look at Harris’ Ergodic Theorem for Markov Chains.” In Seminar on Stochastic Analysis, Random Fields and Applications VI, 63:109–17. Progr. Probab. Birkhäuser/Springer Basel AG, Basel. https://doi.org/10.1007/978-3-0348-0021-1_7. Meyn, Sean, and Richard L. Tweedie. 2009. Markov Chains and Stochastic Stability. Second. Cambridge University Press, Cambridge. https://doi.org/10.1017/CBO9780511626630. "],["advanced-computation.html", "Chapter 6 Advanced Computation 6.1 Gaussian Processes 6.2 Data Augmentation 6.3 Prior Ellicitation (Optional reading) 6.4 Lab", " Chapter 6 Advanced Computation Now we have the tools of Bayesian inference and methods to sample from complex posterior distributions, we can start to look at more advanced methods and models. This chapter is split into distinct parts, each showing a different method in Bayesian inference. 6.1 Gaussian Processes So far in the module, we have considered prior distribution on parameters. These parameters have taken values (mostly real) or real-valued vectors. In this section, we’re going to extend this idea further to place prior distributions on functions. That is, we’re going to describe a prior distribution that when sampled gives us functions. The method we’re going to use is called a Gaussian Process (GP). Before, we define a GP, we’re going to build an intuitive definition of it. Recall the normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\), \\(N(\\mu, \\sigma^2)\\). It assigns probabilities to values on the real line – when we sample from it, we get real values. The plot below shows the density function for a \\(N(0, 1)\\) distribution and five samples. #Plot N(0, 1) x &lt;- seq(-4, 4, 0.01) y &lt;- dnorm(x) plot(x, y, type = &#39;l&#39;) #Add samples samples &lt;- rnorm(5) rug(samples) The multivariate normal distribution extends this to the vector space \\(\\mathbb{R}^N\\). Instead of having a mean and variance value, the distribution is defined through a mean vector and covariance matrix. The mean vector describes the expected value of each component of the vector and the covariance matrix describes the relationship between each pair of components in the vector. When we draw samples, we get vectors. The plot below shows the density of the multivariate normal distribution with \\(N = 2\\), zero mean, \\(\\sigma^2_x = \\sigma^2_y = 1\\) and \\(\\rho = 0.7\\). #Create Grid x &lt;- seq(-3,3,length.out=100) y &lt;- seq(-3,3,length.out=100) #Evaluate density at grid z &lt;- matrix(0,nrow=100,ncol=100) mu &lt;- c(0,0) sigma &lt;- matrix(c(1, 0.7, 0.7, 1),nrow=2) for (i in 1:100) { for (j in 1:100) { z[i,j] &lt;- mvtnorm::dmvnorm(c(x[i],y[j]), mean=mu,sigma=sigma) } } #Generate contour plot contour(x, y ,z) A GP takes this one step further and puts a prior distribution on a function space. It is specified by a mean function, \\(\\mu(\\cdot)\\) and covariance function \\(k(\\cdot, \\cdot)\\). The mean function describes the expected value of each point the function can be evaluated at, and the covariance function describes the relationship between each point on the function. The plot below shows three samples from a GP distribution with mean function the zero function \\(\\mu(x) = 0\\, \\forall x\\) and a covariance function that supports smooth functions. Definition 6.1 A Gaussian Process \\(X = (X(t))_{t \\in T}\\) is a collection of real-valued random variables indexed by some index set \\(T\\), any finite number of which have a joint Gaussian distribution. Its mean function \\(T \\ni t \\mapsto \\mu(t)\\) is defined by \\(\\mu(t) = \\mathbb{E}[X(t)]\\) and its covariance function \\(k(s,t) = \\mathrm{Cov}(X(s), X(t)) = \\mathbb{E}\\left((X(s) - \\mu(s))(X(t) - \\mu(t))\\right)\\). It can be shown that a Gaussian process is uniquely characterised by its mean and covariance function, just as a multivariate normal distribution is uniquely characterised by its mean vector and covariance matrix. Our particular interest is on Gaussian processes on real valued functions defined on \\(\\mathbb{R}^N\\), and therefore for the particular index set \\(T = \\mathbb{R}^N\\). In this case we use the notation \\((f(x))_{x \\in \\mathbb{R}^N}\\) for such a random function, which we formalise as follows: Definition 6.2 A GP on a function \\(f = (f(x))_{x \\in \\mathbb{R}^N}\\) is defined through its mean function \\(\\mu(x) = \\mathbb{E}[f(x)]\\) and covariance function \\(k(x, x&#39;) = \\mathrm{Cov}(f(x),f(x^\\prime)) = \\mathbb{E}\\left[(f(x) - \\mu(x))(f(x&#39;) - \\mu(x&#39;))\\right]\\), where \\(x,x^\\prime \\in \\mathbb{R}^N\\). We write this as \\(f \\sim \\mathcal{GP}(\\mu, k)\\). In particular, for such a Gaussian process \\(f\\), we have that \\[\\forall n \\in \\mathbb{N}, \\boldsymbol{x} = (x_1,\\ldots x_n) \\in (\\mathbb{R}^N)^n: f(\\boldsymbol{x}) := (f(x_1),\\ldots,f(x_n)) \\sim N\\big(\\mu(\\boldsymbol{x}) , k(\\boldsymbol{x},\\boldsymbol{x})\\big),\\] where \\(\\mu(\\boldsymbol{x}) := (\\mu(x_1),\\ldots, \\mu(x_n))\\) and \\(k(\\boldsymbol{x},\\boldsymbol{x}) := (k(x_i,x_j))_{i,j = 1,\\ldots, n}\\). Before we go any further, it is worth proceeding with caution. Those with good memories will recall Bernstein-von-Mises’ theorem from Chapter 3. Theorem 6.1 (Bernstein-von-Mises) For a well-specified model \\(\\pi(\\boldsymbol{y} \\mid \\theta)\\) with a fixed number of parameters, and for a smooth prior distribution \\(\\pi(\\theta)\\) that is non-zero around the MLE \\(\\hat{\\theta}\\), then \\[ \\left|\\left| \\pi(\\theta \\mid \\boldsymbol{y}) - N\\left(\\hat{\\theta}, \\frac{I(\\hat{\\theta})^{-1}}{n}\\right) \\right|\\right|_{TV} \\rightarrow 0. \\] Bernstein-von-Mises’ theorem only holds when the model has a fixed (i.e. finite) number of parameters. A GP is defined on an infinite collection of points, and so this theorem does not hold. This is the first time in this module we have encountered a distribution where Bernstein-von-Mises’ theorem does not hold. Fortunately, various forms of Bernstein-von-Mises’ theorems for GPs exist, with many coming about in the early 2010s. However, this is still an ongoing area of research. 6.1.1 Covariance Functions One issue when using GPs is describing the covariance function. How do we decide how each pair of points (there being an infinite number of them)? There are lots of standard choices of covariance functions that we can choose from, each one making different assumptions about the function we are interested in. The most common covariance function is the squared exponential functions. It is used to model functions that are ‘nice’, i.e. they are smooth, continuous and infinitely differentiable. Definition 6.3 The squared exponential covariance function takes the form \\[ k(x, x&#39;) = \\alpha^2\\exp\\left\\{-\\frac{1}{l^2}(x-x&#39;)^2\\right\\}, \\] where \\(\\alpha^2\\) is the signal variance and \\(l&gt;0\\) is the length scale parameter. For now, consider \\(\\alpha = l = 1\\). What is the covariance between the function evaluated at 0 and the function evaluated at \\(x\\)? The plot below shows the covariance. The covariance is highest when the \\(x\\) is near to 0, i.e. the points are immediately next to each other. If the value of \\(x\\) is \\(\\pm 2\\), the covariance is 0. As we are dealing with a joint normal distribution, a covariance of 0 implies independence. So with this covariance function, the value of \\(f(x)\\) is independent of \\(f(0)\\) if \\(|x|\\) is larger than about two. The parameter \\(l\\) is called the length scale parameter and dictates how quickly the covariance decays. Small values of \\(l\\) mean that the value of the function at nearby points are independent of each other, resulting in functions that look like white noise. Large values of \\(l\\) mean that even if points are far away, they are still highly dependent on each other. This gives very flat functions. The choice of covariance function is a modelling choice – it depends completely on the data generating process you are trying to model. The following properties are useful when deciding which covariance function to use. Definition 6.4 A stationary covariance function is a function of \\(x - x&#39;\\). That means it is invariant to translations in space. Definition 6.5 An isotropic covariance function is a function only of \\(|x - x&#39;|\\). That means it is invariant to rigid translations in space. Definition 6.6 An dot product covariance function is a function only of \\(x\\cdot x&#39;\\). That means it is invariant to rigid rotations in space, but not translations. What is most important is that the matrix resulting from a covariance function is positive semi-definite to be a valid choice for a Gaussian process. This is because covariance matrices must be positive semi-definite. More formally: Definition 6.7 An \\(n \\times n\\) matrix \\(\\Sigma\\) is positive semi-definite if it is symmetric and \\[ x^T\\Sigma x \\geq 0 \\quad \\hbox{for all } x \\in \\mathbb{R}^n. \\] A function \\(k\\colon \\mathbb{R}^N \\times \\mathbb{R}^N \\to \\mathbb{R}\\) is called a covariance function if for any \\(\\boldsymbol{x} = (x_1,\\ldots,x_n) \\in (\\mathbb{R}^N)^n\\) it holds that \\(k(\\boldsymbol{x},\\boldsymbol{x})\\) is positive semidefinite. The squared exponential covariance function is isotropic and produces functions that are continuous and differentiable. There are many other types of covariance functions, including ones that don’t produce functions that are continuous or differentiable. Three more are given below. Definition 6.8 The Matérn covariance function models functions that are differentiable only once: \\[ k(x, x&#39;) = \\left(1 + \\frac{\\sqrt{3}\\lvert x - x&#39;\\rvert}{l} \\right)\\exp\\left\\{-\\frac{\\sqrt{3}\\lvert x - x&#39;\\rvert}{l} \\right\\}. \\] Definition 6.9 The periodic covariance function models functions that are periodic and it is given by \\[ k(x, x&#39;) = \\alpha^2 \\exp\\left\\{-\\frac{2}{l}\\sin^2\\frac{\\lvert x-x&#39;\\rvert^2}{p} \\right\\}, \\] where the period is \\(p\\). Definition 6.10 The dot product covariance function models functions that are rotationally invariant and it is given by \\[ k(x, x&#39;) = \\alpha^2 + x\\cdot x&#39;. \\] These are just some covariance functions. In addition to the covariance functions defined, we can make new covariance functons by combining existing ones. Proposition 6.1 If \\(k_1\\) and \\(k_2\\) are covariance functions, then so is \\(k = k_1 + k_2\\). Proof. Let \\(x_1,\\ldots,x_n \\in \\mathbb{R}^N\\) and \\(a \\in \\mathbb{R}^n\\) be arbitrarily chosen. Then, \\[a^\\top k(\\boldsymbol{x},\\boldsymbol{x}) a = \\underbrace{\\boldsymbol{a}^\\top k_1(\\boldsymbol{x},\\boldsymbol{x}) \\boldsymbol{a}}_{\\geq 0} + \\underbrace{\\boldsymbol{a}^\\top k_2(\\boldsymbol{x},\\boldsymbol{x}) \\boldsymbol{a}}_{\\geq 0} \\geq 0, \\] so \\(k\\) is a covariance function. An alternative probabilistic proof would be this: let \\(f_1\\) and \\(f_2\\) be independent Gaussian processes with covariance functions \\(k_1\\) and \\(k_2\\) and zero mean function, respectively. For any \\(\\boldsymbol{x} = (x_1,\\ldots,x_n)\\) we then have \\(f_i(\\boldsymbol{x}) \\sim N(\\boldsymbol{0}, k_i(\\boldsymbol{x},\\boldsymbol{x}))\\) and therefore by independence of \\(f_1\\) and \\(f_2\\), \\[f_1(\\boldsymbol{x}) + f_2(\\boldsymbol{x}) \\sim N\\big(\\boldsymbol{0}, k_1(\\boldsymbol{x},\\boldsymbol{x}) + k_2(\\boldsymbol{x},\\boldsymbol{x})\\big) = N\\big(\\boldsymbol{0}, k(\\boldsymbol{x},\\boldsymbol{x})\\big),\\] so \\(k\\) is a covariance function. Proposition 6.2 If \\(k_1\\) and \\(k_2\\) are covariance functions, then so is \\(k_1k_2\\). Proof. See problem sheet. 6.1.2 Gaussian Process Regression One of the main applications of GPs in in regression. Suppose we observe the points below \\(\\boldsymbol{y} = \\{y_1, \\ldots, y_N\\}\\) and want to fit a curve through them. One method is to write down a set of functions of the form \\(\\boldsymbol{y} = X^T\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\), where \\(X\\) is the design matrix and \\(\\boldsymbol{\\beta}\\) a vector of parameters. For each design matrix \\(X\\), construct the posterior distributions for \\(\\boldsymbol{\\beta}\\) and use some goodness-of-fit measure to choose the most suitable design matrix. x &lt;- -5:5 y &lt;- sin(x/2)^2 + exp(-x/5) + rnorm(length(x), 0, 0.2) plot(x, y) One difficulty is writing down the design matrices \\(X\\), it is often not straightforward to propose or justify these forms GPs allow us to take a much less arbitrary approach, simply saying that \\(y_i = f(x_i) + \\varepsilon_i\\) and placing a GP prior distribution on \\(f\\). Although we’re placing an prior distribution with an infinite dimension on \\(f\\), we only ever need to work with a finite dimensional object, making this much easier. We only observe the function at finite number of points \\(\\boldsymbol{f} = \\{f(x_1), \\ldots, f(x_N)\\}\\) and we will infer the value of the function at points on a fine grid, \\(\\boldsymbol{f}^* = \\{f(x_1^*), \\ldots, f(x_N^*)\\}\\). By the definition of a GP, the distribution of these points is a multivariate normal distribution. Example 6.1 Suppose we observe \\(\\boldsymbol{y} = \\{y_1, \\ldots, y_N\\}\\) at \\(\\boldsymbol{x} = \\{x_1, \\ldots, x_N\\}\\). The plot below shows these points. Using the model \\(y_i = f(x_i) + \\varepsilon_i\\), where \\(\\varepsilon_i \\sim N(0, \\sigma^2)\\), we want to infer the function \\(f\\) evaluated at a gird of points \\(\\boldsymbol{f}^* = \\{f(x_1^*), \\ldots, f(x_N^*)\\}\\). We place a GP prior distribution on \\(f \\sim \\mathcal{GP}(0, k)\\), where \\(k\\) is the squared exponential covariance function. Using the model, the covariance between points \\(y_i\\) and \\(y_j\\) is \\[ \\textrm{cov}(y_i, y_j) = k(x_i, x_j) + \\sigma^21_{i=j}. \\] That is the covariance function evaluated at \\(x_i\\) and \\(x_j\\) plus \\(\\sigma^2\\) if \\(i = j\\). We can write this in matrix form as \\(K(\\boldsymbol{x}, \\boldsymbol{x}) + \\sigma^2I\\) where \\(I\\) is the identity matrix. The distribution of \\(\\boldsymbol{y}\\) is therefore \\(\\boldsymbol{y} \\sim N(\\boldsymbol{0}, \\, K(\\boldsymbol{x}, \\boldsymbol{x}) + \\sigma^2I)\\). By definition of the GP, the distribution of the function evaluated at the fine grid is \\(\\boldsymbol{f}^* \\sim N(\\boldsymbol{0}, K(\\boldsymbol{x}^*, \\boldsymbol{x}^*))\\). We can now write the joint distribution as \\[ \\begin{pmatrix} \\boldsymbol{y} \\\\ \\boldsymbol{f}^* \\end{pmatrix} \\sim N\\left(\\boldsymbol{0}, \\, \\begin{pmatrix} K(\\boldsymbol{x}, \\boldsymbol{x}) + \\sigma^2I &amp; K(\\boldsymbol{x}, \\boldsymbol{x}^*)\\\\ K(\\boldsymbol{x}^*, \\boldsymbol{x}) &amp; K(\\boldsymbol{x}^*, \\boldsymbol{x}^*) \\end{pmatrix}. \\right) \\] The off-diagonal terms in the covariance matrix describe the relationship between the observed points \\(\\boldsymbol{y}\\) and the points of interest \\(\\boldsymbol{f}^*\\). We can now write down the distribution of \\(\\boldsymbol{f}^*\\) given the observed points \\(\\boldsymbol{y}\\) and \\(\\sigma^2\\). \\[ \\boldsymbol{f}^* \\mid \\boldsymbol{y}, \\sigma^2 \\sim N(\\boldsymbol{\\mu}^*, \\, K^*), \\] where \\(\\boldsymbol{\\mu}^* = K(\\boldsymbol{x}^*, \\boldsymbol{x})(K(\\boldsymbol{x}, \\boldsymbol{x}) + \\sigma^2 I)^{-1} \\boldsymbol{y}\\) and \\(K^* = K(\\boldsymbol{x}^*, \\boldsymbol{x}^*) - K(\\boldsymbol{x}^*, \\boldsymbol{x})(K(\\boldsymbol{x}, \\boldsymbol{x}) + \\sigma^2I)^{-1}K(\\boldsymbol{x}, \\boldsymbol{x}^*)\\). We set the fine gird to be \\(\\boldsymbol{x}^* = \\{-5, -4.99, -4.98, \\ldots, 5\\}\\), the GP parameters \\(\\alpha = l = 1\\) and \\(\\sigma = 0.2\\). The posterior mean and 95% credible interval are shown below. The posterior mean for \\(f\\) is a smooth line passing near each point. The 95% credible interval for \\(f\\) has the smallest variance near each point, and largest furthest away from the points. 6.2 Data Augmentation Real world data are often messy with data points missing which may mean they are partially or completely unobserved. One common example of this is in clinical trials where people drop out of the trial before their treatment is complete. Another example is crime data, where only a fraction of crimes are reported and many crimes go unobserved. Two common ways to deal with partially or completely unobserved data are: Remove data points that are not completely observed. This throws away information and is likely to increase the overall uncertainty in the estimates due to the reduced sample size. Replace data points that are not completely observed with some estimates. This process is known as Data imputation. If we simply treat the imputed data as observed, then we are likely to underestimate the uncertainty as we are treating the observation as completely observed when it is not. The Bayesian framework provides a natural way for dealing with missing, partially, or completely unobserved data using the technique called data augmentation. It allows us to infer the missing/hidden information alongside the model parameters. In data augmentation, we distinguish between two likelihood functions. The observed data likelihood function is the likelihood function of the observed data. The complete data likelihood function is the likelihood function of the observed data and any unobserved data had they been fully observed. The difference between the two likelihood functions is that the complete data likelihood function is the function had we observed everything we want to observe. However, as the complete data likelihood function contains data we didn’t fully observe, we can’t compute it. Instead we can only evaluate the observed data likelihood function. Mathematically, we can write the complete data set \\(X = (X_1,\\dotsc,X_n)\\), with each \\(X_i = (Y_i, Z_i)\\), where \\(Y_i\\) denotes the observed information while \\(Z_i\\) denotes the unobserved/missing information. If we assume all the \\(X_i\\)’s are independent, then the complete (data) likelihood function is \\(\\pi(x \\mid \\theta) = \\prod_{i=1}^n\\pi(x_i\\mid \\theta) = \\prod_{i=1}^n\\pi(y_i,z_i\\mid \\theta)\\). Let \\(Y = (Y_1,\\dotsc,Y_n)\\) denote the observed data in the complete data set \\(X\\). The observed data likelihood function is instead \\(\\pi(y\\mid \\theta) = \\prod_{i=1}^n\\pi(y_i\\mid \\theta) = \\prod_{i=1}^n \\int \\pi(y_i,z_i\\mid \\theta) dz_i\\). In data augmentation, we start off with the observed data likelihood function and then augment this function by introducing hidden (latent) variables which denotes the unobserved data. This then gives us the complete data likelihood function. Throughout the examples that we shall discuss below, we illustrate the point that it is often easier to work with the complete data likelihood in the presence of unobserved data, because it allows the use of Gibbs sampler for simultaneous Bayesian Inference on model parameters and the missing information. 6.2.1 Censored observations The first example we will look at is when data is censored. Instead of throwing away these observations, we will instead treat them as random variables and infer their values. Example 6.2 A bank checks transactions for suspicious activities in batches of 1000. It checks five batches and observes \\(y_1, \\ldots, y_4\\) suspicious transactions in the first four batches. Due to a computer error, the number of suspicious transactions in the final batch is not properly recorded, but is known to be less than 6. It is reasonable to assume \\(Y_1,\\dotsc,Y_5 \\mid p \\overset{i.i.d}\\sim \\text{Binomial}(1000,p)\\), but we do not observe the exact value of \\(Y_5\\). The observed data likelihood functions is \\[\\begin{align*} \\pi(y_1, \\ldots, y_4, y_5 &lt; 6 \\mid p) &amp;=\\sum_{y_5 = 0}^5 \\pi(y_1,\\dotsc,y_4,y_5 \\mid p) = \\sum_{y_5 = 0}^5 \\left(\\prod_{i=1}^5 \\begin{pmatrix} 1000 \\\\ y_i \\end{pmatrix} p^{y_i}(1-p)^{1000 - y_i} \\right) \\\\ &amp;= \\left(\\prod_{i=1}^4\\begin{pmatrix} 1000 \\\\ y_i \\end{pmatrix} p^{y_i}(1-p)^{1000 - y_i} \\right)\\left(\\sum_{y_5=0}^5\\begin{pmatrix} 1000 \\\\ y_5 \\end{pmatrix} p^{y_5}(1-p)^{1000 - y_5}\\right). \\end{align*}\\] Placing a uniform prior distribution on \\(p \\sim \\text{Unif}[0, 1]\\) gives the posterior distribution \\[ \\pi(p \\mid y_1, \\ldots, y_4, y_5 &lt;6) \\propto \\left( p^{\\sum_{i=1}^4 y_i}(1-p)^{\\sum_{i=1}^4y_i} \\right)\\left(\\sum_{y_5=0}^5\\begin{pmatrix} 1000 \\\\ y_5 \\end{pmatrix} p^{y_5}(1-p)^{1000 - y_5}\\right). \\] Although we could sample from this distribution using Metropolis-Hastings algorithms, they do not produce any information regarding the censored data point \\(y_5\\), which may be of interest itself. Instead, we can write down the complete data likelihood by assuming that the exact value of \\(y_5\\) was observed: \\[ \\pi(y_1, \\ldots, y_5 \\mid p) = \\prod_{i=1}^5\\begin{pmatrix} 1000 \\\\ y_i \\end{pmatrix} p^{y_i}(1-p)^{1000 - y_i}. \\] With the same uniform prior, the posterior distribution is \\[ p \\mid y_1, \\ldots, y_5 \\sim \\hbox{Beta}\\left(\\sum_{i=1}^5 y_i + 1, 5000 + 1 - \\sum_{i=1}^5 y_i\\right). \\] The conditional distribution of \\(Y_5\\mid p, y_1, \\dotsc,y_4\\) can be computed using Bayes rule, with the additional constraint \\(Y_5 &lt; 6\\) \\[ \\pi( y_5 \\mid y_1, \\ldots, y_4,y_5&lt;6, p) = \\frac{\\pi(y_1,\\dotsc,y_5\\mid p)}{\\pi(y_1,\\dotsc,y_4, y_5 &lt; 6\\mid p)}=\\frac{\\begin{pmatrix} 1000 \\\\ y_5 \\end{pmatrix} p^{y_5}(1-p)^{1000 - y_5}}{\\sum_{j=0}^{5}\\begin{pmatrix} 1000 \\\\ j \\end{pmatrix} p^{j}(1-p)^{1000-j}}, \\] for \\(y_5 \\in \\{0,\\dotsc,5\\}\\). With these two easy-to-sample conditional distributions identified, we can use a Gibbs sampler alternating between sampling \\(p\\) and \\(y_5\\). 6.2.2 Latent Variables Latent variable are variables that are cannot be observed, these may be hidden somehow or introduced to help with the modelling. Mixture model is a major example where latent variables are particularly useful. Example 6.3 Royal Mail use image detection software to read postcodes on letters. A camera scans the front of an envelope and then records the barcode. This example is a very simplified version of how the system could work. Suppose the machine is processing a bag of letters addressed to people in either B1 or B2 postcodes. The camera scans the first two characters of the postcode (B1 or B2) and records the proportion of the scanned image that is taken up by the characters. The picture below shows an example of what the scanned image looks like. The scanned image is a 10 by 10 grid of pixels, where the number of pixels coloured black of the \\(i^{th}\\) image is \\(Y_i\\mid \\theta \\sim \\hbox{Binomial}(100, \\theta)\\). However, \\(\\theta\\) depends on if the letter is going to B1 or B2. To allow for this, we introduce a latent variable \\(Z_i \\in \\{1,2\\}\\) that describes if the characters on the \\(i^{th}\\) image are B1 or B2. In other words, one can think of \\(Z_i\\) as ‘membership’ of \\(Y_i\\) that we cannot observed. The observation \\(y_i\\) is the number of pixels of the \\(i^{th}\\) image that is coloured black. We observe \\(y_i\\), but want to infer its membership \\(Z_i\\). The difficultly is the lack of one-to-one correspondence between the values \\(y_i\\) can take and the value \\(z_i\\). Due to the different handwriting and fonts used on envelopes, if the letter is going to B1 (\\(Z_i = 1\\)), then \\(Y_i \\mid Z_i = 1 \\sim \\hbox{Binomial}(100, \\theta_1)\\) and if it is going to B2 (\\(Z_i = 2\\)), then \\(Y_i \\mid Z_i = 2 \\sim \\hbox{Binomial}(100, \\theta_2)\\). The plot below shows the two densities and the overlap between them for \\(\\theta_1 = 0.85\\) and \\(\\theta_2 = 0.9\\). a &lt;- 1:100 x &lt;- dbinom(a, 100, 0.9) y &lt;- dbinom(a, 100, 0.85) plot(a, x, type = &#39;l&#39;, xlab = expression(y), ylab = &quot;density&quot;, xlim = c(70, 100)) lines(a, y, lty = 2) Suppose we have \\(X_i = \\{Y_i,Z_i\\}\\) for \\(i = 1,\\dotsc,N\\), where the \\(Z_i\\)’s are latent variables that we do not observe. We assume \\(Z_i\\mid p \\overset{i.i.d.}\\sim 2-\\hbox{Bernoulli}(p)\\), so that \\(\\mathbb{P}(Z_i = 1\\mid p) = p\\) and \\(\\mathbb{P}(Z_i = 2\\mid p) = 1-p\\), where the parameter \\(p\\) describes how likely a letter is from B1 or B2. For each \\(i\\), we have \\[\\begin{align*} \\pi(y_i\\mid p, \\theta_1,\\theta_2) &amp;= \\pi(y_i\\mid Z_i = 1)\\mathbb{P}(Z_i = 1) + \\pi(y_i\\mid Z_i = 2)\\mathbb{P}(Z_i = 2) \\\\ &amp;= p \\,\\pi(y_i\\mid \\theta_1)+(1-p)\\,\\pi(y_i\\mid\\theta_2) \\end{align*}\\] Let \\(\\boldsymbol{Y} = (Y_1,\\dotsc,Y_n)\\) denote all the observed data and \\(\\boldsymbol{Z} = (Z_1,\\dotsc,Z_n)\\) denote all the unobserved data (memberships). The observed data likelihood function is therefore \\[\\begin{align*} \\pi(\\boldsymbol{y} \\mid p, \\theta_1, \\theta_2) &amp;=\\prod_{i=1}^N \\bigg[ p\\pi(y_i \\mid \\theta_1) + (1-p)\\pi(y_i \\mid \\theta_2)\\bigg] \\\\ &amp;=\\prod_{i=1}^N \\bigg[p \\begin{pmatrix} 100 \\\\ y_i \\end{pmatrix} \\theta_1^{y_i}(1-\\theta_1)^{100 - y_i} + (1-p)\\begin{pmatrix} 100 \\\\ y_i \\end{pmatrix} \\theta_2^{y_i}(1-\\theta_2)^{100 - y_i}\\bigg] \\end{align*}\\] Deriving full conditional distributions based on the above likelihood (even assuming uniform priors on \\(p,\\theta_1,\\theta_2\\)) is challenging due to the sum in the product of \\(N\\) terms. Instead, it’s easier to work with the complete data likelihood function, which is given by \\[\\begin{align*} \\pi(\\boldsymbol{y}, \\boldsymbol{z} \\mid p, \\theta_1, \\theta_2) &amp;=\\prod_{i=1}^N \\pi(y_i,z_i\\mid p, \\theta_1, \\theta_2)=\\prod_{i=1}^N \\pi(y_i\\mid z_i,p,\\theta_1,\\theta_2) \\pi(z_i\\mid p,\\theta_1,\\theta_2) \\\\ &amp;= \\prod_{i=1}^N \\begin{pmatrix} 100 \\\\ y_i \\end{pmatrix} \\theta_{z_i}^{y_i}(1-\\theta_{z_i})^{100 - y_i}\\mathbb{P}(Z_i = z_i) \\\\ &amp;= \\prod_{i=1}^N \\begin{pmatrix} 100 \\\\ y_i \\end{pmatrix} (\\theta_{1}^{y_i}(1-\\theta_{1})^{100 - y_i}p)^{\\mathbf{1}_{\\{z_i = 1\\}}}(\\theta_{2}^{y_i}(1-\\theta_{2})^{100 - y_i}(1-p))^{\\mathbf{1}_{\\{z_i = 2\\}}} \\end{align*}\\] \\[ \\small = \\bigg[\\prod_{i=1}^N \\begin{pmatrix} 100 \\\\ y_i \\end{pmatrix}\\bigg] \\bigg[\\theta_{1}^{\\sum_{i:z_i = 1}y_i}(1-\\theta_{1})^{\\sum_{i:z_i = 1}(100 -y_i)}p^{\\sum_{i=1}^N \\mathbf{1}_{\\{z_i = 1\\}}}\\bigg] \\bigg[\\theta_{2}^{\\sum_{i:z_i = 2}y_i}(1-\\theta_{2})^{ \\sum_{i:z_i = 2}(100 -y_i)}(1-p)^{\\sum_{i=1}^N \\mathbf{1}_{\\{z_i = 2\\}}}\\bigg]. \\] This form, although looks complicated, is actually much easier to derive the full conditional distributions. By Bayes’ theorem, the posterior distribution is \\[ \\pi(\\boldsymbol{z}, p, \\theta_1, \\theta_2 \\mid \\boldsymbol{y}) \\propto \\pi(\\boldsymbol{y}, \\boldsymbol{z} \\mid p, \\theta_1, \\theta_2) \\pi(p)\\pi(\\theta_1)\\pi(\\theta_2), \\] where we put some independent prior distributions on \\(p, \\theta_1,\\theta_2\\). Suppose we are ignorant about the value of \\(p\\) and place a uniform prior distribution on the parameter \\(p\\). The full conditional distribution is \\[\\begin{align*} \\pi(p\\mid \\boldsymbol{z}, \\boldsymbol{y}, \\theta_1, \\theta_2) &amp;\\propto \\pi(p,\\boldsymbol{z}, \\boldsymbol{y}, \\theta_1, \\theta_2) = \\pi(\\boldsymbol{y}, \\boldsymbol{z} \\mid p, \\theta_1, \\theta_2) \\pi(p)\\pi(\\theta_1)\\pi(\\theta_2) \\end{align*}\\] Hence, we just need to collect the terms that depend on \\(p\\) from the complete data likelihood, and we obtain \\[ \\pi(p\\mid \\boldsymbol{z}, \\boldsymbol{y}, \\theta_1, \\theta_2) \\propto p^{N_1}(1-p)^{N_2} \\] where \\(N_1 = \\sum_{i=1}^N \\mathbf{1}_{\\{z_i = 1\\}}\\) and \\(N_2 = N - N_1 = \\sum_{i=1}^N \\mathbf{1}_{\\{z_i = 2\\}}\\), which implies \\[ p \\mid \\boldsymbol{z} \\sim \\hbox{Beta}(N_1 + 1, N_2 + 1). \\] Note that we drop other variables as the distribution only depends on \\(\\boldsymbol{z}\\). If we use Beta(\\(\\alpha_1, \\beta_1\\)) as our prior distribution on \\(\\theta_1\\), the full conditional distribution of \\(\\theta_1\\) is \\[\\begin{align*} \\pi(\\theta_1 \\mid \\boldsymbol{z}, \\boldsymbol{y}, \\theta_2, p) \\propto \\pi(\\boldsymbol{y}, \\boldsymbol{z} \\mid p, \\theta_1, \\theta_2) \\pi(\\theta_1) &amp;\\propto \\theta_1^{\\sum_{i; z_i = 1}y_i}(1-\\theta_1)^{ \\sum_{i; z_i = 1}(100-y_i)}\\theta_1^{\\alpha_1 - 1}(1-\\theta_1)^{\\beta_1 - 1} \\\\ &amp;= \\theta_1^{\\sum_{i; z_i = 1}y_i + \\alpha_1 -1 }(1-\\theta_1)^{\\beta_1 + \\sum_{i; z_i = 1}(100-y_i) - 1} \\end{align*}\\] Hence \\(\\theta_1 \\mid \\boldsymbol{z}, \\boldsymbol{y} \\sim \\hbox{Beta}(\\sum_{i; z_i = 1}y_i + \\alpha_1 , 100N_1 + \\beta_1 - \\sum_{i; z_i = 1}y_i)\\). Similarly, \\(\\theta_2 \\mid \\boldsymbol{z}, \\boldsymbol{y} \\sim \\hbox{Beta}(\\sum_{i; z_i = 2}y_i + \\alpha_2 , 100N_2 + \\beta_2 - \\sum_{i; z_i = 2}y_i)\\) under the prior \\(\\theta_2 \\sim \\hbox{Beta}(\\alpha_2,\\beta_2)\\). Finally, we look at the conditional distribution of each \\(Z_i\\) given the remaining hidden variables \\(\\boldsymbol{z}_{-i}\\), the parameters \\(p\\), \\(\\theta_1\\) and \\(\\theta_2\\), and the observations \\(\\boldsymbol{y}\\). Similar to the previous derivation, we have \\[\\begin{align*} \\mathbb{P}(Z_i = 1 \\mid \\boldsymbol{z}_{-i},\\boldsymbol{y}, \\theta_2, p,\\theta_1) \\propto p\\theta_1^{y_i}(1-\\theta_1)^{100-y_i} \\end{align*}\\] Similarly, \\[\\begin{align*} \\mathbb{P}(Z_i = 2 \\mid \\boldsymbol{z}_{-i},\\boldsymbol{y}, \\theta_2, p,\\theta_1) \\propto \\theta_2^{y_i}(1-\\theta_2)^{100-y_i}(1-p). \\end{align*}\\] Since \\(Z_i\\) can only take two values, 1 or 2, we must have \\[ p^*_i = \\mathbb{P}(Z_i = 1 \\mid \\boldsymbol{z}_{-i},\\boldsymbol{y}, \\theta_2, p,\\theta_1) = \\frac{p\\theta_1^{y_i}(1-\\theta_1)^{100-y_i}}{p\\theta_1^{y_i}(1-\\theta_1)^{100-y_i}+\\theta_2^{y_i}(1-\\theta_2)^{100-y_i}(1-p)} \\] and \\[ \\mathbb{P}(Z_i = 2 \\mid \\boldsymbol{z}_{-i},\\boldsymbol{y}, \\theta_2, p,\\theta_1) = \\frac{\\theta_2^{y_i}(1-\\theta_2)^{100-y_i}(1-p)}{p\\theta_1^{y_i}(1-\\theta_1)^{100-y_i}+\\theta_2^{y_i}(1-\\theta_2)^{100-y_i}(1-p)} \\] Therefore, therefore \\(Z_i \\mid \\boldsymbol{y}, p,\\theta_1,\\theta_2 \\sim 2-\\hbox{Bernoulli}(p^*_i)\\). An MCMC algorithm for this would repeat the following steps: Initialise values for \\(p, \\theta_1, \\theta_2\\) and \\(\\boldsymbol{z}\\) Sample \\(p \\mid \\boldsymbol{z} \\sim \\hbox{Beta}(N_1 + 1, N_2 + 1)\\). Sample \\(\\theta_1 \\mid \\boldsymbol{z}, \\boldsymbol{y} \\sim\\hbox{Beta}(\\sum_{i; z_i = 1}y_i + \\alpha_1 , 100N_1 + \\beta_1 - \\sum_{i; z_i = 1}y_i)\\). Sample \\(\\theta_2 \\mid \\boldsymbol{z}, \\boldsymbol{y} \\sim \\hbox{Beta}(\\sum_{i; z_i = 2}y_i + \\alpha_2 , 100N_2 + \\beta_2 - \\sum_{i; z_i = 2}y_i)\\). Sample \\(Z_i \\mid \\boldsymbol{y}, p,\\theta_1,\\theta_2 \\sim 2-\\hbox{Bernoulli}(p^*_i)\\) for each \\(i\\). Repeat Steps 2-5. This algorithm may be slow to converge and explore the posterior distribution. This is because it has many parameters. There are 3 model parameters (\\(p\\), \\(\\beta_1\\) and \\(\\beta_2\\)) and \\(N\\) latent variables. Exploring a posterior distribution with this many dimensions may take a lot of time and more efficient alternatives may be required. 6.2.3 Grouped Data Often, we can only receive data in grouped form. This is often the case in medical and crime settings, where patients are grouped together to protect their identity. Grouping data introduces a difficulty when trying to perform inference. Augmenting the data, by introducing parameters to represent different parts of the group can both simplify the inference workflow and provide more information than is possible otherwise. Example 6.4 Suppose that we have data on the number of treatment sessions required by a random sample of patients before they recover from a disease. For identifiability reasons, the numbers of patients requiring two or fewer treatment sessions are grouped and we obtain the following table Number of sessions \\(x\\) \\(\\leq 2\\) 3 4 5 6 Frequency \\(f_x\\) 25 7 4 3 1 We are interested the the probability the treatment sessions are a success, assuming each session succeed independently. We can model the number of sessions required for each person using the geometric distribution, which has probability mass function \\[ \\mathbb{P}(X = x) = (1-p)^{x-1}p, \\qquad x = 1,2,3,\\dotsc \\] if \\(X \\sim \\hbox{Geometric}(p)\\). For each group of patients, we can work out their contribution to the observed likelihood function using a geometric distribution. For example, the contribution to the likelihood function for patients who had three treatment sessions is \\(((1-p)^2p)^7\\). Since \\[ \\mathbb{P}(X \\leq 2) = \\mathbb{P}(X=1)+\\mathbb{P}(X=2) = p+(1-p)p=(2-p)p, \\] the group that had two or fewer sessions contributes to the likelihood contribution by \\((p(2-p))^{25}\\). The observed likelihood function is therefore \\[\\begin{align*} \\pi(f_{1, 2}, f_3, \\ldots, f_6 \\mid p) &amp;= [p(2-p)]^{25}\\cdot [(1-p)^2p]^7\\cdot [(1-p)^3p]^4 \\cdot [(1-p)^4p]^3 \\cdot (1-p)^5p. \\\\ &amp;= (p(2-p))^{25}(1-p)^{43}p^{15} \\\\ &amp;= p^{40}(1-p)^{43}(2-p)^{25} \\end{align*}\\] There is no conjugate prior distribution that induces conjugacy. We use the prior distribution \\(p \\sim \\hbox{Beta}(\\alpha, \\beta)\\). By Bayes’ theorem, the posterior distribution is \\[ \\pi(p \\mid f_{1, 2}, f_3, \\ldots, f_6 ) \\propto p^{40 + \\alpha - 1}(1-p)^{43 + \\beta - 1}(2-p)^{25}. \\] We can use a Metropolis-Hasting algorithm to sample from this algorithm. A suitable algorithm would look like this Initialize \\(p^{(0)}\\) and set \\(i = 0\\). Propose \\(p&#39; \\sim U[p^{(i)} + \\varepsilon, p^{(i)} - \\varepsilon]\\). Accept \\(p^{(i+1)}=p&#39;\\) with probability \\(p_{acc}\\), otherwise reject as set \\(p^{(i+1)}=p^{(i)}\\). Set \\(i=i+1\\) and repeat steps 2-3. The acceptance probability in step 3 is given by \\[ p_{\\textrm{acc}} = \\min\\left\\{\\frac{(p&#39;)^{40 + \\alpha - 1}(1-p&#39;)^{43 + \\beta - 1}(2-p&#39;)^{25}}{p^{40 + \\alpha - 1}(1-p)^{43 + \\beta - 1}(2-p)^{25}} ,1\\right\\}. \\] Instead of using the observed data likelihood, we can write the complete data likelihood supposing we had seen the number of patients who had had one and two sessions. The complete data likelihood function is \\[\\begin{align*} \\pi(f_1 \\ldots, f_6 \\mid p) &amp;= p^{f_1}\\cdot [(1-p)p]^{f_2} \\cdot [(1-p)^2p]^7\\cdot [(1-p)^3p]^4 \\cdot [(1-p)^4p]^3 \\cdot (1-p)^5p. \\\\&amp;= p^{40}(1-p)^{f_2 + 43}, \\end{align*}\\] where \\(f_2\\) is the unobserved variable. Now, consider Beta(\\(\\alpha, \\beta)\\) as the prior distribution on \\(p\\). We have \\[ p \\mid f_1 \\ldots, f_6 \\sim \\hbox{Beta}\\left(40 + \\alpha, f_2+43 + \\beta\\right). \\] In order to implement Gibbs sampler, we just need to find \\(\\pi( f_2 \\mid f_3 \\ldots, f_6,p)\\). Note that \\(f_1\\) is not considered as an unknown variable since \\(f_1 = 25-f_2\\) and once we know \\(f_2\\), we would know \\(f_1\\). Under the geometric distribution model, the probability a patient recovers after exactly one session is \\(p\\) and after two sessions is \\((1-p)p\\). Moreover, \\(\\mathbb{P}(X = 2\\mid X\\leq 2) = \\frac{(1-p)p}{p+(1-p)p} = \\frac{1-p}{2-p}\\), for \\(X\\sim \\hbox{Geometric}(p)\\). We can therefore model \\(f_2\\) as a Binomial distribution \\[ f_2 \\mid p\\sim \\hbox{Binomial}(25,q), \\] where \\[ q = \\frac{1-p}{2-p}. \\] Our Gibbs sampler could be Initialise \\(p\\) and \\(f_2\\). Sample \\(p \\mid f_2 \\sim \\hbox{Beta}(40 + \\alpha, f_2 + 43 + \\beta)\\) Sample \\(f_2 \\mid p \\sim \\hbox{Binomial}(25,q)\\) with \\(q = \\frac{1-p}{2-p}.\\) Repeat steps 2 - 3. This Gibbs sampler avoids any tuning or accept/reject steps in MH sampler, making it potentially more efficient. 6.3 Prior Ellicitation (Optional reading) Throughout this course, we have tried to be objective in our choice of prior distributions. We have discussed uninformative and invariant prior distributions. Sometimes we have used vague prior distributions, such as the Exp(0.01) distribution. And other times, we have left the parameters as generic values. This misses out on one real difference between Bayesian and frequentist inference. In Bayesian inference, we can include prior information about the model parameters. Determining the value of prior parameters is know as prior elicitation. This is more of an art than a science and still controversial in the Bayesian world. In this section, we are going to look at a few ways of how to elicit prior information from experts. Example 6.5 This example to show how difficult it is to talk to experts in other areas about probabilities and risk. Suppose we are interested in estimating the number of crashes on a new motorway that is being designed. Denote the number of crashes per week by \\(X\\) and suppose \\(X \\sim \\hbox{Po}(\\lambda)\\). We place a \\(\\lambda \\sim \\Gamma(\\alpha, \\beta)\\) prior distribution on the rate parameter \\(\\lambda\\). The density function of this prior distribution is \\[ \\pi(\\lambda) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\lambda^{\\alpha-1}\\exp(-\\lambda\\beta). \\] The parameter \\(\\alpha\\) is know as the shape parameter and \\(\\beta\\) the rate parameter. We interview road traffic police, highway engineers and driving experts to estimate the values of \\(\\alpha\\) and \\(\\beta\\). The difficulty is that they do not know about the Gamma distribution or what shape and rate parameters are. Instead, we can ask them about summaries of the data. For the Gamma distribution, the mean is \\(\\alpha/\\beta\\), the mode is \\((\\alpha - 1)/\\beta\\) and the variance is \\(\\alpha/\\beta^2\\). If we can get information about two of these then we can solve for \\(\\alpha\\) and \\(\\beta\\). But needing two brings about another difficulty, non-mathematicians have limited understanding of statistics and probability. They can find it difficult to differentiate between the mean and the mode, and the concept of variance is very difficult to explain. 6.3.1 Prior Summaries The first method we are going to look at is called summary matching. For this method, we ask experts to provide summaries of what they think the prior distribution is. We then choose a function form for the prior distribution and use these summaries to estimate the prior distribution. The choice of summaries depend on the application in hand as well as the choice of prior distribution. Common choices are: the mean, median, mode, variance, and cumulative probabilities (i.e. \\(\\pi(\\theta &lt; 0.5)\\)). Example 6.6 Let’s return to the motorway example from above. Suppose that a highway engineer tells us the expected number of crashes per week is 0.75 and the probability that \\(\\lambda &lt; 0.9\\) is 80%. Matching summaries tells us \\[ \\frac{\\alpha}{\\beta} = 0.75 \\\\ \\int_0^{0.9}\\pi(\\lambda) d\\lambda = 0.8 \\] From the expectation, we have \\(\\alpha = 0.75\\beta\\). To estimate the value of \\(\\beta\\) from the cumulative probability, we need to find the root to the equation \\[ \\int_0^{0.9} \\frac{\\beta^{0.75\\beta}}{\\Gamma(\\beta)}\\lambda^{0.75\\beta-1}e^{-\\lambda\\beta}d\\lambda - 0.8 = 0. \\] This looks horrible, but there are many optimisation ways to solve this problem. cummulative.eqn &lt;- function(b){ #Compute equation with value beta = b value &lt;- pgamma(0.9, 0.75*b, b)-0.8 return(value) } uniroot(cummulative.eqn, lower = 1, upper = 1000) ## $root ## [1] 21.80224 ## ## $f.root ## [1] -5.747051e-08 ## ## $iter ## [1] 8 ## ## $init.it ## [1] NA ## ## $estim.prec ## [1] 6.103516e-05 This gives us \\(\\beta = 21.8\\) and \\(\\alpha = 13.65\\). 6.3.2 Betting with Histograms The difficulty with the summary matching method is that it requires experts to describe probabilities about the prior parameter, which is difficult to do and quite an abstract concept. Instead of asking them to describe summaries, we can ask them to draw the prior distribution completely, using a betting framework. In the prior weights (sometimes called roulette) method, we give the the experts a set of intervals for the parameter of interest and fixed number of coins. We then ask the experts to place the coins in the intervals according to how likely they think the parameter will be in each interval, effectively betting on what value the parameter will take. For example, they might place \\(n_1\\) coins in the interval \\(\\theta \\in [a, b)\\), \\(n_2\\) in \\(\\theta \\in [b, c)\\) and \\(n_3\\) for \\(\\theta \\in [c, d]\\). From this we can construct our prior density. Example 6.7 Suppose we are interested in the probability a pharmaceutical drug has the desired clinical effect. The outcome of each patient’s treatment can be modelled by \\(X\\sim\\hbox{Bernoulli}(p)\\). We ask a clinical about their experience with patients and similar treatments. We ask the expert to estimate the probability of successful treatment by betting on the value for \\(p\\) using the following table and 20 coins. \\(p\\) Coins [0, 0.2) 3 [0.2, 0.4) 7 [0.4, 0.6) 5 [0.6, 0.8) 3 [0.8, 1] 2 We can use the website http://optics.eee.nottingham.ac.uk/match/uncertainty.php# to fit a distribution to this table. It proposes the best fit of a \\(p \\sim \\Gamma(3.10, 6.89)\\). We can use this distribution as the prior distribution, although it is not conjugate and places weight on \\(p &gt; 1\\). Another option is a Beta\\((1.64, 2.16)\\) distribution, 6.3.3 Prior Intervals The last method we will look at is the bisection method. In this method, we ask the experts to propose four intervals, each of which the parameter value is likely to fall into, i.e. \\(\\pi(\\theta \\in [a, b)) = \\pi(\\theta \\in [c, d)) = \\pi(\\theta \\in [e, f))= \\pi(\\theta \\in [g, h)) = 0.25\\). From these intervals, we develop a prior distribution that fits these intervals. Example 6.8 The police are interested in estimating the number of matching features between a fingerprint from a crime scene and a fingerprint from a suspect in the police station. The number of matching features is \\(X \\sim \\hbox{Po}(\\lambda)\\). We speak to experienced fingerprint analysts. She advises us that about a quarter of the time, she would expect to see between 0 and 4 matches and this is when it is unlikely for the suspect to be the criminal. She says in some cases, it’s very clear that the suspect is the criminal as she would expect to see 20 to 30 matches. The rest of the time, she sees some matches but the fingerprint collected at the crime scene is poor quality, so see may see 5 to 14 matches. She agrees that the matches are uniform across this range. So our four intervals are [0, 4], [5, 12], [13, 19], [20, 30]. Using the Match software, we get a Uniform[0, 30] distribution. 6.4 Lab 6.4.1 Gaussian Processes Example 6.9 The code below shows how to set up a Gaussian Process and draw samples from it in R. require(MASS) ## Loading required package: MASS squared.exponential.covariance &lt;- function(x1, x2, alpha, ell) { #Squared Exponential Covariance Function #Inputs: x1, x2 -- vectors, alpha, ell -- variance and length scale parameter pairwise_distances &lt;- outer(x1, x2, &quot;-&quot;) covariance_matrix &lt;- alpha * exp(-0.5 * (pairwise_distances / ell)^2) return(covariance_matrix) } #Set up GP x &lt;- seq(0, 5, 0.01) mu &lt;- rep(0, length(x)) Sigma &lt;- squared.exponential.covariance(x, x, 1, 1) #Generate Samples and Summaries f &lt;- MASS::mvrnorm(1000, mu, Sigma) f.mean &lt;- apply(f, 2, mean) #GP mean f.95.upper &lt;- apply(f, 2, quantile, 0.975) f.95.lower &lt;- apply(f, 2, quantile, 0.025) #Generate some plots plot(x, f[1, ], type = &#39;l&#39;, ylim = c(min(f[1:3, ]), max(f[1:3, ]))) lines(x, f[2, ], col = 2) lines(x, f[3, ], col = 3) plot(x, f.mean, type = &#39;l&#39;, ylim = c(-5, 5)) #0 -- no surprise there polygon(c(x, rev(x)), c(f.95.lower, rev(f.95.upper)), col = rgb(0, 0, 1, 0.25)) Exercise 6.1 Code up example 6.1. How does your choice of length scale affect the posterior distribution. You can use x &lt;- -5:5 y &lt;- c(3.0942822, 3.0727920, 2.6137341, 1.8818820, 1.2746738, 1.2532116, 1.4620830, 1.4194647, 1.6786969, 1.1057042, 0.4118125) with \\(\\sigma^2 = 0.2\\).To draw samples from the multivariate normal distribution with mean vector \\(\\boldsymbol{\\mu}\\) and covariance matrix \\(\\Sigma\\) use the MASS package. Exercise 6.2 Repeat Exercise 6.1, but this time set the fine grid to be \\(\\boldsymbol{x}^* = \\{-5, -4.9, -4.8, \\ldots, 9.8, 9.9, 10\\}\\). What happens to the posterior distribution after \\(x^* = 5\\)? Exercise 6.3 You observe the following data x &lt;- -5:5 y &lt;- cos(0.5*x) + log(x + 6) y ## [1] -0.8011436 0.2770003 1.1693495 1.9265967 2.4870205 2.7917595 ## [7] 2.8234927 2.6197438 2.2679618 1.8864383 1.5967517 plot(x, y, xlab = &quot;x&quot;, ylab = &quot;f(x)&quot;, ylim = c(0, 4)) Fit a function to the data using a GP prior distribution. Note that this time there is no noise. 6.4.2 Censored Data Exercise 6.4 In Example 6.2, suppose the observed data is \\(\\{y_1, y_2, y_3, y_4\\} = \\{4, 4, 5 ,2\\}\\). Design and code an MCMC algorithm to generate samples from the posterior distribution for \\(p\\) and \\(y_5\\). Exercise 6.5 Suppose you manage a clinical trial. You administer a new drug to patients and record how many days until their symptoms are alleviated. You observe the times for the first 9 patients x &lt;- c(33.5, 17.8, 218.2, 3, 39.2, 3.5, 43.7, 14.6, 20) Patient 10 drops out of the trial on day 50 and at this point, their symptoms have not changed. They send an email on day 200 to say they no longer have any symptoms (i.e. \\(x_i \\in (50, \\ldots, 200]\\). Write down a model for this problem and derive the posterior distribution. Design and code an MCMC algorithm to generate samples from the posterior distribution for any model parameters that you introduce and the censored observation \\(x_{10}\\). Hairer, Martin, and Jonathan C. Mattingly. 2011. “Yet Another Look at Harris’ Ergodic Theorem for Markov Chains.” In Seminar on Stochastic Analysis, Random Fields and Applications VI, 63:109–17. Progr. Probab. Birkhäuser/Springer Basel AG, Basel. https://doi.org/10.1007/978-3-0348-0021-1_7. Meyn, Sean, and Richard L. Tweedie. 2009. Markov Chains and Stochastic Stability. Second. Cambridge University Press, Cambridge. https://doi.org/10.1017/CBO9780511626630. "]]
